[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Welcome to the official course site for PSTAT 5A (titled Understanding Data) at the University of California, Santa Barbara! Please note that this is the site for Summer Session A, 2023 iteration of the course, with Ethan Marzban.\n\nAll relevant information for the course can be found on this site, with the (perhaps crucial) exception of quizzes, which will take place on the course Gradescope site.\n\nIf you are looking for a past iteration of this course, please navigate to https://pstat5a-archives.github.io"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html",
    "href": "Pages/Labs/Lab03/lab03.html",
    "title": "Lab03",
    "section": "",
    "text": "It’s finally time for us to revisit our notions of descriptive statistics (from Week 1 of the course), now in the context of Python!"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#modules-revisited",
    "href": "Pages/Labs/Lab03/lab03.html#modules-revisited",
    "title": "Lab03",
    "section": "Modules, Revisited",
    "text": "Modules, Revisited\nBefore we talk about plotting, we will need to quickly talk about modules again. Recall from Lab01 that modules are Python files containing definitions for functions and classes. Up until now, we’ve been importing all functions and classes from a module using the command\n\nfrom <module name> import *\n\nThere is another way to import modules, which is the following:\n\nimport <module name> as <abbreviation>\n\nFor example,\n\nimport numpy np\n\nnot only imports the numpy module but imports it with the abbreviation (i.e. nickname) np so that we can simply write np in place of numpy.\n\nThe reason this is particularly useful is because module names can sometimes be quite long, so being able to refer to the module with a shortened nickname will save a lot of time!\n\nIn general, if we import a module using\n\nimport <module name> as <abbreviation>\n\nwe reference functions from <module name> using the syntax\n\n<abbreviation>.<function name>()\n\nFor example, after having imported the numpy module with the nickname np, we access the sin() function contained in the numpy module by calling\n\nnp.sin()\n\n\n\n\n\n\n\nTask 1\n\n\n\n\nImport the numpy module as np, and check that np.sin(0) returns a value of 0.\nImport the datascience module as ds, and check that\n\n\nds.Table().with_columns(\n  \"Col1\", [1, 2, 3],\n  \"Col2\", [2, 3, 4]\n)\n\ncorrectly displays as\n\n\n\n\n    \n        \n            Col1 Col2\n        \n    \n    \n        \n            1    2   \n        \n        \n            2    3   \n        \n        \n            3    4   \n        \n    \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you import a module with an abbreviation <abbreviation>, you must always use the abbreviation when referencing the module; not the original module name.\n\n\nFor example, after importing numpy as np, running numpy.sin() would return an error."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#numerical-summaries",
    "href": "Pages/Labs/Lab03/lab03.html#numerical-summaries",
    "title": "Lab03",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nMeasures of Central Tendency\nRecall that for a list of numbers \\(X = \\{x_i\\}_{i=1}^{n}\\), the mean is defined to be \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] Computing the mean of a list or array of numbers in Python is relatively simple, using the np.mean() function [recall that we imported the numpy module with the abbreviation np, meaning np.mean() is a shorthand for numpy.mean()]. Similarly, to compute the median of a list or array we can use np.median().\n\n\n\n\n\n\nTask 2\n\n\n\nLet x_list be a list containing the elements 1, 2, and 3, and let x_array be an array containing the elements 1, 2, and 3. Compute the mean and median of x_list and x_array using the appropriate functions from the numpy module.\n\n\n\n\nMeasures of Spread\nRecall that we also discussed several measures of spread:\n\nStandard deviation\nIQR (Interquartile Range)\nRange\n\nSure enough, the numpy module contains several functions which help us compute these measures. Let’s examine each separately.\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up the help file on the function np.ptp(), and describe what it does. Also, answer the question: what does ptp actually stand for?\nNow, apply the np.ptp() function on your x_list and x_array variables from Task 1 above and check that it functions like you expect.\n\n\n\nNext, we tackle a slightly peculiar function: np.std(). We expect this to compute the standard deviation of a list/array, but…\n\n\n\n\n\n\nTask 4\n\n\n\n\nCompute the standard deviation of the x_list variable from Task 1 by hand, and write down the answer using a comment or Markdown cell.\nNow, run np.std(x_list). Does this answer agree with what you found in part (a) above?\nNow, recompute the standard deviation of x_list by hand but this time use \\((1/n)\\) instead of \\((1 / n - 1)\\) in the formula. How does this answer compare with the result of np.std(x_list)?\n\n\n\nThe result of the previous Task is the following: given a list x = [x1, x2, ..., xn], running np.std(x) actually computes \\[ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 } \\] as opposed to our usual definition of standard deviation \\[ s_X = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\] We can actually fix this issue by passing in an additional argument to the np.std() function:\n\n\n\n\n\n\nTask 4 (cont’d)\n\n\n\n\nRun np.std(x_list, ddof = 1) and check whether this matches the result of part (a) above.\n\n\n\n\n\n\n\n\n\nResult\n\n\n\nTo compute the standard deviation of a list x, we run np.std(x, ddof = 1).\n\n\nFinally, we turn to the IQR: to compute the IQR of a list/array x, we use (after importing numpy as np)\n\nnp.diff(np.percentile(x, [25,75]))[0]"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#visualizations",
    "href": "Pages/Labs/Lab03/lab03.html#visualizations",
    "title": "Lab03",
    "section": "Visualizations",
    "text": "Visualizations\nIt’s finally time to make pretty pictures! The module we will use to generate visualizations in this class is the matplotlib module (though there are quite a few other modules that work for visualizations as well). The official website for matplotlib can be found at https://matplotlib.org/.\n\nBefore we generate any plots, we will need to run the following code once:\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nHere’s what these lines of code are doing:\n\n%matplotlib inline tells Jupyter to actually display our plots in our notebook (if we didn’t include this line, our plots wouldn’t display)\nimport matplotlib imports the matplotlib module\nimport matplotlib.pyplot as plt imports the pyplot submodule (a submodule is just a module contained within another larger module) with the abbreviation plt.\nplt.style.use('seaborn-v0_8-whitegrid') tells Jupyter to use a specific theme (called seaborn-v0_8-whitegrid) when generating plots.\n\nAgain, notice the beauty of the import <module> as <abbreviation> syntax- after running the third line above, we no longer need to write matplotlib.pyplot, just plt! Also, there are lots of other themes you can use when generating your plots: after completing this lab, I encourage you to consult this reference guide for a list of a few other pyplot themes.\n\nBoxplots and Histograms\nNow, let’s proceed on to make some plots. The first two types of plots we will look at are the two we used to describe numerical data: namely, boxplots and histograms. The functions we will use are the plt.boxplot() and plt.his() functions, respectively.\n\n\n\n\n\n\nTask 5\n\n\n\n\nMake a list called y that contains the following elements: [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2].\nRun plt.boxplot(y); (be sure to include the semicolon!). With any luck, your plot should look like:\n\n\n\n\n\n\n\nLet’s make our boxplot horizontal, as opposed to vertical. Consult the help file on the matplotlib.pyplot.boxplot() function here and figure out how to position your boxplot horizontally. Your new plot should look like:\n\n\n\n\n\n\n\nNext, let’s add some color to our plot. Within your call to plt.boxplot(), add the following: patch_artist=True, boxprops = dict(facecolor = \"aquamarine\") (don’t worry too much about what exactly this code is doing). Your boxplot should now look like this:\n\n\n\n\n\n\n\nFinally, let’s add a Title! Right below your call to plt.boxplot(), add the following: plt.title(\"My First Python Boxplot\"); (again, note the semicolons). Your final plot should look like this:\n\n\n\n\n\n\n\nTime for a review: based on the boxplot we just generated, what is the IQR of y? Write your answer in a Markdown cell. Then, use the syntax discussed in the previous section of this Lab to use Python to compute the IQR of y, and comment on the result.\n\n\n\nOf course, boxplots are not the only way to summarize numerical variables: we also have histograms!\n\n\n\n\n\n\nTask 6\n\n\n\nCall the plt.hist() function on the y list defined in Task 3, and use the help file to add arguments to your call to plt.hist() function to generate the following plot:\n\n\n\n\n\nPay attention to the number of bins!\n\n\n\n\nScatterplots\nWe should also quickly discuss how to generate scatterplots in Python.\n\n\n\n\n\n\nTask 7\n\n\n\n\nCopy-paste the following code into a code cell, and then run that cell (don’t worry about what this code is doing- we’ll discuss that in a future lab).\n\n\nnp.random.seed(5)\n\nx1 = np.random.normal(0, 1, 100)\nx2 = x1 + np.random.normal(0, 1, 100)\n\nplt.scatter(x1, x2);\n\nYour plot should look like this:\n\n\n\n\n\n\nAdd an x-axis label that says \"x1\" and a y-axis label that says \"x2\", along with the title “My First Python Scatterplot”. Your final plot should look like:\n\n\n\n\n\n\n\nDoes there appear to be an association between the variables x1 and x2? If so, is the association positive or negative? Linear or nonlinear? Answer using a comment or a Markdown Cell."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#plotting-a-function",
    "href": "Pages/Labs/Lab03/lab03.html#plotting-a-function",
    "title": "Lab03",
    "section": "Plotting a Function",
    "text": "Plotting a Function\nFinally, I’d like to take a quick detour from descriptive statistics and talk about how to plot a function using Python. As a concrete example, let’s try and plot a sine curve from \\(0\\) to \\(2\\pi\\).\nIf you recall, on Lab01 we used the sin() function from the math module- it turns out that the numpy module (which, recall, we have imported as np) also has a sin() function, so let’s use that one today:\n\nnp.sin()\n\nNext, we create a set of finely-spaced points between our two desired endpoints (in this case, \\(0\\) and \\(2\\pi\\), respectively). We will do so using the np.linspace() function, which works as follows:\n\nnp.linspace(start, stop, num)\n\ncreates a set of num evenly-spaced values between start and stop, respectively. For instance:\n\nnp.linspace(0, 1, 10)\n\narray([ 0.        ,  0.11111111,  0.22222222,  0.33333333,  0.44444444,\n        0.55555556,  0.66666667,  0.77777778,  0.88888889,  1.        ])\n\n\nIn the context of plotting, the more points we generate the smoother our plot will seem (you will see what this means in a minute). As such, let’s start with 150 points between 0 and 2 * pi:\n\nx = np.linspace(0, 2 * np.pi, 150)\n\nFinally, we call the plt.plot() function on x and np.sin(x) to generate our plot:\n\nplt.figure(figsize=(4.5, 2.25))\nplt.plot(x, np.sin(x))\n\n\n\n\nLet’s see what would have happened if we used fewer values in our np.linspace() call:\n\nxnew = np.linspace(0, 2 * np.pi, 10)\nplt.plot(xnew, np.sin(xnew))\n\n\n\n\n\n\nSo, the more points we include in our call to np.linspace(), the smoother our final function will look!\n\nSo, to summarize, here is the general “recipe” to plot a function f() between two values a and b in Python:\n\nLet x = np.linspace(a, b, <some large value>)\nCall plt.plot(x, f(x))\nAdd labels/titles as necessary\n\n\n\n\n\n\n\nTask 8\n\n\n\nGenerate a plot of the function \\(f(x) = x - x^2 \\sin(x)\\) between \\(x = -10\\) and \\(x = 10\\). Experiment around with the number of values generated by np.linspace() to ensure your plot is relatively smooth. Be sure to include axis labels; also, change the color of the graph to red. Your final plot should look something like this:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNOTICE\n\n\n\nYou only need to complete up to here during Lab, but you should complete the following tasks on your own as they are fair game for quizzes and exams (and are also very useful for your own edification!)\n\n\n\nOverlaying Plots\nSometimes it will be useful to overlay two plots on top of each other. Recall that, for a function f() and a variable x that has been assigned a value resulting from a call to numpy.linspace(), we generate a graph of f() using (assuming matplotlib.pyplot has been imported as plt)\n\nplt.plot(x, f(x));\n\nIt stands to reason, then that given another function g() we should be able to superimpose the graph of g() onto the graph of f() by simply adding another call to plt.plot():\n\nplt.plot(x, f(x));\nplt.plot(x, g(x));\n\n\n\n\n\n\n\nTask 9\n\n\n\nGenerate a graph of sin(); on top of this graph, superimpose the graph of cos(). Restrict the x values on your graph to be between \\(-4\\pi\\) and \\(4\\pi\\). Your final graph should look like the following (pay attention to the axis labels and title!):\n\n\n\n\n\n\n\nNow, as it stands, it’s a bit difficult to determine which curve corresponds to the sine curve and which corresponds to the cosine curve. As such, we should add some labels!\n\n\n\n\n\n\nTask 10\n\n\n\nCopy your code from Task 9 above into a new code cell, and\n\nadd label = \"sine\" to your call to plt.plot() containing the sine curve\nadd label = \"cosine\" to your call to plt.plot() containing the cosine curve.\n\nDoes this new plot look any different than the plot you generated in Task 3?\n\n\nHm, doesn’t look like anything changed… That’s because we didn’t add a legend to our plot! To add a legend, we simply tack on a call to plt.legend() after our code from above.\n\n\n\n\n\n\nTask 11\n\n\n\nCopy your code from Task 10 above into a new code cell, and add a line underneath it containing a call to plt.legend(). Look up the help file to figure out what arguments you need to pass in to obtain the following graph (note the position of the legend):\n\n\n\n\n\n\n\nOkay, we’re almost there! The only issue is that now the legend is covered up by the actual graphs. One way we can fix this is by extending the \\(y-\\)axis further, using the function plt.ylim():\n\n\n\n\n\n\nTask 12\n\n\n\nCopy your code from Task 11 above into a new code cell, and add a line underneath it containing a call to plt.ylim(). Look up the help file to figure out what arguments you need to pass in to obtain a lower \\(y-\\)limit of \\(-1.5\\) and an upper \\(y-\\)limit of \\(2.0\\). Your final graph should look like this:\n\n\n\n\n\n\n\nFinally, it is sometimes considered bad form to rely too heavily on colors in plots. This is because doing so alienates readers who are colorblind. One way around this is to rely on different line types; e.g. used dashed lines for one graph and dotted lines for another.\n\n\n\n\n\n\nTask 13\n\n\n\nCopy your code from Task 12 above into a new code cell. Read the following help file and figure out how to pass in a value to the linestyle argument to your two calls to plt.plot() to generate the following plot:\n\n\n\n\n\nNote that the sine curve is now dotted, and the cosine curve is now dashed."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html",
    "href": "Pages/Labs/Lab04/lab04.html",
    "title": "Lab04",
    "section": "",
    "text": "If you recall, one of the first things we did in Lab (back in Week 1!) was to use Python as a calculator. At the time, we only used Python to compute relatively simple quantities. Now that we’ve talked a bit about distributions, you can see how Python might be able to simpliy our lives greatly!\nFor instance, take the probability mass function (p.m.f.) of the \\(\\mathrm{Bin}(n, p)\\) distribution: if \\(X \\sim \\mathrm{Bin}(n, p)\\), then \\[ \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k} \\] Can we get Python to compute this for us? Or, remember how when we want to find areas under a normal density curve we have to use tables- can we perhaps compute these areas using Python?\nThe answer to both of these questions is, naturally, “yes”! Specifically, we will make use of the scipy.stats module which contains a plethora of functions relating to the distributions we learned in this class (as well as other distributions we won’t have time to cover).\n\nimport scipy.stats as sps\n\nLet’s tackle the Binomial distribution first. The function sps.binom.pmf() allows us to compute the p.m.f. of the Binomial distribution (with specified parameters) at a particular point.\n\n\n\n\n\n\nTask 1\n\n\n\nLet \\(X \\sim \\mathrm{Bin}(143, 0.153)\\). Compute the following using the sps.binom.pmf() function:\n\n\\(\\mathbb{P}(X = 20)\\)\n\\(\\mathbb{P}(X = 40)\\) [make sure you understand the output of this; feel free to ask your TA if you are confused!]\n\n\n\nNow, let’s talk about areas under the normal curve. If we want to find the following area:\n\nwe would run the following code:\n\nsps.norm.cdf(t, mu, sigma)\n\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf \\(X \\sim \\mathcal{N}(3, 0.5)\\), compute \\(\\mathbb{P}(X \\leq 2)\\).\nIf \\(X \\sim \\mathcal{N}(-2, \\ 1)\\), compute \\(\\mathbb{P}(X \\geq 1)\\).\nIf \\(X \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(-1 \\leq X \\leq 1)\\).\n\n\n\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up how to compute the c.d.f. of the Uniform distribution.\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), use Python to compute \\(\\mathbb{P}(X \\leq 0.1532)\\)."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#percentiles",
    "href": "Pages/Labs/Lab04/lab04.html#percentiles",
    "title": "Lab04",
    "section": "Percentiles",
    "text": "Percentiles\nAs we have seen in lecture, confidence intervals for a population parameter \\(\\theta\\) take the form \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{s.e.} \\] where \\(\\mathrm{s.e.}\\) denotes the standard error (i.e. standard deviation) of the point estimator \\(\\widehat{\\Theta}\\), and \\(c\\) is an appropriately-selected percentile from the distribution of \\(\\widehat{\\Theta}\\).\nUp until now, we have primarily been finding the constant \\(c\\) using the various tables at our disposal. Though being able to read these tables is a useful skill (and a skill that is potentially testable on quizzes and exams…), using computers to compute these percentiles can greatly increase efficiency.\nThe syntax\n\nscipy.stats.norm.ppf(p, m, s)\n\ncomputes the pth percentile of the \\(\\mathcal{N}\\)(m, s) distribution. We will revisit this fairly soon, once we are exposed to another continuous distribution in a few lectures. There are analagous functions that allow us to compute percentiles of other distributions; for example, scipy.stats.t.ppf() can be used to find the percentiles of the \\(t\\) distribution.\n\n\n\n\n\n\nTask 4\n\n\n\n\nUse Python to find the confidence coefficient of a 95% confidence interval for a population proportion.\nUse Python to find the confidence coefficient of an 82% confidence interval for a population proportion."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#simulation",
    "href": "Pages/Labs/Lab04/lab04.html#simulation",
    "title": "Lab04",
    "section": "Simulation",
    "text": "Simulation\nNow, let’s tie things together slightly. As data scientists, we obviously love to use data! However, sometimes data can be too time-consuming, costly, or otherwise unfeasible to collect in large quantities. In certain situations, simulations can help address these issues.\nWhen asked to define a “simulation” in the context of data science, ChatGPT returned the following:\n\n[…] a simulation is a computational model or program that is used to replicate real-world scenarios or systems in order to analyze their behavior, predict outcomes, or test hypotheses.\n\nThis is actually a great definition: simulations are designed to simulate (i.e. mimic) real-world situations to generate new observations/outcomes that (we hope) closely resemble the real-world outcomes.\nFor example, suppose we believe that weights of rats in a particular situation are normally distributed with mean 3.8oz and a standard deviation of 0.5oz. Instead of actually going out and collecting the weights of, say, 10 different rats and recording them, we could simulate collecting these weights by generating a series of random numbers that follow the \\(\\mathcal{N}(3.8, \\ 0.5)\\) distribution:\n\n\narray([3.2571847 , 4.29867272, 3.94148925, 3.04685264, 3.51069987,\n       4.62571827, 2.58666038, 3.58554369, 4.43296813, 3.3666298 ])\n\n\nThere are (once again) several modules that contain functions designed to simulate draws from different distributions: for now, we’ll stick with the scipy.stats module.\nTo simulate n draws from a \\(\\mathcal{N}(\\)mu, sigma\\()\\) distribution we use the code\n\nsps.norm.rvs(mu, sigma, n)\n\n(note that, by default, the sample size comes at the end!) To simulate n draws from a \\(\\mathrm{Unif}(\\)a, b\\()\\) distribution we use the code\n\nsps.uniform.rvs(a, b, n)\n\n\n\n\n\n\n\nTask 5\n\n\n\n\nThe time spent waiting in line at Romaine’s is uniformly distributed between 2 mins and 10 mins. Simulate the process of waiting in line at Romaine’s one hundred times; store your result in a variable called x and us the command x[0:11] to display only the first 10 elements of x. (Aside: See if you can understand the syntax used here!)\nThe temperature of a healthy adult is normally distributed with mean 98.2 degrees Fahrenheit and standard deviation 2.4 degrees Fahrenheit. Simulate the process of selecting 150 healthy adults and recording their temperatures (in degrees Fahrenheit); store your result in a variable called y and display only the first 10 elements of y. (Hint: Remember how to index variables!)\n\n\n\nHow might we simulate the experiment of picking \\(k\\) numbers from a specified set? There are several different ways to do this in Python- the way we will do this is the same way we conducted this experiment during the Lecture 11 demo, using the np.random.choice() function.\n\n\n\n\n\n\nTask 6\n\n\n\nImport the numpy.random module with the nickname npr. Simulate the experiment of rolling a fair six-sided die 10 times and recording the outcome of each roll. Think about how you can translate this experiment into an experiment consisting of drawing numbers at random from a set of specified numbers.\n\n\n\nSetting the Seed\nNow, when it comes to simulations, there is a very important concept known as setting a seed.\n\n\n\n\n\n\nTask 7\n\n\n\n\nWrite npr.choice([1, 2, 3], size = 4) in a code cell, and run it three times. In a Markdown cell just below this cell, answer the following question: did you get the same result each time you ran the code cell?\nIn a new code cell write\n\n\nnpr.seed(15)\nnpr.choice([1, 2, 3], size = 4)\n\nRun this new cell three times and again answer the question: did you get the same result each time you ran the code cell?\n\nNow, turn to your neighbor and check whether you both got the same result as each other when completing task (b) above?\n\n\n\nAs you can see, setting a seed, in a sense, removes a certain amount of randomness in Python. After you set a seed, your random number generator will generate the same number (or set of numbers) every time you run it. Though it may seem unclear as to why we would want this, you may be able to imagine that setting the seed is extremely important when it comes to replicability, a concept we will return to later in the course."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#introduction-to-loops",
    "href": "Pages/Labs/Lab04/lab04.html#introduction-to-loops",
    "title": "Lab04",
    "section": "Introduction to Loops",
    "text": "Introduction to Loops\nSuppose we have the following outcomes of an experiment:\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\nHow might we write code to count the number of successes in this string of outcomes? There are several different ways to accomplish this: one involves what is known as a for loop.\n\n\n\nfigure source: https://i.kym-cdn.com/photos/images/newsfeed/001/393/656/da7.jpg\n\n\nHere’s the general idea: we would like to perform an element-wise comparison; that is, we would like to iteratively check whether each element of x is a success or a failure. The “brute-force” way would be to check each element individually, using comparisons:\n\nx[0] == 'success'\n\nTrue\n\n\n\nx[1] == 'success'\n\nFalse\n\n\n\nx[2] == 'success'\n\nFalse\n\n\nAs you can imagine, though, this would get incredibly tedious, especially if x were large! This is where for loops become useful: they allow us to automate this iterative process.\nBefore returning to this success/failure problem, let’s look at an example to see how for loops work.\n\nfor fruit in ['apple', 'banana', 'pear']:\n  print(fruit)\n\napple\nbanana\npear\n\n\nHere are how the different components work:\n\nThe for keyword signifies the beginning of the for loop.\nThe name fruit is the variable.\nThe list following the in keyword contains all of the different values the variable will take during the execution of the for loop.\nThe code after the initial colon : is called the body of the loop. (Note that the body of a for loop must be indented properly!) Here is how the body is executed:\n\nFirst, the variable fruit is assigned the first value in the list of possible values specified in the first line of the loop\nThen, after assigning fruit this value, the code in the body is executed once.\nNext, the variable fruit is assigned the second value of the list of values, and the body is run again.\nThis continues until the list of all possible values is exhausted.\n\n\nSometimes, it may be useful to sketch a diagram/table to keep track of the code at each iteration of the loop:\n\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\n\nIt may seem strange to keep track of the values of the variables at the end of each iteration. The reason we do so is because sometimes the body of the loop will actually change the value of a variable! For example, consider the code\n\nfor n in [1, 2, 3]:\n  n += 2\n  print(n)\n\n3\n4\n5\n\n\nthe associated diagram would look like\n\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 1\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 2\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 4\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 5\n\n\n\n\n\n\nBy the way, notice the shorthand notation += that was used above:\n\n\n\n\n\n\nTip\n\n\n\nThe code x += y is equivalent to x = x + y.\n\n\nFinally, one thing that should be mentioned is that you can call the variable in a loop whatever you like!\n\nfor yummy in ['apple', 'banana', 'pear']:\n  print(yummy)\n\napple\nbanana\npear\n\n\n\n\n\n\n\n\nTask 8\n\n\n\nCopy-paste the code\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\ninto a cell, and run it. Then, create a for loop that iterates through the elements of x and at each iteration prints True if the corresponding element of x is a 'success' and False if the corresponding element of x is a 'failure'. Your final output should look like:\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n\n\nBy the way, the set of values a variable will take during a for loop doesn’t have to be a list- it could also be an array! This is particularly useful when there are multiple things we would like to iterate over. For example:\n\nimport datascience as ds\ncredit_scores = ds.make_array(\n  [\"Anne\", 750],\n  [\"Barbara\", 755],\n  [\"Cassandra\", 745]\n)\n\nfor k in credit_scores:\n  print(k[0], \"has a credit score of\", k[1])\n\nAnne has a credit score of 750\nBarbara has a credit score of 755\nCassandra has a credit score of 745\n\n\n\n\n\n\n\n\nTask 9\n\n\n\nMake a table like the one above that keeps track of the variables and their values in the above loop. You do not need to turn this in; do it on a separate sheet of paper and in your .ipynb file simply state “I have done Task 2 on a separate sheet of paper.”\n\n\nNow, we never quite finished our problem of counting the number of successes in the variable x. We were able to iterate through the elements of x to determine which were successes and which were failures, but we never counted the number of successes.\n\nHere is the general idea:\n\nWe initialize a counter variable, which starts off with the value of 0.\nThen, we iterate through the elements of x as we did in Task 1 above. Instead of printing True or False, however, we use a conditional statement to add 1 to count if the corresponding element of x (i.e. the element of x under consideration in the current iteration of the loop).\nFinally, we see what the value of our counter variable is- this will be exactly the number of successes in x!\n\n\n\n\n\n\n\nTask 10\n\n\n\nCombine everything we’ve learned so far to count the number of successes in x. Here is a rough template of how your code should look:\n\ncount = 0     # initialize the counter variable\n\n<for loop code here, containing a conditional and a 'count += 1'>\n\ncount       # display the final value of our counter variable\n\n\n\nThere is another way to iterate through the elements in a list, and this is to use indexing. Before talking about how this works, we should quickly introduce another function: the arange() function from the numpy module. Here is how a general call to numpy.arange() works:\n\nnumpy.arange(a, b, n)\n\nThis code returns the array of evenly spaced integers between a and b - including a but excluding b, where each element is s more than the previous element. That is, the code above is equivalent to array([a, a+s, a+2s, ...]) As a concrete example:\n\nimport numpy as np\nnp.arange(0, 5, 2)\n\narray([0, 2, 4])\n\n\nThe arange() function is particularly useful when we are iterating using indices. For example, given a list x = [1, 2, 3, 4, 5], we can loop through the entries of x using:\n\nfor k in np.arange(0, len(x)):\n  print(x[k])\n\n1\n2\n3\n4\n5\n\n\nNote that this is equivalent to\n\nfor k in x:\n  print(k)\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n\nTask 11\n\n\n\nRewrite your loop from Task 10, except now iterate through the indices of x. Check that your output is the same as in Task 10.\n\n\n\nQuick Aside: arange() vs linspace()\nSome of you may recall that we previously used the numpy.linspace() function to generate a list of numbers between two specified endpoints. The key difference between these two functions is that:\n\narange() allows you to specify the step size\nlinspace() allows you to specify the final number of elements\n\n\n\n\n\n\n\nTask 12\n\n\n\nGenerate the list of numbers [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 2.9, 2] in two ways: one using arange() and the other using linspace()."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#data-classes",
    "href": "Pages/Labs/Lab02/lab02.html#data-classes",
    "title": "Lab02",
    "section": "Data Classes",
    "text": "Data Classes\nLast week, we were introduced to the notion of data types. Recall that “data type” can be thought of as the category (or type) of data- i.e. integer, float, character, etc.\n\nIn Python, however, we often need to aggregate data into larger structures, often referred to as data classes.\n\nLists\nPerhaps the most fundamental data structure in Python is that of a list. Just like lists in real life or in mathematics, Python lists are just collections of items enclosed in square brackets:\n\n[<item 1>, <item 2>, ..., <item n>]\n\nAgain, the items in a list can be of any data type; we can even mix and match data types!\n\n\n\n\n\n\nTask 1\n\n\n\nCreate a list containing the elements 1, \"hi\", 3.4, and \"PSTAT 5A\". Assign this list to a variable called list1.\n\n\nJust as we were able to use a Python function (type()) to check the type of a particular piece of data, we can also use Python to check the structure or class of a piece of data. It turns out that we use the same function as before- namely, type()!\n\n\n\n\n\n\nTask 1 (cont’d)\n\n\n\nRun the code type(list1)."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#indexing",
    "href": "Pages/Labs/Lab02/lab02.html#indexing",
    "title": "Lab02",
    "section": "Indexing",
    "text": "Indexing\nAlright, now that we can store data in lists, how can we access elements in a list? The answer is to use what is known as indexing.\n\nGiven a list x, we access the ith element using the code\n\nx[i]\n\nThe reason we call this “indexing” is because the number that goes between the brackets is the index of the element that we want.\n\n\n\n\n\n\nCaution\n\n\n\nPython begins indexing at 0.\n\n\nWhat does this mean? Well, let’s see by way of an example.\n\n\n\n\n\n\nTask 2\n\n\n\n\nCreate a list with the numbers 1 through 10, inclusive, and assign this to a variable called x.\nRun the code x[1].\nRun the code x[0].\n\n\n\nSo, what we would colloquially call the first element of a list, Python calls the zeroeth element.\n\n\nAlright, let’s put together some of the concepts we just learned.\n\n\n\n\n\n\nTask 3\n\n\n\nCreate a list called x that contains the elements 1, \"two\", 3.5, \"four\", and \"five five\". Answer the following questions WITHOUT running any code, writing your answers as a comment in a code cell:\n\nWhat would be the output of type(x)?\nWhat would be the output of type(x[1])?\nWhat would be the output of x[0]?\n\nNow, run code to verify your answers to the above three questions."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#tables",
    "href": "Pages/Labs/Lab02/lab02.html#tables",
    "title": "Lab02",
    "section": "Tables",
    "text": "Tables\nAnother very useful data structure in Python is that of a table. Python tables behave pretty much the same as the tables we’ve used in, say, math- they are a grid of values arranged sequentially.\n\nTables can be created using the Table() function in Python, which itself comes from the datascience module. The general syntax of creating a table with the Table() function is:\n\nTable().with_columns(\n  \"<col 1 name>\", [<col 1, val 1>, <col 1, val 2>, ... ],\n  \"<col 2 name>\", [<col 2, val 1>, <col 2, val 2>, ... ],\n  ...\n)\n\nFor example,\n\nTable().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Office\", [\"South Hall\", \"South Hall\", \"North Hall\"]\n)\n\n\n\n    \n        \n            Name ID Office\n        \n    \n    \n        \n            Ethan  12345 South Hall\n        \n        \n            Morgan 10394 South Hall\n        \n        \n            Amy    20343 North Hall\n        \n    \n\n\n\nThere is nothing stopping us from assigning a table to a variable! For example, after running\n\ntable1 = Table().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Fav_Drink\", [\"Iced Tea\", \"Coffee\", \"Sprite\"]\n)\n\nthe variable table1 is equivalent to the table displayed above:\n\ntable1\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n        \n            Ethan  12345 Iced Tea \n        \n        \n            Morgan 10394 Coffee   \n        \n        \n            Amy    20343 Sprite   \n        \n    \n\n\n\n\n\n\n\n\n\nTerminology\n\n\n\nSometimes in Python we will encounter expressions of the form\n\n<object type>.<function name>()\n\nIn this syntax, the function <function name> is said to be a method. For example, the function with_columns() is a method for the Table object.\n\n\nThe datascience module contains a plethora of methods we can use to manage tables. For example, the select() method can be used to select columns by name:\n\ntable1.select(\"ID\")\n\n\n\n    \n        \n            ID\n        \n    \n    \n        \n            12345\n        \n        \n            10394\n        \n        \n            20343\n        \n    \n\n\n\n\n\n\n\n\n\nSyntax\n\n\n\nMethods are always appended to either a function that creates a blank object type (like Table()) or a variable of the correct type.\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nRead the list of methods for Table objects at http://data8.org/datascience/tables.html, and write down (in a code cell, using comments) at least three different methods, including a short description of what each method does. For example:\n\n# .with_columns(): adds specified columns to a table.\n\n\n\n\n\n\n\n\n\nTask 5\n\n\n\n\nCreate the following table, and assign it to a variable called profs:\n\n\n\n\n\n    \n        \n            Professor Office Course\n        \n    \n    \n        \n            Dr. Swenson    South Hall PSTAT 130 \n        \n        \n            Dr. Wainwright Old Gym    PSTAT 120A\n        \n        \n            Dr. Mouti      Old Gym    PSTAT 126 \n        \n    \n\n\n\nRun a cell containing only the code profs to make sure (visually) that your table looks correct.\n\nSelect the column called Course from profs.\nCreate a new table called profs_new that contains the same rows as the profs table, but with the following additional row:\n\n\n\n\n\n    \n        \n            Professor Office Course\n        \n    \n    \n        \n            Dr. Ravat South Hall PSTAT 120B\n        \n    \n\n\n\nRun a cell containing only the code profs_new to make sure (visually) that the appending was successful. Hint: think about how you can use our discussion on updating variable values from last lab. Also, the method .with_row() may be useful; see the help file at http://data8.org/datascience/tables.html for more information.\n\n\nSuppose we want to select rows of a table that satisfy a given condition. For example, if we wanted to find the information of only people who like Sprite in the table1 table above, we would call\n\ntable1.where(\"Fav_Drink\", \"Sprite\")\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n        \n            Amy  20343 Sprite   \n        \n    \n\n\n\nWhat would happen if we tried to select the rows of table1 with Coke in the Fav_Drink column? Well, since there is nobody in table1 that has coke as their favorite drink, we should hope that Python returns an empty table.\n\ntable1.where(\"Fav_Drink\", \"Coke\")\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n    \n\n\n\nSure enough, Python has returned an empty table!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#arrays",
    "href": "Pages/Labs/Lab02/lab02.html#arrays",
    "title": "Lab02",
    "section": "Arrays",
    "text": "Arrays\nThe final Data Structure we will examine in this class is that of an array. Arrays behave very similarly to Tables, with a few differences. For one, the syntax used to create an array is slightly different:\n\nmake_array(<item 1>, <item 2>, <item 3>, ...)\n\nFor example,\n\nmake_array(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\n\narray(['Spring', 'Summer', 'Autumn', 'Winter'],\n      dtype='<U6')\n\n\nYou may ask- what’s that dtype='<U6' symbol at the end of the output? For now, don’t worry about it, as we will revisit this later."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "href": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "title": "Lab02",
    "section": "Lists vs. Arrays",
    "text": "Lists vs. Arrays\nSo, we now know about three different data classes in Python: lists, tables, and arrays. At first glance, lists and arrays may seem somewhat similar. However, there are a few key differences between them:\n\n\n\n\n\n\nTask 6\n\n\n\nMake a list called my_list containing the elements 1, 2, and 3, and make an array called my_array also containing the elements 1, 2, and 3. Run the following commands in separate code cells:\n\nsum(my_list)\nsum(my_array)\nmy_list + 2\nmy_array + 2\n\n\n\nWhat the previous Task illustrates is the fact that arrays lend themselves to element-wise operations, whereas lists do not. One important limitation about arrays, though, is that the elements in an array must all be of the same data type. If you try to make an array consisting of elements that are different data types Python will still run, however it will not run in the way you expect it to!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#comparisons",
    "href": "Pages/Labs/Lab02/lab02.html#comparisons",
    "title": "Lab02",
    "section": "Comparisons",
    "text": "Comparisons\nHere’s a question: is 2 less than 3? Well, yes it is! If we wanted to confirm this, we could simply ask Python whether 2 is less than 3 by running\n\n2 < 3\n\nTrue\n\n\nNotice, however, how Python answered this question: it simply returned True. Let’s see what the data type of True is:\n\ntype(True)\n\nbool\n\n\nTrue is of the type bool, which is short for boolean. There are only two boolean quantities in Python: True and False. Let’s see how we can generate a False value:\n\n3 < 2\n\nFalse\n\n\nHere is a list of comparison operators, taken from the Inferential Thinking textbook:\n\n\n\nComparison\nOperator\nTrue Example\nFalse Example\n\n\n\n\nLess than\n<\n2 < 3\n2 < 2\n\n\nGreater than\n>\n3 < 2\n3 > 3\n\n\nLess than or equal\n<=\n2 <= 2\n3 <= 2\n\n\nGreater than or equal\n>=\n3 >= 3\n2 >= 3\n\n\nEqual\n==\n3 == 3\n3 == 2\n\n\nNot equal\n!=\n3 != 2\n2 != 2\n\n\n\nOne nice thing about Python is that it allows for multiple simultaneous comparisons. For example,\n\n2 < 3 < 4\n\nTrue\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a multiple comparison, Python will only return True when all of the included comparisons are true.\n\n\nFor instance, 2 < 3 < 1 would return False, because even though 2 is less than 3 it is not true that 3 is less than 1.\n\nBelieve it or not, you can compare strings as well! Python compares strings alphabetically; that is, letters at the beginning of the alphabet are considered to have smaller ordinal value than letters at the end of the alphabet. For example:\n\n\"apple\" < \"banana\"\n\nTrue\n\n\n\n\"zebra\" < \"zanzibar\"\n\nFalse\n\n\n\n\"cat\" <= \"catenary\"\n\nTrue\n\n\n\n\n\n\n\n\nTask 7\n\n\n\nCheck how \"statistics\" and \"Statistics\" (note the capitalization!) compare. Use this to answer the question: when Python is comparing strings, does it give precedence to capital letters or not? If so, which (lowercase or capital) is given a “higher” value?\n\n\nFinally, we discuss how comparisons work in the context of lists and arrays. The way Python compares lists is by what is known as lexicographical order. From the official Python help documentation, this means\n\nfirst the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted.\n\nFor instance, [1, 2, 3] < [2, 1, 1] would return True since 1 (the first element of the first list) is less than 2 (the first element of the second list).\n\nThe comparison of arrays is a little more straightforward, except\n\n\n\n\n\n\nImportant\n\n\n\nWhen comparing two arrays, the arrays must be of the same length.\n\n\nTo see exactly how comparison of arrays works, let’s work through a Task:\n\n\n\n\n\n\nTask 8\n\n\n\nMake an array with the elements 1, 2, and 3, and call this x. Make another array with the elements 2, 3, 1, and call this y. Run x < y, and comment on the result.\n\n\nWhat the previous task illustrates is that Python compares arrays element-wise."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#conditionals",
    "href": "Pages/Labs/Lab02/lab02.html#conditionals",
    "title": "Lab02",
    "section": "Conditionals",
    "text": "Conditionals\nNow, we can use comparisons for much more than verifying simple arithmetic relationships. One of the main areas in which comparisons arise is the area of conditional expressions.\n\nSimply put, conditional expressions are how we can convey a set of choices to Python. As an example, let’s consider finding someone’s city based on their zip code. To simplify, let’s assume the only zip codes we consider are 9311, 93120, and 93150. From postal data, we know that:\n\na zip code of 93117 corresponds to Goleta\na zip code of 93120 corresponds to Santa Barbara\na zip code of 93150 corresponds to Montecito\n\nWe can rephrase this information in terms of “if” statements:\n\nIf a person has a zip code of 93117, then they are in Goleta\nOtherwise, if they have a zip code of 93120, then they are in Santa Barbara\nOtherwise, if they have a zip code of 93150, then they are in Montecito\n\nThis is precisely the syntax we would use when translating this experiment into Python syntax:\n\nif zip_code == 93117:\n  location = \"Goleta\"\nelif zip_code == 93120:\n  location = \"Santa Barbara\"\nelif zip_code == 93150:\n  location = \"Montecito\"\n\nBy the way: elif is an abbreviation for else if, which itself can be thought of as equivalent to otherwise, if.\n\nHere’s the general syntax of a conditional expression in Python:\n\nif <condition 1>:\n  <task 1>\nelif <condition 2>:\n  <task 2>\n...\nelse:\n  <final task>\n\nWhen executing the above conditional statement, Python first checks whether <condition 1> returns a value of True or False. If it returns a value of True, then <task 1> is executed and the statement ends. Otherwise, Python checks whether <task 2> is True or False; if it is True then <condition 2> is executed, etc.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the example code above: if <condition 1> is True, then no tasks beyond <task 1> are evaluated. If <condition 2> is True, then no tasks beyond <task 2> are evaluated. And so on and so forth.\n\n\n\n\n\n\n\n\nTask 9\n\n\n\nConsider the code:\n\nx = 2\n\nif x < 2:\n    x = \"hello\"\nelif x < 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nBefore running any code, write down what you think the result of executing x would be. Then, run the loop, execute x, and check whether your answer was correct or not.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIndentation is very important in Python.\n\n\nFor example, if instead of the conditional expression in Task 2 we had instead put\n\nx = 2\n\nif x < 2:\nx = \"hello\"\nelif x < 3:\nx = \"goodbye\"\nelse:\nx = \"take care\"\n\nthen we would have received an error!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#functions",
    "href": "Pages/Labs/Lab02/lab02.html#functions",
    "title": "Lab02",
    "section": "Functions",
    "text": "Functions\nFinally, let’s quickly discuss Python functions. We’ve already been using quite a few functions:\n\n\n\n\n\n\nTask 10\n\n\n\nIn a Markdown cell, write down three functions we’ve used in Lab thus far.\n\n\nIf you recall, the general syntax for calling a function is:\n\n<function name>(<arg1>, <arg2>, ... )\n\nwhere <function name> denotes the function name and <arg1>, <arg2>, etc. denote the arguments of the function.\n\nCreating your own function in Python is actually fairly simple! Here is the syntax we use:\n\ndef <function name>(<list out the argument names>):\n  \"\"\"include a 'docstring' here\"\"\"\n  <body of the function>\n  return <what you want the function to output>\n\nFor example,\n\ndef f(x, y):\n  \"\"\"returns x^2 + y^2\"\"\"\n  return x**2 + y**2\n\ncreates a function f that can be called on two arguments, x and y, and returns the sum of squares of the arguments; e.g.\n\nf(3, 4)  # should return 3^2 + 4^2 = 25\n\n25\n\n\nBy the way, the docstring referenced above is a verbal description of what the function does. (Recall from Lab01 that it is just a multi-line comment, since it is enclosed in triple quotation marks!). All functions should include a docstring to convey to the user what the function does.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t include a return statement in the definition of a function, then your function will never return anything.\n\n\nFor instance,\n\ndef g(x, y):\n  \"\"\"should return x^2 + y^2\"\"\"\n  x**2 + y**2\n\ng(3, 4)\n\n\n\n\n\n\n\nTask 11\n\n\n\nWrite a function called cent_to_far() which takes in a single temperature c as measured in degrees Centigrade and returns the corresponding temperature in degrees Farenheit. Check that cent_to_far(0) correctly returns 0 and cent_to_far(68) correctly returns 69.7777. As a reminder: \\[ {}^{\\circ}\\mathrm{F} = \\frac{5}{9} ({}^{\\circ}\\mathrm{C}) + 32 \\]\n\n\nFinally, let’s combine some things by way of a concluding Task:\n\n\n\n\n\n\nTask 12\n\n\n\nWrite a function called parity() that returns the parity (i.e. whether a number is even or odd) of an input x. Call your parity() function on 2 and then 3 to make sure your function behaves as expected. Some hints:\n\nRecall that % is the modulus operator in Python. Specifically, x % y returns the remainder of performing y divided by x.\nRecall that even numbers are divisible by 2 (so what does this mean about the remainder of dividing x by 2 if x is even?)"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html",
    "href": "Pages/Labs/Lab01/lab01.html",
    "title": "Lab01",
    "section": "",
    "text": "Welcome to the first PSTAT 5A Computing Lab! As we will soon learn, computers play an integral part in effectively and efficiently performing statistical analyses. The primary goal of these Computing Labs is to develop the skills to be able to communicate with computers, and also learn the basic principles and language of programming."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#structure-of-labs",
    "href": "Pages/Labs/Lab01/lab01.html#structure-of-labs",
    "title": "Lab01",
    "section": "Structure of Labs",
    "text": "Structure of Labs\nEvery week we (the course staff) will publish a lab document, which is intended to be completed during your Lab Section (i.e. your first Section) of the week. Each lab document will consist of a combination of text, tips, and the occasional task for you to complete based on the text provided. Your TA will cover exactly what you need to turn in at the end of each lab in order to receive credit, but you should read all lab material carefully and thoroughly as content from labs will appear on quizzes and exams."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#what-is-programming",
    "href": "Pages/Labs/Lab01/lab01.html#what-is-programming",
    "title": "Lab01",
    "section": "What Is Programming?",
    "text": "What Is Programming?\nComputers, though incredibly useful, are fairly complex machines. To communicate with them, we need to use a specific language, known as a Programming Language. There are a number of programming languages currently in use, with names such as R, Julia, MatLab, and - the language we will use for this course - Python.\n\nPython programs can be written in a number of different environments, such as a text editor (e.g. Notepad, VS Code, etc.) or a Terminal window. For this class, we will use Jupyter Notebook (where Jupyter is pronounced like the planet), an interactive environment that has the added benefit of being hosted online meaning you do not have to download anything onto your personal machines in order to run Python code!"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#getting-started",
    "href": "Pages/Labs/Lab01/lab01.html#getting-started",
    "title": "Lab01",
    "section": "Getting Started",
    "text": "Getting Started\n\nNavigate to https://pstat5a.lsit.ucsb.edu\nClick the “Sign in with your UCSB NetID” button, and sign in.\nUnder “Notebook”, click “Python 3 (ipykernel)” (see below).\n\n\nCongratulations- you have just made your first Jupyter notebook! Now, it’s time for our first task:\n\n\n\n\n\n\nTask 1\n\n\n\nChange the name of your notebook to “Lab01” using the following steps:\n\nIn the lefthand menu bar, find the notebook you just created (by default this will be something like “Untitled” or “Untitled1”), and right-click and click “Rename” (see picture below)\n\n\n\nRename your file to “Lab01”, and then hit the return (enter) key on your keyboard. You should see the filename in the menubar update:\n\n\n\n\n\nJupyterHub Environment\nLet’s take a minute to familiarize ourselves with the JupyterHub environment. Every Jupyter notebook is comprised of what are known as cells; these are the shaded grey rectangles that appear in a Jupyter notebook.\n\nIf your cell has a grey background (like in the image above), it is inactive. To activate a cell, place your cursor inside it, and click:\n\nmeans it is selected and active, and ready to be populated with text and/or code.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you run code using the “Run” button at the top of your environment, only the active cell will be executed.\n\n\n\n\nCells\nThere are two main types of cells we will be using in this class: Markdown cells (which include text/descriptions, but no code) and code cells (which contain code that needs to be run). We’ll be talking a bit more about Markdown cells in a few weeks.\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf you haven’t already, click into the code cell that was automatically created when you created your document to activate it.\nClick on the dropdown menu that currently says “code” (near the center of the top of your interface), and select “Markdown”\n\n\n\nClick back into the cell, copy-paste the text [including the hashtag!] # Task 2, and then run the cell by clicking on the button that looks like a “play” symbol at the top of your window:\n\n\n\nNote that after running your cell from step 3 above, Jupyter automatically created a new code cell. Click into this code cell and run the code 2 + 2.\n\nWhen you are done, your notebook should look something like this:\n\n\n\nNotice that after running a cell, Jupyter automatically adds a new cell right after it!\n\n\n\n\n\n\nTip\n\n\n\nTo run a cell and automatically create a new cell underneath it, use the keyboard shortcut SHIFT + ENTER.\n\n\nBy the way, do you notice the little In [1]: at the left of our first cell? This is Jupyter’s way of letting us know the order in which the code cells have been executed. The 1 in our cell from Task 2 above corresponds to the fact that this was the 1st code cell we executed in our document."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#coding-with-python",
    "href": "Pages/Labs/Lab01/lab01.html#coding-with-python",
    "title": "Lab01",
    "section": "Coding with Python",
    "text": "Coding with Python\nThere is a reason we use the word “language” to describe programming languages- that is because they function quite like a human language. This means that they each have their own syntax (i.e. set of grammar rules). It is precisely the syntax of the Python language that we will be learning over the course of these Computing Labs!\n\nPrograms are made up of expressions, like 2 + 2. We evaluate expressions by running (or executing) them in a programming language. Expressions are like the sentences of programming- they contain complex pieces of information that are conveyed between the user and the computer.\n\nMuch like sentences in other languages, expressions must obey a rigid syntax. For example, when we want to perform addition in Python we must use the + symbol; we can’t, for example, say 2 plus 2.\n\nWhat happens when we violate a syntax rule? Well…\n\n\n\n\n\n\nTask 3\n\n\n\n\nCreate a mardown cell and write # Task 3\nCreate a code cell, and run 2 plus 2. (You should get an error!)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this class, we expect you to precede each code cell from a particular task with a markdown cell that says # Task X (where X is the number of the task).\n\nWe will stop explicitly writing this step in the tasks below, but you are still expected to include a labeling cell!\n\n\nWell, what is this error saying? Let’s examine it more closely.\n\n  File \"<ipython-input-2-5196071441ec>\", line 1\n    2 plus 2\n      ^\nSyntaxError: invalid syntax\n\nIndeed, Python is telling us exactly what went wrong- the SyntaxError part of the error message tells us that we violated one of the syntax rules of Python, and the ^ pointing to the p in plus is telling us that the exact syntax error occurred when we tried to use the word plus.\n\n\n\n\n\n\nTip\n\n\n\nAlways read error messages!\n\n\nThe messages that Python displays when we get an error are Python’s way of trying to communicate with us what is going wrong!"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#python-as-a-calculator",
    "href": "Pages/Labs/Lab01/lab01.html#python-as-a-calculator",
    "title": "Lab01",
    "section": "Python as a Calculator",
    "text": "Python as a Calculator\nAlright, let’s get our hands dirty with some real programming! One of the many uses of Python is to help us compute arithmetic quantities very quickly. As a rule-of-thumb, Python adheres to the order of operations:\n\nParentheses\nExponents\nMultiplication\nDivision\nAddition\nSubtraction\n\nHere is a list of mathematical operators and their corresponding Python syntax:\n\n\n\nOperation\nPython Operator\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1\n\n\nExponentiation\n**\n2 ** 2\n4\n\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nCompute the following:\n\n\\(\\displaystyle \\frac{2 + 3}{4 + 5^6}\\)\n\\(\\displaystyle (1 - 3 \\cdot 4^5)^{6}\\)\n\n\n\nNaturally, Python is capable of much more than just basic arithmetic!\n\n\n\n\n\n\nTask 5\n\n\n\nCreate a code chunk and run sin(1) to compute the sine of 1.\n\n\nUh-oh- looks like we’ve encountered another error! Indeed, even the most experience coder will often run up against errors like this, and need to subsequently enter the stage of debugging their code.\n\nWe’re now getting a new error: this time, it’s a NameError. As the name suggests, this is Python’s way of telling us that it doesn’t recognize the name of something we’ve written. In fact, it’s explicitly saying:\n\nNameError: name 'sin' is not defined,\n\nSpecifically, Python is telling us that it (somehow) doesn’t know what sin means. Why? Well, to answer that, we need to take a bit of a detour into the world of modules."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#python-modules",
    "href": "Pages/Labs/Lab01/lab01.html#python-modules",
    "title": "Lab01",
    "section": "Python Modules",
    "text": "Python Modules\nIt is important to note that all Python objects take up space in the form of memory (i.e. storage space on your computer). Nowadays, with recent innovations in computers and computer memory, this is not so much of an issue but historically, when many of these programming languages were first being created, optimizing space was of the utmost concern. (Even today, efficiency is a guiding tenet of most programmers!)\n\nThink of it this way- if you are doing work on code that doesn’t involve much trigonometry, there isn’t a whole lot of need to have the sin function readily available. The idea programmers had was to compartmentalize, and store certain functions in what are known as modules.\n\nModules are Python files containing definitions for functions and classes (we’ll talk about data classes a little later). While data types and built-in functions in the Python standard library are available for immediate use, modules need to be imported first.\n\nThe syntax for importing all functions from a module is:\n\nfrom <module name> import *\n\nSometimes, we may not want to import the entirety of a module and instead import only a couple of functions from that module. In that case, we would use the syntax:\n\nfrom <module name> import <function name>\n\nWe’ll talk a bit more about modules in a future lab. For now, let’s return to our task of computing \\(\\sin(1)\\).\n\n\n\n\n\n\nTask 5 (cont’d)\n\n\n\nIt turns out that the sin() function is located in the math module Load all functions from the math module, and then try re-running sin(1)."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#functions",
    "href": "Pages/Labs/Lab01/lab01.html#functions",
    "title": "Lab01",
    "section": "Functions",
    "text": "Functions\nWe will talk extensively about Python functions in a few weeks. For now, suffice it to say that Python functions work just like mathematical functions: for example, note how we used the sin() function in the previous task. One piece of terminology that is somewhat specific to programming is the notion of calling- when we say we call a function on an argument, we mean that we’re passing that argument through the function. So, for example, in Task 5 we called the sin() function on the argument 1."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#variable-assignment",
    "href": "Pages/Labs/Lab01/lab01.html#variable-assignment",
    "title": "Lab01",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nLet’s talk a bit about variables. Just like in math, variables in a programming language refer to a placeholder name for a particular piece of information (be it a function, value, etc.) The act of storing information in a variable is called assignment, and in Python variable assignment is performed using the = symbol.\n\n<variable name> = <what you want to associate with the variable>\n\nFor example, after running\n\nx = 2\n\nthe quantity x will always be synonymous with the quantity 2, and running x + 2 will return a value of 4 (as 2 + 2 = 4).\n\nPython affords a lot of flexibility when it comes to variable names- that is, we can pick almost anything we want to be a variable name! There are, however, some exceptions:\n\nVariable names cannot start with a number\nVariable names cannot include a space\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good programming practice to give your variables names that are descriptive, but not overly long.\n\n\nIf we want to view the value stored in a variable, we have two options: we could simply type the name of the variable, and run the cell:\n\nx\n\n2\n\n\nor we could pass the variable name into a call to the print() function:\n\nprint(x)\n\n2\n\n\n\n\n\n\n\n\nTask 6\n\n\n\n(a) Define a variable called my_variable, and assign it the value 5.\n(b) Now, run the command print(My_variable) (note the capitalization!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython is case-sensitive.\n\n\nSometimes it will be necessary to update or re-assign a new value to an existing variable. For example, let’s examine the structure of the following code:\n\nx = 2\nx = x + 3\n\nWhat do you think running x will return? If you said 5, you’d be correct! The key point of this is:\n\n\n\n\n\n\nImportant\n\n\n\nIn variable assignment, Python starts by executing the righthand side of the equality before executing the lefthand side.\n\n\nSo, in code example above, Python first executed x + 3 (which is equivalent to 2 + 3; i.e. 5), and then re-assigned x the value 5."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#comments",
    "href": "Pages/Labs/Lab01/lab01.html#comments",
    "title": "Lab01",
    "section": "Comments",
    "text": "Comments\nWhen writing large pieces of code, programmers will often utilize comments to annotate their work and help readers understand what their code is doing. In Python there are two types of comments: inline comments and multiline comments. Inline comments are created using the hashtag (#) and multiline comments must be enclosed in three quotation marks (\"\"\"). As an example of both, consider the following snippet of code:\n\nx = 1             # define x\ny = 2             # define y\nz = (x + y) ** 2  # define z\ny = z / 3         # redefine y\n\n\"\"\"This code is defines 3 variables,\ncalled 'x', 'y', and 'z'.\"\"\"\n\n\n\n\n\n\n\nTask 7\n\n\n\nGo back and add some descriptive comments to some of your previous code cells. (You don’t need a separate markdown cell indicating you have done so.)"
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#data-types",
    "href": "Pages/Labs/Lab01/lab01.html#data-types",
    "title": "Lab01",
    "section": "Data Types",
    "text": "Data Types\nBefore closing out this lab, we should talk a bit about the quantities we assign to variables- i.e. the different data types in Python.\n\nThe term data type loosely refers to the actual type of a particular quantity (e.g. numerical, character, etc.) The main data types we will discuss in this lab are:\n\nfloat: refers to numerical (real-valued) quantities\nint: short for integer; refers to numerical quantities that are integers\nstr: short for string; refers to character- or text-type data (and will always be enclosed in either single quotation marks or double quotation marks)\n\n\n\n\n\n\n\nTask 8\n\n\n\nRun each of the following:\n\ntype(1)\ntype(1.1)\ntype(\"hello\")\n\n\n\nLet’s combine our knowledge of variable assignment with our newfound knowledge of data types!\n\n\n\n\n\n\nTask 9\n\n\n\n(a) Perform the following variable assignments:\n\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\n\n(c) A new section has been added! Update the variable num_sections to be one more than when you initially defined it above. (Don’t just use num_sections = 5- think about our discussion on updating variables above!)\n(b) Using comments, write down what you think the output of each of the following expressions will be:\n\ntype(course)\ntype(num_sections)\nnum_sections * section_capacity\n\nThen, run each expression in a separate code chunk and comment on the results.\n(c) Create a new variable called course_capacity and assign it the value of the maximum capacity of the course. (Hint: there are only 5 sections, and each section has a maximum capacity of 25. Try to use your already-defined variables as much as possible!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe type() function can be used to identify the data type of a particular quantity."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#final-formatting",
    "href": "Pages/Labs/Lab01/lab01.html#final-formatting",
    "title": "Lab01",
    "section": "Final Formatting",
    "text": "Final Formatting\nIt’s time to start adding the finishing touches to our first lab!\n\n\n\n\n\n\nTask 10\n\n\n\n\nClick on the gear-shaped icon in the top-right of your console:\n\n\n\nScroll down until you see the Notebook Metadata:\n\n\n\nRight after the second-to-last brace (}), add a comma , and then the following code:\n\n\n\"authors\": [\n        {\n            \"name\": \"<YOUR NAME>\"\n        },\n        {\n            \"name\": \"<YOUR NETID>\"\n        }\n    ]\n\nwhere you replace <YOUR NAME> and <YOUR NETID> with your name and NetID, respectively. For example, after performing the above steps, my Notebook Metadata would look like:\n\nIMPORTANT: If your NetID contains an underscore, e.g. my_netid, then you need to replace the underscore with the text \\\\textunderscore (with the two backslashes at the beginning) followed by a space. Otherwise, your Lab will not convert to a PDF properly. So, for example, if your NetID is my_netid then you should write your NetID as my\\\\textunderscore netid. Your TA will go over how to update your Notebook Metadata in the last few minutes of Lab."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html#what-to-turn-in",
    "href": "Pages/Labs/Lab01/lab01.html#what-to-turn-in",
    "title": "Lab01",
    "section": "What to Turn In",
    "text": "What to Turn In\nCongrats on finishing the first PSTAT 5A Computing Lab! Here’s what you need to submit:\n\nYour downloaded .ipynb file\nYour downloaded .PDF file\n\n(Please consult the video on Canvas showing you how to upload your work to Gradescope). Toward the end of Lab, your TA will show you how to download the above files. You will have until 40 minutes after the end of your Section (e.g. if your Section ends at 1:20pm, then you have until 2:00pm to submit) to turn in your work in order to get credit for the lab. Also, please remember that you need to upload both a .ipynb file and a .pdf. We will be grading labs based on effort, so just turn in what you are able to!"
  },
  {
    "objectID": "Pages/course_staff.html",
    "href": "Pages/course_staff.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Instructor: Ethan P. Marzban\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthan P. Marzban\n\n\nHello! I currently just finished the 3rd year of my PhD program here in the PSTAT department, having joined back in 2020 (after having completed my undergraduate degree in Statistics as well). Outside of school I enjoy playing the piano, drinking boba, and talking about cats!\n\n\n\n\n\nEmail:\n\n\nepmarzban@pstat.ucsb.edu (Please use only in case of emergency)\n\n\n\n\nOH:\n\n\nTuesdays 4:30 - 5:30pm, over Zoom  Thursdays 2:30 - 3:30pm, in SH 5607F (the Sobel Seminar Hall)"
  },
  {
    "objectID": "Pages/course_staff.html#teaching-assistants-tas",
    "href": "Pages/course_staff.html#teaching-assistants-tas",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\n\n\n\n\n\n\nTA: Mengrui Zhang\n\n\n\n\n\n\n\n\n\n\n\n\n\nMengrui Zhang\n\n\nMengrui is a PhD Student in the PSTAT Department.\n\n\n\n\n\nEmail:\n\n\nmengrui@ucsb.edu\n\n\n\n\nOH:\n\n\nMW, 4 - 6pm (SH 5431 W)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTA: Olivier Mulkin\n\n\n\n\n\n\n\n\n\n\n\n\n\nOlivier Mulkin\n\n\nOlivier is a PhD Student in the PSTAT Department.\n\n\n\n\n\nEmail:\n\n\nomulkin@ucsb.edu\n\n\n\n\nOH:\n\n\nT, 1 - 3pm (Zoom) Starts in Week 2"
  },
  {
    "objectID": "Pages/exam_prep.html",
    "href": "Pages/exam_prep.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Information Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams:\n\nBlue Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 13(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nYellow Version: Blank Solns\n\nTypo in Solutions: In 12(b), the \\(\\frac{1}{5 - 1}\\) should be \\(\\frac{1}{4 - 1}\\)\nTypo in Solutions: Answer to 14(d) should be \\(A = \\{ (1, -1, \\ \\ 1), \\ (1, -1, -1)\\}\\)\n\nPlease note that the format of exams this quarter will be slightly different; see the coverpages below\n\nExam Coverpages: Multiple Choice Free Response\nFormula Sheet: .pdf\nPractice Problems: .pdf Solns"
  },
  {
    "objectID": "Pages/exam_prep.html#midterm-2",
    "href": "Pages/exam_prep.html#midterm-2",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Midterm 2",
    "text": "Midterm 2\n\nInformation Document: .pdf\nList of Topics (not comprehensive): .pdf\nSpring 2023 Exams (Solutions will be posted on Wednesday, July 19, 2023):\n\nSalmon Version: Blank\nYellow Version: Blank\n\nExam Coverpages: Multiple Choice Free Response\nFormula Sheet: .pdf\nPractice Problems: .pdf"
  },
  {
    "objectID": "Pages/syllabus.html",
    "href": "Pages/syllabus.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "WELCOME TO PSTAT 5A! I am very excited to introduce to you the wonderful fields of Statistics and Data Science. As our world becomes ever more saturated with data, the need for data literacy becomes increasingly important. By the end of this course, I hope you will be able to think critically about statistical studies and results, understand how data can be used to simultaneously inform and manipulate, and begin applying your newfound techniques to your future endeavors. I am very much looking forward to a great quarter with all of you!\n— Ethan"
  },
  {
    "objectID": "Pages/syllabus.html#lecture-information",
    "href": "Pages/syllabus.html#lecture-information",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Lecture Information",
    "text": "Lecture Information\n\n\n\n\n\n\nLecture Times and Locations\n\n\n\nM, T, W, Th: 11:00am - 12:20pm in ILP 1101"
  },
  {
    "objectID": "Pages/syllabus.html#course-staff",
    "href": "Pages/syllabus.html#course-staff",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\n\n\n\n\n\n\nInstructor:\nEthan P. Marzban\n\n\n\n\nEmail:\nepmarzban@pstat.ucsb.edu (Please use only in case of emergency)\n\n\nHelp Hours:\n\nTuesdays: 4:30pm - 5:30pm (Zoom)\nThursdays: 2:30pm - 3:30pm (SH 5607 F)\n\n\n\n\n\n\n\n\n\n\n\n\nTAs\n\n\n\n\nMengrui Zhang\n(mengrui@ucsb.edu)\n\n\nOlivier Mulkin\n(omulkin@ucsb.edu)"
  },
  {
    "objectID": "Pages/syllabus.html#schedule-of-sections",
    "href": "Pages/syllabus.html#schedule-of-sections",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Schedule of Sections",
    "text": "Schedule of Sections\n\n\n\nTimes\nTA\nLocation\n\n\n\n\nMW 12:30 - 1:20pm\nOlivier Mulkin\nPHELPS 1513\n\n\nMW 2:00 - 2:50pm\nMengrui Zhang\nPHELPS 1513\n\n\nMW 3:00 - 3:50pm\nMengrui Zhang\nPHELPS 1513"
  },
  {
    "objectID": "Pages/syllabus.html#course-description",
    "href": "Pages/syllabus.html#course-description",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Description",
    "text": "Course Description\nThe official description of this course, from the Course Catalog, is:\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required."
  },
  {
    "objectID": "Pages/syllabus.html#textbooks",
    "href": "Pages/syllabus.html#textbooks",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Textbook(s)",
    "text": "Textbook(s)\nThis quarter, we do not have a required textbook- the lecture slides and lab activities are designed to be self-sufficient. However, the following textbooks are highly recommended:\n\nOpenIntro: Statistics. David Diez, Mine Çetinkaya-Rundel, and Christopher D Barr. (free version, courtesy of the authors, available at https://leanpub.com/os)\nComputational and Inferential Thinking: The Foundations of Data Science. Ani Adhikari and John DeNero. (available at: https://www.inferentialthinking.com)\nStatClass (2nd Edition, Revised). Dawn E. Holmes and Lubella A. Lenaburg"
  },
  {
    "objectID": "Pages/syllabus.html#course-components",
    "href": "Pages/syllabus.html#course-components",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Components",
    "text": "Course Components\nThe following are the assignments and metrics that will be used to compute your final grade in this course:\n\nLabs\nEvery Monday you will work through a Lab worksheet that will be posted to the website by the previous day. You will then have until 40 minutes after the end of your scheduled section to submit your work through Gradescope. Your TA will explain more about the structure of lab during your first Section meeting of the quarter.\n\n\nQuizzes\nQuizzes will be administered asynchronously on Fridays, through Gradescope. Specifically, the quiz will remain open from 10am until 11:59pm, and you must find 40 consecutive minutes to take the quiz. (Consecutive means you cannot start the quiz, and then come back to it later- once you start, you will have 40 minutes to both complete the quiz as well as upload your work). I encourage you to spend 25 minutes working on the quiz, and 15 minutes uploading. No quiz scores will be dropped. There are no quizzes in Exam Weeks.\n\n\nExams\nThere are two midterms and a final exam for this class. You are required to take all three exams; failure to do so will result in an automatic grade of “F”, so please ensure you are able to take the exams on the dates listed below.\n\n\n\n\n\n\nExam Dates:\n\n\n\n\nMidterm 1 is scheduled to take place Thursday July 6 from 11:00am - 12:20pm (lecture time)\nMidterm 2 is scheduled to take place Thursday July 20 from 11:00am - 12:20pm (lecture time)\nThe final is scheduled to take place FRIDAY, AUGUST 4 from 4 - 7pm\n\n\n\nAll exams are scheduled to take place in our lecture classroom, ILP 1101. Unless stated otherwise, all exams will be cumulative.\n\n\nHomework:\nHomework for this class will be neither collected nor graded. However, the weekly quizzes will be based heavily (though potentially not entirely) off the homework assignments. Homework assignments will be released on Mondays, along with their final answers. Solutions will be posted biweekly, on the Tuesdays before exams. As such, you should not rely on the solutions to help you study; instead, please come to office hours to check your work regularly!\n\n\nSchedule of Due Dates\nA tentative schedule of release and due dates can be found here"
  },
  {
    "objectID": "Pages/syllabus.html#grading-scheme",
    "href": "Pages/syllabus.html#grading-scheme",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Grading Scheme",
    "text": "Grading Scheme\nYour final grade will be computed using the following weights:\n\n\n\nLabs:\n10%\n\n\nQuizzes:\n15%\n\n\nMidterm 1\n20%\n\n\nMidterm 2\n20%\n\n\nFinal Examination:\n35%\n\n\n\nPlease note that late submissions for any of the above will not be accepted. Additionally, make-up exams cannot be accommodated.\nYour final letter grade will be issued according to the following scheme (cutoffs between plusses and minuses will be calculated at the end of the quarter):\n\nA– – A+: 90 – 100%\nB– – B+: 80 – 89.99%\nC– – C+: 70 – 79.99%\nD– – D+ : 60 – 69.99%\nF: 0 – 59.99%\n\nI have elected to adopt an uncurved grading scheme to eliminate any sense of “competition” among students; I highly encourage you all to collaborate with and uplift each other. Having said that, I will certainly consider adjusting the cutoffs (naturally, in everyone’s favor) at the end of the quarter if necessary."
  },
  {
    "objectID": "Pages/syllabus.html#academic-integrity",
    "href": "Pages/syllabus.html#academic-integrity",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a member of the UCSB community, it is expected that you will act with academic integrity. This means, among other things, that the work you submit should be entirely your own and not copied from any external sources. Collaboration on non-quiz and non-exam assignments is perfectly acceptable (and even encouraged), but the work you submit should still be your own; you can’t have someone else write up solutions for you.\nAnyone found guilty of academic misconduct will be reported to the Academic Senate, and will receive at minimum a failing grade on the assignment in question; further actions may also include failing the course, and marks being made on permanent records. Depending on the severity of the infraction, expulsion is also a possibility.\nBasically, don’t cheat- please! If you’re ever struggling with course material, please come talk to me or the TA’s. We are truly here for you, and want only the best for you."
  },
  {
    "objectID": "Pages/syllabus.html#intellectual-property",
    "href": "Pages/syllabus.html#intellectual-property",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Intellectual Property",
    "text": "Intellectual Property\nYou’ve probably seen a clause on other syllabi stating something to the effect of “all material in this course is the intellectual property of myself and may not be shared with anyone outside this class without my explicit written permission.”\nThough this does still hold true for this course, I will be making most course-related material available on a public GitHub site, which can be accessed here: https://pstat5a.github.io."
  },
  {
    "objectID": "Pages/syllabus.html#disabled-students-program-dsp",
    "href": "Pages/syllabus.html#disabled-students-program-dsp",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disabled Students Program (DSP)",
    "text": "Disabled Students Program (DSP)\nIf you have a disability, or otherwise require accommodations for the exams and/or quizzes please reach out to the Disabled Students Program (DSP) ASAP to ensure your request(s) for accommodation can be processed. We ask that all requests be logged at least a week in advance, to ensure the system enough time to process. Please note that we cannot grant any requests for accommodations unless they come to us from DSP directly."
  },
  {
    "objectID": "Pages/syllabus.html#technology-needs",
    "href": "Pages/syllabus.html#technology-needs",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Technology Needs",
    "text": "Technology Needs\nAs a part of this course, you will be required to program in Python. Though the Lab Sections take place in specially designed classrooms that come equipped with computers, your homework and quizzes may cover Python-related questions, which means we expect you to have access to a laptop capable of connecting to the internet. If you do not currently possess such a laptop, please check out UCSB’s Basic Needs Resource page on Technology Resources to try and acquire one."
  },
  {
    "objectID": "Pages/syllabus.html#section-switching",
    "href": "Pages/syllabus.html#section-switching",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Section Switching",
    "text": "Section Switching\nAs mentioned above, Sections (both Discussion and Lab) take place in special “Collaborate Classrooms” which are equipped with laptops. There are a fixed number of seats and laptops in these classrooms, meaning we cannot under any circumstance over-enroll sections. Therefore, if you want to switch section unofficially (we do not have the ability to switch your official enrollment through GOLD), please follow the steps at this link. Any requests to switch sections that do not adhere to the guidelines posted at that link will be ignored."
  },
  {
    "objectID": "Pages/syllabus.html#email-policy",
    "href": "Pages/syllabus.html#email-policy",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Email Policy",
    "text": "Email Policy\nPlease note that we have a no-email (including Canvas messages) policy, except for emergencies; I leave it to you to decide what’s an ‘emergency’. Please bring all of your questions to me during Office Hours or at the end of lectures. (Also, please note that DMs [Direct Messages] sent to the course staff through Discord will be ignored.) Thank you!"
  },
  {
    "objectID": "Pages/syllabus.html#disclaimer",
    "href": "Pages/syllabus.html#disclaimer",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe instructor reserves the right to modify this syllabus if he deems such modifications academically advisable. Such modifications, should they occur, will be announced publicly."
  },
  {
    "objectID": "Pages/hw.html",
    "href": "Pages/hw.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Here you will find all homework assignments and their solutions. Please remember that solutions to homework will only be released during Exam Weeks; we encourage you to come to Office Hours to ask any questions you may have about the assignments!\n\nHomework 1: Blank Answers Solns \nHomework 2: Blank Answers Solns \nHomework 3: Blank Answers\nHomework 4: Blank Answers"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "href": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we discussed the basics of probability.\n\nThese included things like: experiments, outcome spaces, events, and probability.\n\nWe briefly talked about the different approaches to defining the probability of an even.\nFinally, we also saw how Venn Diagrams can help visualize set relationships and operations."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "href": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Leadup",
    "text": "Leadup\n\nNow, let’s return to the classical approach to probability.\nAssuming the outcomes in our outcome space \\(\\Omega\\) are equally likely, the classical approach tells us to compute the probability of any event \\(E\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of elements in $\\Omega$}} \\]\nUp until now, we’ve computed both the numerator and the denominator by explicitly listing out the elements contained in the respective sets, and then counting the number of elements.\nThis works decently for small sets, but is highly inefficient for large sets."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ice Cream",
    "text": "Ice Cream\n\nBefore diving fully into the principles of counting, let’s examine a simple situation.\nSuppose we are at a small boutique ice cream parlor that offers only 3 flavors (Vanilla, Chocolate, and Matcha), and 2 toppings (sprinkles or coconut).\n\nFurther suppose that an order of ice cream must contain only 1 flavor and 1 topping.\n\nWe can list out the different orders that are possible (i.e. the outcome space of the experiment of ordering an ice cream from this shop) using a tree diagram:"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "href": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Fundamental Principle of Counting",
    "text": "Fundamental Principle of Counting\n\n\nThis is no accident!\n\n\n\n\n\n\n\n\n\n\nFundamental Principle of Counting\n\n\n\nIf an experiment consists of \\(k\\) stages, where the \\(i\\)th stage has \\(n_i\\) possible configurations, then the total number of elements in the outcome space is \\[ n_1 \\times n_2 \\times \\cdots \\times n_k \\]\n\n\n\n\n\n\n\nSo, when we obtained our answer of \\(6\\) on the previous slide, we were implicitly using the Fundamental Principle of Counting with 2 stages (picking a flavor, and picking a topping) where the first stage (picking a flavor) had 3 possible configurations (Vanilla, Chocolate, or Matcha) and the second stage (picking a topping) had two possible configurations (sprinkles or coconut)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "href": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Slot Diagrams",
    "text": "Slot Diagrams\n\nWhen dealing with the Fundamental Principle of Counting, I find it useful to utilize what are sometimes referred to as slot diagrams.\nHere’s how we use slot diagrams:\n\nFirst put down as many slots as there are stages in our experiment: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ }  \\]\nThen, fill in each slot with the corresponding number of configurations: \\[ \\underline{\\ n_1 \\ } \\ \\ \\  \\underline{\\ n_2 \\ } \\ \\ \\ \\cdots \\ \\ \\  \\underline{\\ n_k \\ }  \\]\nFinally, invoke the Fundamental Principle of Counting to multiply the slots together: \\[ \\underline{\\ n_1 \\ } \\ \\times \\  \\underline{\\ n_2 \\ } \\ \\times \\ \\cdots \\ \\times  \\underline{\\ n_k \\ }\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a (different) ice cream parlor has 32 flavors, 5 toppings, and 3 drizzles. If a “scoop” consists of a flavor, topping, and drizzle, how many scoops can be created?\n\n\n\n\n\n\nThere are 3 stages: picking a flavor, picking a topping, and picking a drizzle. Hence, we draw three slots: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\]\nThe first stage has 32 configurations, the second has 5, and the third has 3: \\[ \\underline{\\ 32 \\ } \\ \\ \\ \\underline{\\ 5 \\ } \\ \\ \\ \\underline{\\ 3 \\ } \\]\nFinally, we multiply through: \\[ \\underline{\\ 32 \\ } \\ \\times \\ \\underline{\\ 5 \\ } \\ \\times \\ \\underline{\\ 3 \\ } = \\boxed{480} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ordering",
    "text": "Ordering\n\nHere’s a question: given \\(n\\) tickets (labeled \\(1\\) through \\(n\\)), how many different ways are there to arrange them in a line?\nLet’s answer this using a slot diagram!\nWe can think of \\(n\\) stages, where the first stage corresponds to placing the first ticket down, the second stage corresponds to placing the second ticket down, and so on and so forth. \\[ \\underbrace{ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } }_{\\text{$n$ slots}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "href": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Factorials",
    "text": "Factorials\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\), we define \\(n\\) factorial (denoted \\(n!\\)), to be \\[ n! = n \\times (n - 1) \\times (n - 2) \\times \\cdots \\times 2 \\times 1 \\]\n\n\n\n\n\n\nFor example:\n\n\\(3! = 3 \\times 2 \\times 1 = 6\\)\n\\(4! = 4 \\times 3 \\times 2 \\times 1 = 24\\)\n\\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSiobhan has 4 shirts in her closet: 2 purple shirts and 2 red shirts. When organizing these shirts in her closet she wants to keep the purple shirts together and the red shirts together, but doesn’t care if the purple group is to the left or the right of the red group. How many ways are there for Siobhan to arrange these shirts in her closet?\n\n\n\n\n\n\nLet’s answer this question two ways: using direct enumeration, and then using counting techniques.\nLabel the two purple shirts \\(P_1\\) and \\(P_2\\) respectively, and label the two red shirts \\(R_1\\) and \\(R_2\\) respectively. Then here are all of the possible reorderings of the shirts:\n\n\n\\[\\begin{align*}\n  \\Omega = \\{ & P_1 P_2 R_1 R_2, \\ P_1 P_2 R_2 R_1, \\ P_2 P_1 R_1 R_2, \\ P_2 P_1 R_2 R_1 , \\\\\n  & R_1 R_2 P_1 P_2, \\ R_2 R_1P_1 P_2, \\ R_1 R_2 P_2 P_1, \\  R_2 R_1P_2 P_1 \\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\nLet’s now consider a slightly more abstract experiment: consider drawing two tickets from a box with tickets labeled \\(A\\) through \\(C\\), not replacing my first ticket after I draw it.\n\nHow many elements are in the outcome space of this experiment?\n\nWell, the answer is…. it depends!\nSpecifically, we need to know: does order matter?\nHere’s what I mean by order mattering: in a license plate, 123ABC and ABC123 are clearly two different license plates, despite the fact that they are comprised of the same letters and numbers!\n\nAn example of a situation in which order does not matter is drawing cards from a deck of cards: whether I get the Ace of Hearts before or after the King of Diamonds doesn’t matter- all that mattes is that I have the Ace of Hearts and the King of Diamonds!\nSpeaking of cards, you’ll discuss playing cards a bit more on future assignments."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "If Order Does Matter",
    "text": "If Order Does Matter\n\nLet’s examine what happens when we assume order does matter.\nIn the context of our drawing tickets example, this means that getting \\(A\\) followed by \\(C\\) is different than getting \\(C\\) followed by \\(A\\).\nThen, letting \\((X, Y)\\) denote the outcome “I drew the ticket labelled \\(X\\) first, then the ticket labelled \\(Y\\) second” (for \\(X \\in \\{A, B, C\\}\\) and \\(Y \\in \\{A, B, C\\}\\)), we have \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\n\nBy the way, can anyone tell me why I didn’t include outcomes like \\((A, A)\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "href": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Generalizing",
    "text": "Generalizing\n\nLet’s generalize to picking \\(k\\) tickets from a total \\(n\\): \\[ \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n}} \\ \\ \\ \\ \\ } \\ {\\times} \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 1}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 2}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\cdots \\ {\\times}  \n  \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n - k + 1}} \\ \\ \\ \\ \\ } \\]\nWe can write this a little more succinctly using factorials: \\[ n \\times (n - 1) \\times \\cdots \\times (n - k + 1) = \\frac{n!}{(n - k)!} \\]\nThis is yet another quantity that arises so often, we give it a name: this time we call it \\(n\\) order \\(k\\), and write \\((n)_k\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "href": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "title": "PSTAT 5A: Lecture 04",
    "section": "\\(n\\) order \\(k\\)",
    "text": "\\(n\\) order \\(k\\)\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\) and another positive integer \\(k\\) that is less than \\(n\\), \\[ (n)_k = \\frac{n!}{(n - k)!}  = n \\times (n - 1) \\times \\cdots \\times (n - k + 1)  \\]\n\n\n\n\n\n\nFor example:\n\n\\((5)_3 = 5 \\times 4 \\times 3 = 60\\)\n\\((6)_2 = 6 \\times 5 = 30\\)\n\\((4)_4 = 4 \\times 3 \\times 2 \\times 1 = 24\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nSuppose now that I have 5 tickets, labeled \\(A\\) through \\(E\\), and I now want to draw 3. How many ways are there to do this, assuming order matters?\n\n\n\n\n\n\nBy our work above, the answer is \\((5)_3 = 5 \\times 4 \\times 3 = \\boxed{60}\\).\nDon’t believe me?\n\n\n\\[{\\tiny \\begin{aligned}[t]\n\\Omega  & = \\{ (A, B, C), \\ (A, B, D), \\ (A, B, E), \\ (A, C, B), \\ (A, C, D), \\ (A, C, E), \\ (A, D, B), \\ (A, D, C), \\ (A, D, E), \\ (A, E, B), \\ (A, E, C), \\ (A, E, D), \\\\\n    %\n    & \\hspace{5mm} (B, A, C), \\ (B, A, D), \\ (B, A, E), \\ (B, C, A), \\ (B, C, D), \\ (B, C, E), \\ (B, D, A), \\ (B, D, C), \\ (B, D, E), \\ (B, E, A), \\ (B, E, C), \\ (B, E, D), \\\\\n    %\n    & \\hspace{5mm} (C, A, B), \\ (C, A, D), \\ (C, A, E), \\ (C, B, A), \\ (C, B, D), \\ (C, B, E), \\ (C, D, A), \\ (C, D, B), \\ (C, D, E), \\ (C, E, A), \\ (C, E, B), \\ (C, E, D), \\\\\n    %\n    & \\hspace{5mm} (D, A, B), \\ (D, A, C), \\ (D, A, E), \\ (D, B, A), \\ (D, B, C), \\ (D, B, E), \\ (D, C, A), \\ (D, C, B), \\ (D, C, E), \\ (D, E, A), \\ (D, E, B), \\ (D, E, C), \\\\\n    %\n    & \\hspace{5mm} (E, A, B), \\ (E, A, C), \\ (E, A, D), \\ (E, B, A), \\ (E, B, C), \\ (E, B, D), \\ (E, C, A), \\ (E, C, B), \\ (E, C, D), \\ (E, D, A), \\ (E, D, B), \\ (E, D, C) \\}\n    \\end{aligned}}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Order Doesn’t Matter",
    "text": "Order Doesn’t Matter\n\nLet’s return to our example of drawing \\(2\\) tickets from a set of tickets labeled \\(A\\) through \\(C\\).\nWe previously saw that if order does matter, there are 6 possible outcomes: \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\nIf order doesn’t matter, we actually have fewer outcomes! Specifically:\n\n\\((A, C)\\) and \\((C, A)\\) become equivalent\n\\((A, B)\\) and \\((B, A)\\) become equivalent\n\\((B, C)\\) and \\((C, B)\\) become equivalent\n\nSo, \\(\\Omega\\) becomes \\(\\{(A, B), \\ (A, C), \\ (B, C)\\}\\), so we have only 3 elements in \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "href": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Don’t Worry!",
    "text": "Don’t Worry!\n\nI know that was a lot of math, pretty quickly.\nDon’t worry! We will be returning to a few of these concepts over the coming weeks.\nMy main goal for today’s lecture was to give you an overview of the types of arguments we make when dealing with counting problems, as well as to give you some tools to answer counting problems."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "href": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "title": "PSTAT 5A: Lecture 04",
    "section": "An Exercise",
    "text": "An Exercise\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCalifornia state license plates consist of 7 characters: a digit, followed by 3 letters, followed by 3 digits.\n\nSuppose we do not allow repeated letters or digits in a license plate: i.e. A123BCD456 is a valid plate whereas A122BCC345 is not. How many license plates can be created using this scheme?\nRealistically, license plates are allowed to contain repeated letters or digits. Re-answer the question of how many license plates can be created using this scheme.\nUsing the scheme outlined in part (b), what is the probability of picking a random license plate and having it be B131GHA?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nThe main topic of today’s lecture was counting, which refers to the tools we use to systematically count the elements in a set without having to list out all of the elements contained in it.\nWe talked briefly about what it means for order to matter (or, consequently, not matter).\n\nThis lead us to the notations \\(n!\\), \\((n)_k\\), and \\(\\binom{n}{k}\\).\n\nNext time, we’ll see how we can use new information to update our beliefs on certain events by way of what are known as conditional probabilities.\n\nWe’ll also be able to work through a few more interesting examples."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\nWe started by talking about the structure of data.\nWe were exposed to the notion of a data matrix, which is comprised of a series of observational units (i.e. rows) on a series of variables (i.e. columns)\nFor instance, the palmerpenguins data matrix is:\n\n\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\n\nOf course, the reader is not expected to a priori know what the variables in a dataset represent; as such, most datasets come equipped with a data dictionary that lists out the variables included in the dataset along with a brief description of each.\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nspecies\nThe species of penguin (either Adelie, Chinstrap, or Gentoo)\n\n\nisland\nThe island on which the penguin was found (either Biscoe, Dream, or Torgersen)\n\n\nbill_length_mm\nThe length (millimeters) of the penguin’s bill\n\n\nbill_depth_mm\nThe depth (in millimeters) of the penguin’s bill\n\n\nflipper_length_mm\nThe length (in millimeters) of the penguin’s flipper\n\n\nbody_mass_g\nThe mass (in grams) of the penguin\n\n\nsex\nThe sex of the penguin (either Male or Female)\n\n\nyear\nThe year in which the penguin was observed"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWe also saw that variables fall into two main types: numerical and categorical.\n\nRemember that it is not enough to simply check whether our data is comprised of numbers, as categorical data can be encoded using numbers (e.g. months in a year).\nRather, we should check whether it makes interpretable sense to add two elements in our variable (e.g. 1 + 2 is 3, whereas Jan + Feb is not March)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWithin numerical data, we have a further subdivision into discrete and continuous variables.\n\nThe set of possible values of a discrete variable has jumps, whereas the set possible values of a continuous variable has no jumps.\n\nWithin categorical data, we have a further subdivision into ordinal and nominal variables.\n\nOrdinal variables have a natural ordering (e.g. letter grades, months of the year, etc.) whereas nominal variables do not (e.g. favorite color)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\n\n\n\n\n\ndata_classification\n\n \n\ncluster_main\n\n  \n\ncluster_0\n\n  \n\ncluster_1\n\n  \n\ncluster_2\n\n  \n\ncluster_3\n\n   \n\nData\n\n Variable   \n\nnumerical\n\n Numerical   \n\nData->numerical\n\n    \n\ncategorical\n\n Categorical   \n\nData->categorical\n\n    \n\ncontinuous\n\n Continuous   \n\nnumerical->continuous\n\n    \n\ndiscrete\n\n Discrete   \n\nnumerical->discrete\n\n    \n\nnominal\n\n Nominal   \n\ncategorical->nominal\n\n    \n\nordinal\n\n Ordinal   \n\ncategorical->ordinal"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Visualization",
    "text": "Visualization\n\nOnce we have classified a variable as being either numerical or categorical, we can ask ourselves: how can we best visualize this variable?\nFor categorical data, we use a bargraph and for numerical data we use either a histogram or a boxplot."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Bargraph",
    "text": "Bargraph"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Histogram",
    "text": "Histogram\n\n\nRemember the importance of binwidth: demo"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nRemember that the whiskers are never allowed to extend beyond 1.5 times the IQR (and recall that the IQR is just the width of the box)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nWe can also produce numerical summaries of numerical variables.\nMeasures of Central Tendency are different quantities that summarize the “center” of a variable\n\nThere are two main measures of central tendency we discussed: the mean and the median."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#the-mean",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#the-mean",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "The Mean",
    "text": "The Mean\n\nThe mean (or arithmetic mean) is a sort of “balancing point”:\n\n\n\n\n\n\n\n\n\n\\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]\n\n\nAlso recall our discussion on data aggregation, and how the incorporation of new data changes the mean."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nAnother way we could summarize a numerical dataset (i.e. a dataset containing only one variable, one that is numerical) is to describe how “spread out” the values are.\nThe variance is a sort of “average distance of points to the mean”:\n\n\n\n\n\n\n\n\n\n\\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\]\n\n\nThe standard deviation is just the square root of the variance"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nThe interquartile range (IQR) is another measure of spread: \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\nRecall that the \\(p\\)th percentile of a dataset \\(X\\) is the value \\(\\pi_{x, \\ 0.5}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ 0.5}\\).\n\\(Q_1\\) is the 25th percentile and \\(Q_3\\) is the 75th percentile\n\nThe third measure of spread we discussed is the range: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "5-Number Summary",
    "text": "5-Number Summary\n\nRecall the five number summary, which contains:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum\n\nAlso recall how all of these quantities appear on a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Comparisons of Variables",
    "text": "Comparisons of Variables\n\nIf we want to compare two variables, there are three cases to consider:\n\nNumerical vs. Numerical\nNumerical vs. Categorical\nCategorical vs. Categorical\n\nWhen comparing two numerical variables, we use a scatterplot\nWhen comparing a numerical variable to a categorical variable, we use a side-by-side boxplot\nWhen comparing two categorical variables, we construct a contingency table"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Probability",
    "text": "Basics of Probability\n\nProbability is, in many ways, the language of uncertainty.\nAn experiment is any procedure we can repeat an infinite number of times, where each time we repeat the procedure the same fixed set of “things” can occur\n\nThese “things” are called outcomes\nThe outcome space, denoted \\(\\Omega\\), is the set containing all outcomes associated with a particular experiment.\nEvents are just subset of the outcome space.\n\nWe can express outcome spaces using tables or trees."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability",
    "text": "Probability\n\nProbability is a function that acts on events\n\nNotationally: \\(\\mathbb{P}(E)\\)\n\nThere are two main approaches to computing probabilities:\n\nThe Classical Aproach: if outcomes are equally likely, then for any event \\(E\\) \\[ \\mathbb{P}(E) = \\frac{\\#(E)}{\\#(\\Omega)} \\]\nThe long-run [relative] frequency approach: repeat the experiment an infinite number of times and define \\(\\mathbb{P}(E)\\) to be the proportion of times \\(E\\) occurs"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Long-Run Frequencies Example",
    "text": "Long-Run Frequencies Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToss\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nOutcome\nH\nT\nT\nH\nT\nH\nH\nH\nT\nT\n\n\nRaw freq. of H\n1\n1\n1\n2\n2\n3\n4\n5\n5\n5\n\n\nRel. freq of H\n1/1\n1/2\n1/3\n2/4\n2/5\n3/6\n4/7\n5/8\n5/9\n5/10"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Set Operations",
    "text": "Set Operations\n\nGiven two events \\(E\\) and \\(F\\), there are several operations we can perform:\n\nComplement: \\(E^\\complement\\); denotes “not \\(E\\)”\nUnion: \\(E \\cup F\\); denotes \\(E\\) or \\(F\\) (or both)\nIntersection: \\(E \\cap F\\); denotes \\(E\\) and \\(F\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\) (i.e. for \\(E \\cap F = \\varnothing\\)), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability Rules",
    "text": "Probability Rules\n\nProbability of the Empty Set: \\(\\mathbb{P}(\\varnothing) = 0\\)\nComplement Rule: \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\nAddition Rule: \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Conditional Probabilities",
    "text": "Conditional Probabilities\n\n\\(\\mathbb{P}(E \\mid F)\\) denotes an “updating” of our beliefs on \\(E\\) in the presence of \\(F\\))\n\nDefinition: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}\\), provided \\(\\mathbb{P}(F) \\neq 0\\)\n\nMultiplication Rule: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\\)\nBayes’ Rule: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)}\\)\nLaw of Total Probability: \\(\\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#independence",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#independence",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Independence",
    "text": "Independence\n\nIndependence asserts that \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\), which in turn implies \\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\) and \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)\n\nNote that \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\) only when \\(E\\) and \\(F\\) are independent! Otherwise, you have to compute \\(\\mathbb{P}(E \\cap F)\\) using the multiplication rule.\nThe interpretation of independence is that the two events “do not affect each other”"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Programming",
    "text": "Basics of Programming\n\nRecall that, in this class, we use Python as our main computing language\n\nWe run Python in Jupyter Notebooks\n\nThere are a few data types we have encountered thus far:\n\nstr, for “string”\nint, for “integer”\nfloat, aka “real number with decimals”\nbool, for “boolean”"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-programming-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Programming",
    "text": "Basics of Programming\n\nThere are also certain data classes we have encountered, which are effectively various python objects aggregated into a larger structure. These include:\n\nTables\nLists\nArrays\n\nWe can use indexing to extract certain pieces of data classes; e.g. for a list x, x[i] returns the (i + 1)th element of x.\n\nRemember that Python starts indexing at zero!"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-and-conditionals",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-and-conditionals",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Comparisons and Conditionals",
    "text": "Comparisons and Conditionals\n\nTake a look at lab 2."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Functions",
    "text": "Functions\n\nI’d also like to take a minute to talk about functions in Python.\nPython functions behave much like mathematical functions: they take in some number of arguments (i.e. inputs) and can output a variety of things.\n\nFor example, the type() function returns either the data type or data structure of a single input.\nThere is some language we use when dealing with functions in programming: when we pass the argument x into the function f(), we say that we have “called” the function f() on the argument/input x. The object f(x) is called a function call."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#functions-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Functions",
    "text": "Functions\n\nTo create a user-defined function function_name(), we use the following structure:\n\n\n\ndef function_name(<arg1>, ...., <argn>):\n  \"\"\"\n  include a docstring here\n  \"\"\"\n  <body of the function here>\n\n\n\nBy the way, when writing skeleton code like the above (i.e. a chunk of code that mimics the structure/format of an actual piece of code, but isn’t fully filled in), programmers often use the symbols < and > to denote text. These symbols should not be included in your actual code.\n\nFor example, we would not actually enclose our arguments in < and >’s; we would simply write out the arguments. We’ll see an example of this in a minute"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#exercise",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#exercise",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\n\n\nExercise\n\n\n\nWrite a function is_mult_of_three() that takes in a single input x and outputs True if x is a multiple of three and False if not. Additionally, the function should return \"Error: input must be numeric\" if the argument x that is provided is not numerical.\n\n\n\n\n\nTake a moment to open up our JupyterHub server instance, and try writing out the function on your own. Then we’ll work through it together (please note that solutions to this won’t be provided, so make sure you take notes!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "href": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\nUncertainty surrounds us!\n\n\n\nStatistics is, in many ways, the study of uncertainty.\nProbability is the language of uncertainty; it gives us a way to quantify exactly how this uncertainty factors into our decision making process."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "href": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Experiment",
    "text": "Experiment\n\n\nWe begin with the notion of an experiment. In the context of Probability, we have the following definition:\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn experiment is any procedure that can be repeated an infinite number of times, and each time the procedure is repeated there are a fixed set of things that could occur.\n\n\n\n\n\n\nAn example of an experiment is tossing a coin:\n\nI could go into Storke field and toss a coin an infinite number of times, and each time I toss the coin it will land on either heads or tails."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "href": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Outcome Space",
    "text": "Outcome Space\n\n\nThe things that could occur on each repetition of an experiment are called outcomes.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe outcome space of an experiment is the set \\(\\Omega\\) consisting of all outcomes of the experiment.\n\n\n\n\n\n\nFor instance, in the coin tossing example the outcome space is  \\(\\Omega =\\{\\)heads,  tails\\(\\}\\).\nAs an aside: some textbooks/professors refer to the outcome space as the sample space, and use the letter \\(S\\) to denote it.\n\nSo, if you are doing self-study and encounter the term “sample space”, know that it is the same thing as what we are calling the “outcome space”!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\nLet’s do an example together.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nConsider the experiment of rolling two four-sided dice and recording the faces that appear. What is an appropriate outcome space for this experiment?\n\n\n\n\n\nFirst, let’s find the outcome space.\nOn each dice roll, we will observe either a \\(1\\), \\(2\\), \\(3\\), or \\(4\\).\nBut, we cannot simply say that our outcome space is \\(\\{1, 2, 3, 4\\}\\) as this does not take into account the fact that we rolled two dice!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "href": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Now it’s Your Turn!",
    "text": "Now it’s Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\nConsider the experiment of tossing a coin, picking a number from the set \\(\\{1, 2\\}\\), and then tossing another coin. What is the outcome space of this experiment?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nThere are a few other ways we can use to describe the outcome space of an experiment.\nLet’s return to the tossing four dice example from a few slides ago. Another way we could have kept track of the outcomes was by using a table, recording the outcome of the first die roll in the rows and the outcomes of the second in the columns:\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n1\n\n\n(1, 1)\n\n\n(1, 2)\n\n\n(1, 3)\n\n\n(1, 4)\n\n\n\n\n2\n\n\n(2, 1)\n\n\n(2, 2)\n\n\n(2, 3)\n\n\n(2, 4)\n\n\n\n\n3\n\n\n(3, 1)\n\n\n(3, 2)\n\n\n(3, 3)\n\n\n(3, 4)\n\n\n\n\n4\n\n\n(4, 1)\n\n\n(4, 2)\n\n\n(4, 3)\n\n\n(4, 4)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nSo, tables are a good way of keeping track of outcomes.\nBut, they really only work when we have two of something (e.g. two dice, two coins, etc.). What happens if we, for example, toss three coins?\nThis is where tree diagrams can become useful.\n\n\n\n\n\n\n\n\n\ntree_diagram\n\n  \n\nbase\n\no   \n\nH1\n\nH   \n\nbase->H1\n\n    \n\nT1\n\nT   \n\nbase->T1\n\n    \n\nH21\n\nH   \n\nH1->H21\n\n    \n\nT21\n\nT   \n\nH1->T21\n\n    \n\nH22\n\nH   \n\nT1->H22\n\n    \n\nT22\n\nT   \n\nT1->T22\n\n    \n\nH311\n\nH   \n\nH21->H311\n\n    \n\nT311\n\nT   \n\nH21->T311\n\n    \n\nH321\n\nH   \n\nT21->H321\n\n    \n\nT321\n\nT   \n\nT21->T321\n\n    \n\nH312\n\nH   \n\nH22->H312\n\n    \n\nT312\n\nT   \n\nH22->T312\n\n    \n\nH322\n\nH   \n\nT22->H322\n\n    \n\nT322\n\nT   \n\nT22->T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#events",
    "href": "Pages/Lectures/Lecture03/Lec03.html#events",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Events",
    "text": "Events\n\nSometimes, it will be useful to consider quantities that are a bit more complex than single outcomes.\nFor example, consider the experiment of rolling two 4-sided dice. I could ask myself: in how many outcomes does the second dice roll result in a higher number than the first?\nThis leads us to the notion of an event.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn event is a subset of the outcome space. In other words, an event is just a set consisting of one or more outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "href": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Unions and Intersections",
    "text": "Unions and Intersections\n\nRemember how we talked about the union of two sets last lecture?\nWell, since events are just sets, we can talk about the union of two sets.\n\nIn words, the union corresponds to an “or” statement.\nFor example, let \\(E\\) denote the event “it is raining” and \\(F\\) denote the event “the ground is wet”, then the event \\(E \\cup F\\) would be the event “it is raining or the ground is wet”.\n\nThe intersection of two events (denoted with the \\(\\cap\\) symbol), corresponds to an “and” statement\n\nFor example, if \\(E\\) and \\(F\\) are defined as in the bullet point above, then \\(E \\cap F\\) denotes the event “it is raining and the ground is wet”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "href": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Complements",
    "text": "Complements\n\nThe complement of an event \\(E\\), denoted \\(E^{\\complement}\\), represents the event “not \\(E\\)”\n\nFor instance, if \\(E\\) again denotes the event “it is raining”, then \\(E^{\\complement}\\) denotes the event “it is not raining”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nRecall that \\(E \\cap F\\) denotes the event “both \\(E\\) and \\(F\\) occurred.”\nAlso recall that \\(A^{\\complement}\\) denotes “not \\(A\\)”; i.e. “\\(A\\) did not occur”\nAs such, \\((E \\cap F)^{\\complement}\\) denotes the event “it is not the case that both \\(E\\) or \\(F\\) occurred.”\n\nThis means that either \\(E\\) did not occur, or \\(F\\) did not occur (or both).\nMathematically, this is equivalent to \\(E^\\complement \\cup F^\\complement\\).\n\nAs such, it seems we have arrived at the following equality: \\[ (E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "href": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "title": "PSTAT 5A: Lecture 03",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\n\nThis is one of what are known as DeMorgan’s Laws.\n\n\n\n\n\n\n\n\n\n\nDeMorgan’s Laws\n\n\n\nGiven two events \\(E\\) and \\(F\\), we have the following:\n\n\\((E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement\\)\n\\((E \\cup F)^{\\complement} = E^\\complement \\cap F^\\complement\\)\n\n\n\n\n\n\n\n\nWe will not prove these in this class. However, please familiarize yourself with them as they will be incredibly useful!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is something interesting about the events defined in the previous example.\nThe outcome space of the underlying experiment is \\[ \\Omega = \\{ (H, H), \\ (H, T), \\ (T, H), \\ (T, T)\\} \\] and\n\n\\(A = \\{(H, H), \\ (T, T)\\}\\)\n\\(B = \\{(H, T), \\ (T, H)\\}\\)\n\\(C = \\{(H, H)\\}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\nNow, you may note that we have yet to mention the term “probability.”\nTo get a better sense of “probability”, let’s examine how we use the word in everyday speech:\n\n“the chance of rain is 50%”\n“odds of winning big at a Casino is 1%”\n“probability of scoring a 100% on the PSTAT 5A Midterm 1 is 95%”\n\nNotice that “rain”, “winning big at a Casino”, and “scoring 100% on the PSTAT 5A Midterm 1” are all events.\nAs such, “probability” seems to take in an event and spit out a number."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\n\nIn other words, we can think of “probability” (or, more accurately, what we refer to as a probability measure) as a function that takes in an event and outputs a number.\n\n\n\nThe symbol we use for a probability measure is \\(\\mathbb{P}\\); i.e. we write \\(\\mathbb{P}(E)\\) to denote “the probability of event \\(E\\)”.\nNow, this doesn’t really tell us how to define \\(\\mathbb{P}(E)\\) for an arbitrary event \\(E\\).\nThere are (roughly) two schools of thought when it comes to defining the probability of an event: the long-run frequency approach, and the classical approach."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Long-Run-Frequency Approach",
    "text": "Long-Run-Frequency Approach\n\nThe long-run frequency approach defines the probability of an event \\(E\\) to be the proportion of times \\(E\\) occurs, if the underlying experiment were to be repeated a large number of times.\nTo help us understand the notion of long-run frequencies, let’s go through an example together. Suppose we toss a coin and record whether the outcome lands heads or tails, and further suppose we observe the following tosses:\n\n\n\nH,  T,   T,   H,   T,   H,   H,   H,   T,   T\n\n\n\nTo compute the relative frequency of heads after each toss, we count the number of times we observed heads and divide by the total number of tosses observed."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Classical Approach",
    "text": "Classical Approach\n\nThe second way to define probabilities is what is known as the classical approach.\nAs an important note: we can only apply the classical approach if we believe all outcomes in our experiment to be equally likely.\n\nExamples of situations in which it is safe to assume equally likely outcomes include: tossing a fair coin some number of times, rolling a fair \\(k\\)-sided die, selecting a card at random from a deck of cards, etc.\n\nIf we make the equally likely outcomes assumption, then the classical approach to probability tells us to define \\(\\mathbb{P}(E)\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of outcomes}} \\]\nSo, for example, if we toss a fair coin once, then the classical approach to probability (which can be used since the coin is fair) states that \\[ \\mathbb{P}(\\texttt{heads}) = \\frac{1}{2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\nLet’s quickly compare these two approaches to defining the probability of an event.\nThe long-run frequencies definition has the benefit of not requiring the assumption of equally likely outcomes.\n\nHowever, it relies on the (perhaps odd) consideration of considering what happens when we repeat an experiment a large number of times.\n\nThe classical approach does not rely on such considerations, making the definitions it produces perhaps a bit more easily interpretable.\n\nHowever, it crucially requires the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nFor the purposes of this class, we won’t be too concerned with defining the probability of an event: in many cases, we will just give you the probability.\n\n\n\nIn situations where we do not provide a probability a priori, there will likely be some key word or phrase that lets you know we are looking for the classical definition.\n\nAgain, important words/phrases to look out for are: fair, at random, etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\nIt turns out that there are three axioms that a probability measure must satisfy, collectively called the axioms of probability:\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\).\n\nIf you are not familiar with the notion of axioms: an axiom is a fundamental “truth” of math, that does not need to be proven.\n\nAxioms are like the building blocks, or base assumptions on which a system of math is predicated."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "href": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Summary",
    "text": "Summary\n\nLet’s quickly summarize the concepts/terms we’ve covered:\n\nExperiment: any procedure that can be repeated an infinite number of times, where each time the procedure is repeated there are a fixed set of outcomes that can occur.\nOutcome Space: the set of all outcomes associated with a particular experiment.\nEvent: a subset of the outcome space\nProbability (measure): a function that takes an event and outputs a number\n\nThese are the basic building blocks of probability.\nWe will now combine them!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "href": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Notational Reminder",
    "text": "Notational Reminder\n\nBefore we go any further, I’d like to stress something:\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this class, using proper notation is very important.\n\n\n\n\n\n\n\nFor example, if we have an event \\(E\\) whose probability of occurring is, say, \\(0.5\\), we must write \\(\\mathbb{P}(E) = 0.5\\); it is NOT correct to say \\(E = 0.5\\), or \\(\\mathbb{P} = 0.5\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Complement Rule",
    "text": "The Complement Rule\n\nThe first result we will explore is the so-called complement rule.\n\n\n\n\n\n\n\n\n\nThe Complement Rule\n\n\n\nGiven an event \\(E\\), we have \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\n\n\n\n\n\n\n\nAs an example: if we roll a fair six-sided die and if \\(E =\\) “rolling a \\(1\\)”, then \\(E^\\complement =\\) “not rolling a \\(1\\)” and \\[\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E) = 1 - 1/6 = \\boxed{5/6}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Probability of the Empty Set",
    "text": "The Probability of the Empty Set\n\nRecall that the empty set (\\(\\varnothing\\)) is the set containing no elements.\n\n\n\n\n\n\n\n\n\nThe Probability of the Empty Set\n\n\n\n\\(\\mathbb{P}(\\varnothing) = 0\\).\n\n\n\n\n\n\n\nThe “proof” of this is relatively simple: note that \\(\\Omega^\\complement = \\varnothing\\) (the opposite of “everything” is “nothing”); we also know that \\(\\mathbb{P}(\\Omega) = 1\\) so \\[ \\mathbb{P}(\\Omega^\\complement) = \\mathbb{P}(\\varnothing) = 1 - 1 = 0 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Addition Rule",
    "text": "The Addition Rule\n\nThe next result we will explore is the so-called addition rule.\n\n\n\n\n\n\n\n\n\nThe Addition Rule\n\n\n\nGiven events \\(E\\) and \\(F\\), we have \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)\n\n\n\n\n\n\n\nNote that if \\(E\\) and \\(F\\) are disjoint, then we recover the third axiom of probability!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA recent survey at the Isla Vista Co-Op revealed that 50% of shoppers buy bread, 30% buy jam, and 20% buy both bread and jam.\n\nWhat is the probability that a randomly selected shopper will not purchase jam?\nWhat is the probability that a randomly selected shopper will purchase either bread or jam (or both)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (a)",
    "text": "Solution to Part (a)\n\nLet \\(J\\) denote the event “a randomly selected shopper will purchase jam”.\n\nFrom the problem statement, we have that \\(\\mathbb{P}(J) = 0.3\\)\n\nThe event “a randomly selected shopper will not purchase jam” is given by \\(J^\\complement\\), meaning the quantity we seek is \\(\\mathbb{P}(J^\\complement)\\).\nBy the Complement Rule, we have \\[ \\mathbb{P}(J^\\complement) = 1 - \\mathbb{P}(J) = 1 - 0.3 = \\boxed{0.7 = 70\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (b)",
    "text": "Solution to Part (b)\n\nLet \\(J\\) be defined as before, and let \\(B\\) denote the event “a randomly selected shopper will purchase bread or jam”\n\nThe first quantity provided in the problem statement tells us that \\(\\mathbb{P}(B) = 0.5\\)\nThe final quantity provided in the problem statement tells us that \\(\\mathbb{P}(B \\cap J) = 0.2\\)\n\nThe event “a randomly selected shopper will purchase either bread or jam” is given by \\(B \\cup J\\), meaning we seek \\(\\mathbb{P}(B \\cup J)\\).\nBy the Addition Rule,\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(B \\cup J)     & = \\mathbb{P}(B) + \\mathbb{P}(J) - \\mathbb{P}(B \\cap J)   \\\\\n  & = 0.5 + 0.3 - 0.2 = \\boxed{0.6 = 60\\%}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "href": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "title": "PSTAT 5A: Lecture 03",
    "section": "General Strategy",
    "text": "General Strategy\n\n\n\n\n\n\n\n\nGeneral Strategy for Probability Word Problems\n\n\n\n\nStart by defining events\nNext, translate the information provided to you (through the problem statement) to be in terms of the events you defined above\nThen, identify the quantity you are trying to obtain\nFinally, apply the various probability rules to solve for the desired quantity."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "href": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Time to Put Everything Together!",
    "text": "Time to Put Everything Together!\n\n\n\n\n\n\nExercise 5\n\n\n\n\nTwo fair six-sided dice are rolled.\n\nWhat is the outcome space of this experiment?\nWhat is the probability that the first die lands on the number 2?\nWhat is the probability that the first die lands on the number 2, or the second die lands on an even number?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#venn-diagrams",
    "href": "Pages/Lectures/Lecture03/Lec03.html#venn-diagrams",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\n\nI’d like to impart one additional tool that can help with visualizing the relationship between events: Venn Diagrams\nWe denote the outcome space \\(\\Omega\\) by a large rectangle, and denote events by circles.\nSince events are subsets of \\(\\Omega\\), we draw them inside (physically) the rectangle representing \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#summary-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#summary-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws-2",
    "href": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws-2",
    "title": "PSTAT 5A: Lecture 03",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\nThis can also help give us some intuition on DeMorgan’s Laws as well!\n\nLet’s do this on the whiteboard together.\n\nWe will return to Venn Diagrams periodically throughout this course- for now, I hope they provide a useful tool to help you visualize the set operations we learned this lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nToday, we began our introduction to the field of probability.\n\nProbability, loosely speaking, provides us with a way to quantify uncertainty.\n\nWe discussed the notions of experiments, outcomes, outcome spaces, events, and probabilities.\n\nRemember that there are certain tools/diagrams (namely, tables and tree diagrams) that can help us determine the outcome space of an experiment.\n\nWe then discussed three probability rules: the complement rule, the probability of the empty set, and the addition rule.\nWe also saw how Venn Diagrams can help us visualize set operations.\nNext time, we will start talking about ways to compute the probability of more complex events under the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "href": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we started discussing how to produce and interpret visual summaries for datasets consisting of only one variable.\n\nWe learned that histograms and boxplots are good visualizers for numerical variables and barplots are good visualizers for categorical data.\n\nBut, as we also saw (in the palmerpenguins dataset), data is usually comprised of several variables.\nA natural question therefore arises: how might we visualize the relationship between multiple variables?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Multiple Variables",
    "text": "Multiple Variables\n\nPerhaps unsurprisingly, visualizing the relationship between 3 or more variables can be a bit tricky.\nAs such, we will restrict ourselves to comparing only two variables.\nEven if we compare only two variables, three cases arise:\n\nComparing two numerical variables\nComparing one numerical and one categorical variable\nComparing two categorical variables\n\nWe will examine the first two cases above, and save the third for later."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Two Numerical Variables",
    "text": "Two Numerical Variables\n\nLet’s say we have two variables, and we want to visualize their relationship.\nAs an example, let’s return to the palmerpenguins dataset and compare the bill_length_mm and bill_depth_mm variables. Let’s also restrict ourselves to Gentoo penguins.\n\n\n\n\n   bill_length_mm bill_depth_mm\n 1           46.1          13.2\n 2           50            16.3\n 3           48.7          14.1\n 4           50            15.2\n 5           47.6          14.5\n 6           46.5          13.5\n 7           45.4          14.6\n 8           46.7          15.3\n 9           43.3          13.4\n10           46.8          15.4\n# ℹ 114 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "href": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nThis type of visualization is called a scatterplot.\nSpecifically, when comparing two numerical variables of the same length, we generate a scatterplot by plotting each observational unit on a Cartesian coordinate system where the axes are prescribed by the variables in question.\n\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing two numerical variables (of the same length), a scatterplot is the best visulization tool."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Interpreting Scatterplots",
    "text": "Interpreting Scatterplots\n\n\n\n\nLet’s return to the scatterplot we generated before:\n\n\n\n\n\n\n\n\n\nNotice how as the values of bill_length_mm increase, the corresponding values of bill_depth_mm also increase on average?\n\nThis makes intutive sense: longer bills are probably deeper!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "href": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Trend",
    "text": "Trend\n\nThis is an example of what we call a trend; specifically, a positive linear trend.\n\nA trend is, loosely speaking, any relationship we observe between the two variables in a scatterplot.\nA trend is said to be linear if a one-unit change in one variable corresponds to a fixed amount of change in the other (we’ll talk about nonlinear trends in a bit)\nA trend is said to be positive (or increasing) if a one-unit increase in one variable corresponds to a one-unit increase in the other."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#nonlinear-trends",
    "href": "Pages/Lectures/Lecture02/Lec02.html#nonlinear-trends",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Nonlinear Trends",
    "text": "Nonlinear Trends\n\nSo, what concretely makes a trend nonlinear?\nWell, let’s first quickly talk about what makes a function nonlinear.\nConsider two functions, \\(f\\) and \\(g\\), whose graphs are depicted below:"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nThe key to determining linearity is to check the rate of change.\nFor a linear function: a one-unit change in x corresponds to the same amount of change in y, regardless of where the change in x occurs.\n\nI.e. as x increases from 0 to 1, y should increase/decrease by the same amount as if x had increased from, say, 2 to 3."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nFor the function f above, this is the case:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx: \\(0 \\to 1\\)\ny: \\(0 \\to -1.5\\)\n\\(\\Delta\\)y: \\((-1.5 - 0) = -1.5\\)\n\n\nx: \\(2 \\to 3\\)\ny: \\(-3 \\to -4.5\\)\n\\(\\Delta\\)y: \\((-4.5 + 3.5) = -1.5\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-2",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-2",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nFor the function g above, this is not the case:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx: \\(0 \\to 1\\)\ny: \\(0.12 \\to 0.09\\)\n\\(\\Delta\\)y: \\((0.09 - 0.12) = -0.03\\)\n\n\nx: \\(2 \\to 3\\)\ny: \\(0.08 \\to 0.073\\)\n\\(\\Delta\\)y: \\((0.073 - 0.08) = -0.007\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linearity-3",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linearity-3",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linearity",
    "text": "Linearity\n\nThe notion of in the context of trends is pretty much the same: if a one-unit change in x corresponds to the same amount of change in y everywhere, then we say that the trend is linear. Otherwise, we say the trend is nonlinear.\nBy the way- another way to talk about trends is to phrase things in terms of the variables being compared.\n\nFor example, if the scatterplot of two variables displays a positive linear trend, we might say that the two variables have a positive linear association.\nAs a concrete example: bill length and bill depth appear to have a positive linear association, as seen in the scatterplot from a few slides ago."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Numerical and a Categorical Variable",
    "text": "A Numerical and a Categorical Variable\n\nThe final case we will consider today is comparing a numerical variable to a categorical one.\nAs a concrete example, here is a (mock) dataset comprised of the following variables:\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nstdy_hrs\naverage amount of time (in hrs) a student spent studying for a particular class each week\n\n\nltr_grd\nthe final letter grade (A+, A, A-, etc.) the student received in the class"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing one numerical and one categorical variable, it is best to visualize their relationship using a side-by-side boxplot.\n\n\n\n\n\n\nThough the notion of trend is slightly different in the context of a side-by-side boxplot, we can still use them to determine relationships.\nFor example, from the plot on the previous slide, we can see that, on average, students who received lower grades tended to study less than those students who received higher grades."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "href": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Causality",
    "text": "Causality\n\nI should make a very important point: identifying trends is not the same thing as identifying causal relationships.\nFor example, the side-by-side boxplot from a few slides ago does not tell us that “studying less causes your grade to decrease”\n\nThere are a lot of other confounding variables that could contribute to the decrease in grade.\n\nWe won’t talk too much about causality in this course, but it is an important thing to be aware of: association is not the same thing as causation!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying Data",
    "text": "Quantifying Data\n\nLet’s once again consider a single numerical variable.\nAs a concrete example, we can consider the exam scores variable from the previous slides:\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nThe remainder of today’s lecture will be devoted to finding numerical summaries of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\).\nThis will lead us to several different summary statistics, which are mathematical quantities that seek to describe different aspects of our data."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\nHere is a very broad question: what is the center of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\)?\nPerhaps the scores dataset from earlier is a bit too complicated- let’s simplify things and look at the dataset \\[ X = \\{1, 1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 6\\} \\]\nAs a starting point, I can “plot” these points on a number line (to produce what is known as a dotplot):"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\n\nBack to our question: what is the center of this dataset?\n\n\n\nPerhaps we can think of center as a balancing point. In other words: where should I place a fulcrum to ensure this number line remains balanced?\n\n\n\n\n\n\n\n\n\nWe call this balancing point the arithmetic mean (or just mean, or average, for short), and denote it \\(\\overline{x}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Mean",
    "text": "The Mean\n\n\n\n\n\n\nFormula: The Mean\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its mean, denoted \\(\\overline{x}\\), using the formula \\[ \\overline{x} = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] which can be equivalently written as \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]\n\n\n\n\n\nIn words: we compute the mean by adding up all of the points included in our dataset, and dividing by the total number of points."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Note on Notation",
    "text": "A Note on Notation\n\nPerhaps you haven’t seen the notation \\(\\sum_{i=1}^{n} x_i\\) before. Don’t get scared by it! It’s just a shorthand notation for the sum of the points \\(\\{x_1, x_2, \\cdots, x_n\\}\\).\nSo, if it’s easier for you, you can always think of \\((x_1 + \\cdots x_n)\\) in place of \\(\\sum_{i=1}^{n} x_i\\).\nHaving said that, I will often use this notation (called sigma notation, as the symbol \\(\\sum\\) is the capital Greek letter “sigma”) as it saves quite a bit of time in the long run.\n\nI also urge you to familiarze yourself with \\(\\Sigma\\) notation, as I’m sure you will encounter it even beyond this course!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nCompute the mean of the set \\(B = \\{-1, 0, 1, 1, 2, 4\\}\\). Discuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "href": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Another One!",
    "text": "Another One!\n\n\n\n\n\n\nExercise 2\n\n\n\nA collection of \\(n = 42\\) exam scores have an average 50%. Two additional scores of 100% are reported. How does the average of scores change with the addition of these two new scores?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#range",
    "href": "Pages/Lectures/Lecture02/Lec02.html#range",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Range",
    "text": "Range\n\nAnother way we can summarize a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\) is to describe how spread out it is.\nOne idea on how we can capture the spread is to say: how far apart is the smallest value from the largest value?\nIndeed, this statistic has a name: the range.\n\n\n\n\n\n\n\n\nFormula: Range\n\n\n\nGiven a set of numbers \\(X = \\{x_1, x_2, \\cdots, x_n\\}\\), we compute the range of \\(X\\) as: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\] i.e. the largest value minus the smallest value."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "href": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is another way to think about spread: suppose we look at the average distance of points from their mean.\nMore specifically: define \\(d_i := x_i - \\overline{x}\\) to be the deviation of the \\(i\\)th point from the mean:"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Variance",
    "text": "The Variance\n\n\nThis is what we call the variance of the set \\(X\\), denoted by \\(s_x^2\\).\n\n\n\n\n\n\n\n\nFormula: Variance, and Standard Deviation\n\n\n\nGiven a set of data \\(X = \\{x_i\\}_{i=1}^{n}\\), we compute the variance of \\(X\\) by \\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\] We define the standard deviation, denoted by \\(s_x\\), to be \\(\\sqrt{s_X^2}\\); i.e. \\[ s_x := \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 3\n\n\n\nFor the set \\(X = \\{1, 2, 3, 4, 5\\}\\), compute \\(s_x\\). Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "href": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "title": "PSTAT 5A: Lecture 02",
    "section": "IQR",
    "text": "IQR\n\nThere is yet another way to quantify the spread of a dataset, and that is what is known as the Interquartile Range (IQR, for short).\n\n\n\n\n\n\n\n\nFormula: The IQR\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its interquartile range using the formula \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\n\n\n\n\n\nIn other words, the IQR is just the width of the box in a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Five Number Summary",
    "text": "Five Number Summary\n\nSpeaking of boxplots, there is a set of numbers that occurs frequently when summarizing numerical data, collectively called the five number summary. The elements of the five number summary are:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nConsider a dataset \\(X\\) that has boxplot given by:\n\n\n\n\n\nProvide the five number summary, along with the IQR. Discuss with your Neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#leadup-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#leadup-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Leadup",
    "text": "Leadup\n\nSuppose \\(F = \\{f_i\\}_{i=1}^{n}\\) denotes a set of temperature measurements, as recorded in Fahrenheit.\nNow, let’s say I want to convert the measurements to be in terms of Centigrade, and will store the resulting values in a set called \\(C = \\{c_i\\}_{i=1}^{n}\\) (for “Centigrade”).\nThe \\(c_i\\) values aren’t new data- they are still linked with our original \\(f_i\\) Fahrenheit measurements! Specifically, they are linked by way of the following formula: \\[ c_i = \\frac{5}{9} (f_i - 32) \\] (as this is the conversion from Fahrenheit to Centigrade)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Transformations",
    "text": "Transformations\n\n\nThis is a specific example of a more general concept known as data transformation.\n\n\n\nIn general, given a set of data \\(X = \\{x_i\\}_{i=1}^{n}\\), we can consider the set of points obtained by applying some function (a.k.a. a transformation) \\(g\\) to the points \\(x_i\\), to obtain a new set \\(\\{g(x_i)\\}_{i=1}^{n}\\)\n\nFor instance, we used \\(g(x) = (5/9) \\cdot (x - 32)\\) in our Fahrenheit-to-Centigrade example on the previous slide.\nThere are many other functions we could consider! For instance, in certain cases we may need to transform our data using a logarithm function; i.e. \\(g(x) = \\ln(x)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#linear-transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#linear-transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Linear Transformations",
    "text": "Linear Transformations\n\nFor now, let’s restrict ourselves to considering linear transformations of our data.\nRecall that a function \\(g(x)\\) is said to be linear if it is of the form \\(g(x) = ax + b\\).\n\nSo, we are restricting ourselves to functions \\(g\\) of this form.\n\nOne question we may ask is: what is the mean of our transformed data?\n\nIn other words, given a set \\(X = \\{x_i\\}_{i=1}^{n}\\) and a new set \\(Y := \\{ax_i + b\\}_{i=1}^{n}\\) for fixed constants \\(a\\) and \\(b\\), what can we say about \\(\\overline{Y}\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#more-complicated-transformations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#more-complicated-transformations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "More Complicated Transformations",
    "text": "More Complicated Transformations\n\nFor more general transformations \\(g\\), things don’t work out as nicely.\nFor instance, if we take the logarithm of each datapoint, the average of these new points is not necessarily going to be the logarithm of the average.\n\nTo see that, we can consider a specific example: Let \\(X = \\{1, 2, 3\\}\\) and take \\(Y = \\{\\ln(1), \\ \\ln(2), \\ \\ln(3)\\}\\). Then \\(\\overline{x} = 2\\) and \\(\\overline{y} \\approx 0.597\\), whereas \\(\\ln(\\overline{x}) = \\ln(2) \\approx 0.693\\).\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf \\(X = \\{x_i\\}_{i=1}^{n}\\) and \\(Y = \\{g(x_i)\\}_{i=1}^{n}\\) where \\(g\\) is not a linear transformation, then it is not the case that \\(\\overline{y} = g(\\overline{x})\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summary",
    "text": "Summary\n\nWe started off by finishing our discussion of data visualizing, identifying ways to visualize the relationship between two variables.\n\nSuch visualizations included: scatterplots and side-by-side boxplots.\nWe also discussed notions of trend.\n\nNext, we discussed various numerical summaries of data.\n\nThese included measures of central tendency (like the mean or the median), along with measures of spread (like the variance, standard deviation, or IQR).\nWe were also introduced to the five number summary, which is closely related to boxplots.\n\nNext time we’ll begin our discussion on Probability, which, as we will see, provides a rigorous way of quantifying uncertainty."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nRemember how, last week, we discussed ways to compare two variables?\nAt the time, we only considered comparing two numerical variables and comparing one numerical and one categorical variable.\nWhat about comparing two categorical variables?\nAs a concrete example, let’s return to…"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Revisited",
    "text": "Penguins, Revisited\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Subsetted",
    "text": "Penguins, Subsetted\n\n\n   species island   \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Questions",
    "text": "Some Questions\n\nHere are some questions we could ask:\n\nHow many Adelie penguins were found on Biscoe island?\nWere any Gentoo penguins found on Torgersen island?\nWhat proportion of Chinstrap penguins were found on Dream island?\n\nThese sorts of questions are very nicely answered by way of what is known as a contingency table:\n\n\n\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n\n\n\nThus, the answers to the questions above are: “44”, “no”, and “100%”, respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "href": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Why Bring This Up Now?",
    "text": "Why Bring This Up Now?\n\nYou may be asking yourselves: “why bring this up now? Weren’t we talking about probability?”\nLet’s re-examine the third question we asked on the previous slide: What proportion of Chinstrap penguins were found on Dream island?\nWhat we really did when we answered this was to first restrict ourselves to the row of the contingency table corresponding to Chinstrap penguins, tally up the entries in that row, and then divided the number of penguins that were both Chinstrap and found on Dream Island by the row total."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThis leads us to the main topic of today’s lecture: conditional probabilities.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nIf \\(E\\) and \\(F\\) are two events with \\(\\mathbb{P}(F) \\neq 0\\), then we define the probability of \\(E\\) given \\(F\\), notated \\(\\mathbb{P}(E \\mid F)\\), to be \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\] If \\(\\mathbb{P}(F) = 0\\), then \\(\\mathbb{P}(E \\mid F)\\) is not defined."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "href": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\mathbb{P}(E \\mid F)\\) essentially gives us the proportion of \\(F\\) that is explained by \\(E\\).\n\nAs such, another way to think about conditional probabilities is as an “if-then” statement: if \\(F\\) has occurred, what is the probability that \\(E\\) also occurs?\n\nIf we adopt the classical approach to probability, we have \\[\\begin{align*}\n\\mathbb{P}(E \\mid F)    & = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}    \\\\\n  & = \\frac{\\left( \\frac{\\#(E \\cap F)}{\\#(\\Omega)} \\right)}{\\left( \\frac{\\#(F)}{\\#(\\Omega)} \\right)} = \\frac{\\#(E \\cap F)}{\\#(F)}\n\\end{align*}\\]\nThis is also why contingency tables are so useful in the context of probability- the numerator above will be an entry in the table, and the denominator will be either a row-sum or a column-sum (depending on how the table was constructed)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\n75 UCSB students were surveyed about whether they like pineapple on pizza or not. In addition to their pineapple preference, their standing was also recorded.\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\nA student is to be randomly selected. If the student is a Freshman, what is the probability that they like pineapple on pizza?\n\n\n\n\n\nAs always, we begin by defining events and notation. Let \\(P =\\) “the student likes pineapple on pizza” and \\(F =\\) “the student is a freshman”. We then seek \\(\\mathbb{P}(P \\mid F)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\n\nRecall that. provided \\(\\mathbb{P}(F) \\neq 0\\) \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\]\nWe can multiply both sides of this equation by \\(\\mathbb{P}(F)\\) to obtain the so-called multiplication rule:\n\n\n\n\n\n\n\n\nThe Multiplication Rule\n\n\n\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F)\\), for any events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(F) \\neq 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nNote that “\\(E\\) and \\(F\\)” is the same as “\\(F\\) and \\(E\\)”.\nThat is: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E)\\).\nSo, if we interchange the place of \\(E\\) and \\(F\\) in the multiplication rule, we obtain \\[ \\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) \\]\nThat is to say, we have \\[\\begin{align*}\n\\mathbb{P}(E \\cap F)    & = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\n\\end{align*}\\]\nDividing the last equation by \\(\\mathbb{P}(F)\\) yields an important result:"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\n\n\n\n\n\nBaeys’ Rule\n\n\n\n\\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)} \\] for events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\).\n\n\n\n\n\n\nIn a sense, Bayes’ Rule gives us a way to “reverse the order of a conditional”"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAs an illustration, let’s return to our pineapple-on-pizza contingency table:\n\n\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\n\n\nLetting \\(P\\) and \\(F\\) be defined as before, let’s compute \\(\\mathbb{P}(P \\mid F)\\) using Bayes’ Rule.\nWe need to first compute \\(\\mathbb{P}(F \\mid P)\\), which we see to be \\[ \\mathbb{P}(F \\mid P) = \\frac{10}{10 + 7 + 5 + 13} = \\frac{10}{35} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "href": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Hang In There!",
    "text": "Hang In There!\n\nAt the moment, it may not seem obvious why Bayes’ Rule is helpful.\n\nIt seems like it just makes more work!\n\nBut, rest assured, we will see a very practical application of Bayes’ Rule in a few slides.\nBefore we do, there’s just one more concept we need to discuss."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nConsider an event \\(F\\), and another event \\(E\\).\nIf \\(F\\) happened, it could have happened along with \\(E\\) or it could have happened along with not-\\(E\\).\nThat is, \\[ F = (F \\cap E) \\cup (F \\cap E^\\complement)\\]\nNow, let’s take the probability of both sides. Since the events on the RHS are disjoint, the probability on the RHS just becomes a sum of probabilities: \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\cap E) + \\mathbb{P}(F \\cap E^\\complement) \\]\nFinally, we apply the Multiplication Rule to the probabilities on the RHS to obtain \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\nGiven two events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\), we have \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]\n\n\n\n\n\n\nThis is often useful in the context of a Bayes’ Rule problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAlright, let’s get down to business and tackle a slightly more real-world problem.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nIt is known that a particular disease affects 5% of the population. There exists a test for this disease, but it is not perfect: there is a 10% chance it will return a “negative” result for a person who is actually infected, and there is a 8% chance it will return a “positive” result for a person who is actually healthy.\n\nArasha has taken a test for the disease, and it has indicated a “positive” result. What is the probability that Arasha actually has the disease?"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 1: Define Events",
    "text": "Step 1: Define Events\n\nAs always, we start by defining events.\nLet + denote “the test returns a positive result” and let \\(D\\) denote “Arasha actually has the disease.”\nFirst of all, note that we are not interested in simply finding \\(\\mathbb{P}(D)\\); rather, we are interested in finding \\(\\mathbb{P}(D \\mid +)\\).\n\nThis is because Arasha has already been tested and received a positive result; this is information we need to incorporate into our beliefs!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nWith our events from Step 1, we now turn our attention to translating the information provided in the problem.\nSince there is a \\(10\\%\\) chance that the test returns a negative result given that a person actually has the disease, we have \\[ \\mathbb{P}(+^\\complement \\mid D) = 0.1 \\]\nAdditionally, we are told that there is an \\(8\\%\\) chance that the test returns a positive result given that a person does not have disease, we have \\[ \\mathbb{P}(+ \\mid D^\\complement) = 0.08 \\]\nFinally, we are told that 5% of the population has the disease; hence, \\[ \\mathbb{P}(D) = 0.05 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nBut wait- there’s more!\nGiven that a person has the disease, they will either test positive or test negative.\n\nWhat that means is that \\[ \\mathbb{P}(+ \\mid D) = 1 - \\mathbb{P}(+^\\complement \\mid D) = 1 - 0.1 = 0.9 \\]\nThink of this as a modified complement rule\n\nSimilarly, \\[ \\mathbb{P}(+^\\complement \\mid D^\\complement) = 1 - \\mathbb{P}(+ \\mid D^\\complement) = 1 - 0.08 = 0.92 \\]\nAdditionally, \\[ \\mathbb{P}(D^\\complement) =  1 - \\mathbb{P}(D) = 1 - 0.05 = 0.95 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nSo, here’s a summary of everything we know, just from the problem statement: \\[\\begin{align*}\n\\mathbb{P}(+ \\mid D) = 0.9    & \\hspace{15mm} \\mathbb{P}(+^\\complement \\mid D) = 0.1    \\\\\n\\mathbb{P}(+ \\mid D^\\complement) = 0.08   & \\hspace{15mm}  \\mathbb{P}(P^\\complement \\mid D^\\complement) = 0.92   \\\\\n\\mathbb{P}(D) = 0.05    & \\hspace{15mm}  \\mathbb{P}(D^\\complement) = 0.95\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nNow we are in a position to begin solving the problem.\nRecall that we seek \\(\\mathbb{P}(D \\mid +)\\).\nBut, we only have information on \\(\\mathbb{P}(+ \\mid D)\\).\nAny ideas what rule/tool we should use?\n\nThat’s right; Bayes’ Rule!\n\nWe use Bayes’ Rule to write \\[ \\mathbb{P}(D \\mid +) = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nDo we have \\(\\mathbb{P}(+)?\\)\n\nNo…\nBut how can we get it?\nYup- Law of Total Probability!\n\nWe use the Law of Total Probability to write\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(+)   & = \\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(+ \\mid D^\\complement) \\cdot \\mathbb{P}(D^\\complement)    \\\\\n    & = (0.9) \\cdot (0.05) + (0.08) \\cdot (0.95) = 0.121\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nFinally, putting everything together:\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(D \\mid +)    & = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)}   \\\\\n    & = \\frac{(0.9) \\cdot (0.05)}{0.121} \\boxed{\\approx 37.19\\%}\n\\end{align*}\\]\n\n\nIf that seems low… you’re right! But, it is actually in line with the problem- the test for the disease is pretty bad, considering how often it gets things wrong. This is why this probability is low- because the test is so bad, we cannot be confident that Arasha actually has the disease, even though she tested positive!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Terminology",
    "text": "Some Terminology\n\nBy the way, there’s some terminology I’d like to quickly introduce to make our lives easier going forward.\nThe False Positive Rate of a test is the proportion of times it returns a “positive” result, when the truth is actually “negative”.\n\nSo, in the context of epidemiology, the false positive rate of a test is the proportion of times it says someone has a disease when they do not actually have the disease.\n\nAnalogously, the False Negative Rate of a test is the proportion of times it returns a “negative” result, when the truth is actually “positive”.\n\nSo, in the context of epidemiology, the false negative rate of a test is the proportion of times it says someone does not have a disease when they do actually have the disease."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Problems To Think On:",
    "text": "Some Problems To Think On:\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA recent survey interviewed several UCSB students about their pets. The following data was collected:\n\n\n             Animal\nAdopted       Bunny Cat Dog Hamster\n  Adopted         3   5   8       4\n  Not Adopted     1   5   7       7\n\n\n\nIf a pet is to be selected at random, what is the probability that it is either a cat or adopted?\nA pet is selected at random: what is the probability that it is an adopted dog?\nA pet is selected at random: if it is a dog, what is the probability that it was adopted?"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-3",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-3",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nThere is a very important concept we need to discuss before concluding our initial discussion on Conditional Probabilities.\nConsider two events \\(E\\) and \\(F\\).\n\n\\(\\mathbb{P}(E)\\) represents our beliefs on the event \\(E\\).\n\\(\\mathbb{P}(E \\mid F)\\) represents our beliefs on the event \\(E\\), in the presence of the additional information contained in \\(F\\).\n\nWhat happens if \\(\\mathbb{P}(E) = \\mathbb{P}(E \\mid F)\\)?\n\nThis asserts that our beliefs on \\(E\\) remain unchanged in the presence of \\(F\\).\nThat is, \\(E\\) is “unaffected” by \\(F\\) ; i.e. \\(E\\) is independent of \\(F\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#independence-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#independence-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Independence",
    "text": "Independence\n\n\n\n\n\n\n\nDefinition\n\n\n\nTwo events \\(E\\) and \\(F\\) are defined to be independent (notated \\(E \\perp F\\)) if any of the following hold:\n\n\\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\)\n\\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\)\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)\n\n\n\n\n\n\n\nNote- the probability of an intersection equals the product of unconditional (i.e. marginal) probabilities ONLY WHEN THE EVENTS ARE INDEPENDENT.\n\nIn general, the only way to compute the probability of an intersection is to use the Multiplication Rule.\n\nI’ll show you one of the equivalencies on the board."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s, for the moment, don our Sociology hats and say we’re interested in estimating the average monthly income of US Citizens.\nSurveying every single US Citizen and recording their income is not feasible- doing so would be far too expensive (both in terms of monetary cost as well as temporal cost)\nInstead, a natural idea is to take a sample of some subset of US Citizens, and record the average monthly income of these sampled individuals.\nNow, here’s a question: given two separate samples of, say, 125 US Citizens- do we expect the average income of these two samples to be exactly the same, or slightly different?\n\nProbably slightly different!"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nSo, it seems we have two things to keep track of:\n\nThe true average monthly income of US Citizens\nThe average monthly income of a sample of US Citizens.\n\nBy the last point on the previous slide, we can see that the second quantity above is a random variable.\n\nThis is an example of a sample statistic, which is basically any quantity that is computed from our data.\n\nThe true average monthly income of US Citizens is a fixed number, which we call a population parameter.\n\nIn general, a population parameter is just a parameter that relates to the population (e.g. mean, median, variance, etc.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#inferential-statistics",
    "href": "Pages/Lectures/Lecture11/Lec11.html#inferential-statistics",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nWe consider the population to be some large group we are interested in studying.\n\nThe population is governed by some set of parameters (e.g. mean, median, variance, etc.)\n\nFrom the population we draw a sample (which is random!), and compute sample statistics to try and make inference about the population parameters.\nThis is the structure of inferential statistics."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#parameter-vs.-statistic",
    "href": "Pages/Lectures/Lecture11/Lec11.html#parameter-vs.-statistic",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Parameter vs. Statistic",
    "text": "Parameter vs. Statistic\n\nIt is extremely important to be able to distinguish a population parameter from a sample statistic.\nThere are a couple of ways to do this.\nThe first is to consider the general structure of the situation: if a given quatntity is describing the population, then it must be a population parameter.\n\nIf, instead, it is describing a sample, then it must be a sample statistic.\n\nThe other way to think about this is through randomness: remember that different samples correspond to different observed values of our sample statistic.\n\nSo, imagine asking yourself: “if I took a different sample, would this quantity change?” If so, then it is a sample statistic (or, more carefully, an observed instance of a sample statistic)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of all cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of \\(3.2\\%\\) a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solution",
    "text": "Solution\n\nthe population is the set of all cats in the world, since we the veterinarian seeks to describe the prevalence of FIV among all cats.\nIn this context, the sample is the set of 100 cats the veterinarian examined.\nThe population parameter of interest is \\(p =\\) “the true proportion of cats that suffer from FIV”.\nThe value of 3.2% is an observed instance of a sample statistic, because if the veterinarian had taken a different sample of 100 cats she likely would have observed a different proportion of FIV-positive cats.\n\nIf, instead, the problem stated that 3.2% of all cats have FIV, then this would be describing a population parameter as it is a statement about the population."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA group of (slightly bored) college students would like to determine the true average amount of soda (in liters) in 1-liter soda bottles. To that effect, they purchase 12 different 1-liter soda bottles and find the average amount of soda in these 12 bottles is 0.98L.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of 0.98 a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#different-population-parameters",
    "href": "Pages/Lectures/Lecture11/Lec11.html#different-population-parameters",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Different Population Parameters",
    "text": "Different Population Parameters\n\nSo far we’ve seen examples of two different population parameters:\n\nA population proportion\nA population mean\n\nOther population parameters exist! For instance, we could talk about the population median, the population variance, or even the population IQR.\nUp until now, I’ve been pretty vague about what “inferences” mean. This is because “making inferences” is a broad term!\nOne part of making inferences is trying to estimate the value of a population parameter.\n\nThis process is called parameter estimation."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#parameter-estimation",
    "href": "Pages/Lectures/Lecture11/Lec11.html#parameter-estimation",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nBoth Worked-Out Example 1 and Exercise 1 were (implicitly) problems about parameter estimation.\n\nIn Worked-Out Example 1, the veterinarian wanted to estimate the true proportion of FIV-positive cats\nIn Exercise 1, the college students wanted to estimate the true average amount of soda in 1L soda bottles.\n\nIn general, we use a sample statistic to estimate a population parameter. Some common estimators of some population parameters are:"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#a-wrench-in-the-spanner",
    "href": "Pages/Lectures/Lecture11/Lec11.html#a-wrench-in-the-spanner",
    "title": "PSTAT 5A: Lecture 11",
    "section": "A Wrench in the Spanner",
    "text": "A Wrench in the Spanner\n\nNow, can anyone see a potential problem in using a sample statistic to estimate a population parameter?\n\nThat’s right- the sample statistics are random!\nFor example, every time the veterinarian in Worked-Out Example 1 took a new sample of 100 cats she would obtain a different estimate for the true proportion of cats that have FIV.\nSo, we need some way to express the uncertainty that comes from the randomness of these sample statistics.\n\nTo explore this, let’s do a live demo using Python.\n\nIn the demo, I will simulate drawing several samples of 500 cats, recording the proportion of FIV-positive cats, and drawing the distribution of these sample proportions.\n\nBy the way, the distribution of a sample statistic is called the sampling distribution of that statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#result",
    "href": "Pages/Lectures/Lecture11/Lec11.html#result",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Result",
    "text": "Result\n\nBefore we summarize these results, we should introduce a bit of notation.\n\nWe let \\(\\widehat{P}\\), read “p-hat” denote the random variable that is the proportion of a hypothetical sample.\nWe let \\(\\widehat{p}\\) denote an observed instance of \\(\\widehat{P}\\); i.e. the sample proportion of a particular sample.\n\nWith this notation in mind, we can see that the demo illustrated the following fact: \\(\\widehat{P}\\) is normally distributed!\n\nIt turns out that the expected value of \\(\\widehat{P}\\) is \\(p\\), the true population proportion, and the standard deviation (which, in the context of estimation, is sometimes called the standard error) is \\[ \\sqrt{\\frac{p(1 - p)}{n} } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#central-limit-theorem-for-proportions",
    "href": "Pages/Lectures/Lecture11/Lec11.html#central-limit-theorem-for-proportions",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Central Limit Theorem for Proportions",
    "text": "Central Limit Theorem for Proportions\n\n\n\n\n\n\nImportant\n\n\n\nIf we have reasonably representative samples taken from a population with true proportion \\(p\\) and let \\(\\widehat{P}\\) denote the sample proportion, then \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] provided that\n\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\)\n\n\n\n\n\n\n\nThe two conditions above are sometimes referred to as the success-failure conditions, and must be satisfied in order to invoke the Central Limit Theorem for Proportions (CLTP).\n\nWe’ll talk a bit more about “reasonably representative” samples later in the course (time-permitting)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a recent study has revealed that 87% of Americans are in favor of offering more healthy options at fast-food restaurants. A surveyor takes a representative sample of size 120 Americans, and records the proportion of these Americans that support offering more healthy options at fast-food restaurants.\n\nDefine the random variable of interest.\nWhat is the probability that fewer than 90% of people in the sample support offering more healthy options at fast-food restaurants?\nWhat is the probability that the proportion of people in the sample who support offering more healthy options at fast-food restaurants lies within 2% of the true proportion of 87%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nLet \\(\\widehat{P} =\\) the proportion of Americans in the sample of size \\(n = 120\\) that support offering more healthy options at fast-food restaurants."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nWe would like to utilize the Central Limit Theorem for Proportions. First, we check:\n\n\\(np = (120) \\cdot (0.87) = 104.4 \\geq 10 \\ \\checkmark\\)\n\\(n(1 - p) = (120) \\cdot (1 - 0.87) \\approx 15.6 \\geq 10 \\ \\checkmark\\)\n\n\nSince both of the success-failure conditions are satisfied, we know that \\[ \\widehat{P} \\sim \\mathcal{N}\\left(0.87, \\ \\sqrt{\\frac{0.87 \\cdot (1 - 0.87)}{120}} \\right) \\sim \\mathcal{N}\\left(0.87, \\ 0.031 \\right) \\] and so \\[ \\mathbb{P}(\\widehat{P} \\leq 0.9) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.031} \\leq \\frac{0.9 - 0.87}{0.031} \\right) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.031} \\leq 0.97 \\right) \\] which equates to around \\(\\boxed{0.8340 = 83.40\\%}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solutions-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solutions-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solutions",
    "text": "Solutions\n\nWe seek \\(\\mathbb{P}(0.85 \\leq \\widehat{P} \\leq 0.89)\\).\n\nFirst, we write this as \\(\\mathbb{P}(\\widehat{P} \\leq 0.89) - \\mathbb{P}(\\widehat{P} \\leq 0.85)\\)\nNext, we standardize: \\[\\begin{align*}\nz_1 & = \\frac{0.89 - 0.87}{0.031} \\approx 0.65   \\\\\nz_2 & = \\frac{0.85 - 0.87}{0.031} \\approx -0.65\n\\end{align*}\\]\nFinally, we consult a table to see that the desired probability is \\[ 0.7422 - 0.2578 = \\boxed{0.4844 = 48.44\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAt a certain company, it is known that 65% of employees are from underrepresented minorities (UMs). A representative sample of 80 employees is taken, and the proportion of people from UMs is recorded.\n\nDefine the random variable of interest, and use the notational convention introduced above.\nWhat is the probability that greater than than 50% of people in the sample are from UMs?\nWhat is the probability that the proportion of people in the sample who are from UMs lies within 5% of the true proportion of 65%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#last-time",
    "href": "Pages/Lectures/Lecture10/Lec10.html#last-time",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Last Time",
    "text": "Last Time\n\nLast lecture we started talking about random variables.\nA random variable is a numeric outcome of some random process or experiment.\n\nFor example, “number of heads observed in \\(5\\) independent tosses of a fair coin”\n\nThe state space of a random variable \\(X\\) is the set \\(S_X\\) of possible values the random variable could attain.\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is a “discrete random variable”\nOtherwise, we say \\(X\\) is a “continuous random variable.”\n\nToday we’ll talk about continuous random variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nClassify the following random variables as either discrete or continuous. Make sure to provide appropriate justification!\n\n\\(X =\\) the number of times a computer program crashes in a given day.\n\\(Y =\\) the height of a randomly-selected skyscraper in downtown Los Angeles\n\\(Z =\\) the weight of a randomly-selected fish from a lake\n\\(W =\\) the number of cats that are adopted out of the Santa Barbara location of the Santa Barbara Humane Society each year."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#continuous-random-variables",
    "href": "Pages/Lectures/Lecture10/Lec10.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nContinuous random variables are described by their so-called probability density function (or p.d.f. for short).\n\nThe graph of a p.d.f. is called the density curve.\n\nThe p.d.f. is such that probabilities are found as areas underneath the density curve.\nFor example, if the random variable \\(X\\) has the following density curve…"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#two-properties",
    "href": "Pages/Lectures/Lecture10/Lec10.html#two-properties",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Two Properties",
    "text": "Two Properties\n\nSince probabilities are areas underneath the density curve, we arrive at the following two properties (which themselves follow from the Axioms of Probability):\n\n\n\n\n\n\n\n\nProperties of a P.D.F.\n\n\n\n\nDensity curves must always be nonnegative; i.e. the corresponding p.d.f. \\(f_X(x)\\) must obey \\(f_X(x) \\geq 0\\) for every \\(x\\).\nThe area underneath a density curve must be \\(1\\).\n\n\n\n\n\n\n\nIn this lecture, we will examine two continuous distributions: the uniform distribution, and the normal distribution.\n\nWe will see that the density curves/p.d.f.’s of these two distributions will satisfy the above two properties."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-distribution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-distribution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nThe uniform distribution takes two parameters: \\(a\\) and \\(b\\), with \\(a < b\\).\n\nWe denote the fact that a random variable \\(X\\) follows the uniform distribution with parameters \\(a\\) and \\(b\\) using the notation \\[ X \\sim \\mathrm{Unif}(a, \\ b) \\]\n\nThe \\(\\mathrm{Unif}(a, \\ b)\\) distribution has the following p.d.f.: \\[ f_X(x) = \\begin{cases} \\displaystyle \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\[3mm] 0 & \\text{otherwise} \\\\ \\end{cases} \\] which corresponds to a rectangular density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-density-curves",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-density-curves",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Density Curves",
    "text": "Uniform Density Curves\n\nOftentimes, we will be a bit lazy with our density curve and omit the open/closed circles. For example, we might sketch the density curve of the \\(\\mathrm{Unif}(1, \\ 2.15)\\) distribution as"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#effect-of-changing-a-and-b",
    "href": "Pages/Lectures/Lecture10/Lec10.html#effect-of-changing-a-and-b",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Effect of Changing \\(a\\) and \\(b\\)",
    "text": "Effect of Changing \\(a\\) and \\(b\\)\n\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d => d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d => d.y),0), Math.max(1,d32.max(data2, d => d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d => x_values2(d.x))\n.y(d => y_values2(d.y))\n\nxAxis2 = g => g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g => g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value < a){\n  return 0\n} else if(input_value > b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x < abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#uniform-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#uniform-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Uniform Probabilities",
    "text": "Uniform Probabilities\n\nRecall, from our initial discussion on continuous random variables, that probabilities are found as areas underneath the density curve.\nDue to the rectangular shape of the Uniform density curves, finding probabilities under the Uniform distribution ends up being relatively straightforward (so long as we remember how to find the area of a rectangle!)\nLet’s work through an example together.\n\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), compute \\(\\mathbb{P}(X \\leq 0.57)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solution",
    "text": "Solution\n\nWhen working through probability problems involving continuous distributions, sketching a picture is always a good first step.\n\nSometimes, we will explicitly make that the first step of a problem, meaning failure to sketch a relevant picture may result in less-than-full marks!\n\nThe density curve of the \\(\\mathrm{Unif}(-1, \\ 1)\\) distribution is given by"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solution",
    "text": "Solution\n\nThe desired probability is thus\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a rectangle with base \\((0.57 - (-1)) = 1.57\\) and height \\(1 / (1 - (-1)) = 1/2\\). Therefore, the area of this rectangle - and, also, the desired probability - is \\[ (1.57) \\times \\frac{1}{2} = \\boxed{0.785 = 78.5\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#another-example",
    "href": "Pages/Lectures/Lecture10/Lec10.html#another-example",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(0, 1)\\), compute \\(\\mathbb{P}(0.25 \\leq X \\leq 0.75)\\).\n\n\n\n\n\n\n\nWe are going to solve this problem in two different ways.\nAgain, we always begin with a sketch of the desired probability as an area underneath the density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nThis is not a coincidence!\nFor a more arbitrary distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan be decomposed as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\huge - \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#tail-probabilities-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nIn math, what we have found is:\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\[ \\mathbb{P}(x_1 \\leq X \\leq x_2)  = \\mathbb{P}(X \\leq x_2) - \\mathbb{P}(X \\leq x_1) \\]\n\n\n\n\n\n\nThe quantity \\(\\mathbb{P}(X \\leq x)\\), where we view \\(x\\) as an arbitrary input (and hence the quantity \\(\\mathbb{P}(X \\leq x)\\) as a function of \\(x\\)) is called the cumulative distribution function (or c.d.f. for short) of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a person is selected at random from the line at Starbucks, what is the probability that they spend between 3 and 7 minutes waiting in line?\nOptional What is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than \\(x\\) minutes waiting in line, for an arbitrary value \\(x\\). Yes, your final answer will depend on \\(x\\); that’s why the c.d.f. is a function!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#probability-of-attaining-an-exact-value",
    "href": "Pages/Lectures/Lecture10/Lec10.html#probability-of-attaining-an-exact-value",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Probability of Attaining an Exact Value",
    "text": "Probability of Attaining an Exact Value\n\nIf \\(X \\sim \\mathrm{Unif}[0, 1]\\), what is the probability that \\(X\\) equals, say \\(0.5\\)?\n\nThe area this corresponds to is a rectangle of height \\(1 / (1 - 0) = 1\\), but with width \\(0\\).\nTherefore, the probability is zero.\n\nThis is not unique to the Uniform distribution!\n\n\n\n\n\n\n\n\nProbability of Attaining an Exact Value\n\n\n\nIf \\(X\\) is a continuous random variable, \\(\\mathbb{P}(X = x) = 0\\) for any value \\(x\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-uniform-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-uniform-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Mean and Variance of the Uniform Distribution",
    "text": "Mean and Variance of the Uniform Distribution\n\nIf \\(X \\sim \\mathrm{Unif}[a, b]\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\frac{a + b}{2}\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\frac{1}{12}(b - a)^2\\)\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nConsider again the setup of Exerise 2: the time (in minutes) spent waiting in line at Starbucks is found to vary uniformly on between 5mins and 15mins.\n\nIf we select a person at random, what is the expected amount of time (in minutes) they will spend waiting in line? What about the variance and standard deviation of the time (in minutes) they will spend waiting in line?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-distribution-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-distribution-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nThe normal distribution takes two parameters \\(\\mu\\) and \\(\\sigma\\). We use the notation \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\) to denote “\\(X\\) follows the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\).”\nThe normal distribution has distribution function given by \\[ f(x) = \\frac{1}{\\sigma \\cdot \\sqrt{2 \\pi}} \\cdot \\exp\\left\\{ - \\frac{1}{2} \\cdot \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right\\} \\]\nLet’s determine how the parameters affect the shape of the density curve."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-mu-and-sigma",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-mu-and-sigma",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Changing \\(\\mu\\) and \\(\\sigma\\)\n\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d => d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d => d.y),0), Math.max(1,d3.max(data, d => d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d => x_values(d.x))\n    .y(d => y_values(d.y))\n\nxAxis = g => g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g => g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x < abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-mu",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-mu",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\mu\\)",
    "text": "Changing \\(\\mu\\)\nHolding \\(\\sigma = 1\\) fixed and varying \\(\\mu\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#changing-sigma",
    "href": "Pages/Lectures/Lecture10/Lec10.html#changing-sigma",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Changing \\(\\sigma\\)",
    "text": "Changing \\(\\sigma\\)\nHolding \\(\\mu = 0\\) fixed and varying \\(\\sigma\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#standard-normal-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe standard normal distribution is the normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\); i.e. \\(\\mathcal{N}(0, 1)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nRecall that for continuous variables, probabilities are found as areas underneath the density curve. For example, if \\(X \\sim \\mathcal{N}(0, 1)\\), then \\(\\mathbb{P}(X \\leq -1)\\) is found by computing the area below:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nNow, unlike with the Uniform density curve, we don’t have a simple closed-form formula for areas under the Normal curve.\nFor instance, how would you get a numerical value for the area shaded on the previous slide?\nThe answer is by way of what is known as a normal table, or z-table.\nTo illustrate how to read a normal table, let’s work through an example:\n\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(Z \\leq 0.83)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-table",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-table",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Table",
    "text": "Normal Table"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#reading-the-normal-table",
    "href": "Pages/Lectures/Lecture10/Lec10.html#reading-the-normal-table",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Reading the Normal Table",
    "text": "Reading the Normal Table\n\nTo find \\(\\mathbb{P}(Z \\leq 0.83)\\), we break up \\(0.83\\) as \\[ 0.83 = 0.8 + 0.03 \\]\nThis tells us to find the desired probability in the intersection of the \\(0.8\\) row and the \\(0.03\\) column:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#another-example-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#another-example-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), find\n\n\\(\\mathbb{P}(Z \\leq -1.01)\\)\n\\(\\mathbb{P}(Z \\leq -2.25)\\)\n\\(\\mathbb{P}(-2.25 \\leq Z \\leq -1.01)\\)\n\\(\\mathbb{P}(X \\geq -0.7)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#standardization",
    "href": "Pages/Lectures/Lecture10/Lec10.html#standardization",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Standardization",
    "text": "Standardization\n\nNow, all of our considerations above were in the case of the standard normal distribution. How do we find areas under nonstandard normal density curves?\nThe answer: we use a process called standardization.\n\n\n\n\n\n\n\n\nStandardization\n\n\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), then \\[ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, 1) \\] That is, if we take a normally distributed random variable, subtract off its mean, and divide by its standard deviation, we obtain a random variable whose distribution is the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-general-case",
    "href": "Pages/Lectures/Lecture10/Lec10.html#normal-probabilities-general-case",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Normal Probabilities; General Case",
    "text": "Normal Probabilities; General Case\n\nThus, if \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), here are the steps we use to compute \\(\\mathbb{P}(X \\leq x)\\):\n\nCompute the \\(z-\\)score \\(z = \\frac{x - \\mu}{\\sigma}\\), rounded to two decimal places.\nLook up the corresponding entry in a standard normal table."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn-2",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nIt is found that the scores on a particular exam are normally distributed with a mean of 83 and a standard deviation of 5.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a student is selected at random, what is the probability that they scored 81 or lower?\nIf a student is selected at random, what is the probability that they scored 75 or higher?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-normal-distribution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#mean-and-variance-of-the-normal-distribution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Mean and Variance of the Normal Distribution",
    "text": "Mean and Variance of the Normal Distribution\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\mu\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\sigma^2\\)\n\nSo, the two parameters we use to describe the normal distribution are the mean and the variance.\nWe’ll talk more about parameters in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\nInstructor:\n\nEthan (He/Him)\nepmarzban@pstat.ucsb.edu\nT 4:30 - 5:30pm (Zoom) and Th 2 - 3pm (SH607F)\n\n\n\n\n\n\n\n\n\nTeaching Assistants:\n\n\n\n\nMengrui Zhang\nmengrui@ucsb.edu\nOH: MW, 4-6pm (SH 5431W)\n\n\n\n\n\nOlivier Mulkin\nomulkin@ucsb.edu\nOH: T, 1 - 3pm (Zoom; starts next week)"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Resources",
    "text": "Course Resources\n\nCanvas: for grades\nGradescope: for quizzes and labs\nCourse Website: https://pstat5a.github.io\n\nAll relevant course material will be posted to the website!\nOne exception: quizzes, which will be administered through Gradescope\n\nPlease read the syllabus fully and carefully!\n\nThere’s also a small blurb about email policies in the syllabus. Basically, I’m asking that you email me and the TA’s only in case of emergency (and I leave it to you to determine what’s an emergency); instead, I ask that you please use Office Hours (or even the end of lecture) to communicate with us. I do this to help alleviate stress on both ends of email exchanges; thank you for understanding!"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#discord",
    "href": "Pages/Lectures/Lecture00/Lec00.html#discord",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Discord",
    "text": "Discord\n\n\n\n\n\n\n\n\n\n\nbit.ly/ssa5adisc"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#bit.lyssa5adisc",
    "href": "Pages/Lectures/Lecture00/Lec00.html#bit.lyssa5adisc",
    "title": "PSTAT 5A: Lecture 00",
    "section": "bit.ly/ssa5adisc",
    "text": "bit.ly/ssa5adisc"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "href": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "title": "PSTAT 5A: Lecture 00",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\nNot a bad definition!\nThough, there isn’t a single agreed-upon definition of what data science is.\nMost people agree that Data science is cross-disciplinary, drawing experience and expertise from a wide variety of different fields.\n\nPerhaps the two main fields from which Data Science draws are Statistics and Computer Science\n\nLike ChatGPT suggested, computation is an integral part of Data Science.\n\nAs we will soon see, the data that is being analyzed these days is huge; certainly too large to be able to do anything with it on pen and paper."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "href": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "title": "PSTAT 5A: Lecture 00",
    "section": "The Path Forward",
    "text": "The Path Forward\n\nSo, how does this course factor into the discourse surrounding Data Science?\nFrom the course description:\n\n\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required.\n\n\n\nIndeed, this course will serve as a sort of “table of contents” of Data Science, touching on many (but still not all) of the wonderful subfields and subtopics that comprise the field."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care",
    "href": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Why Should I Care?",
    "text": "Why Should I Care?\n\nA natural question that arises in a class like this is: “why should I care?”"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care-1",
    "href": "Pages/Lectures/Lecture00/Lec00.html#why-should-i-care-1",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Why Should I Care?",
    "text": "Why Should I Care?\n\n\nI suspect not all of you are necessarily pursuing a degree in Statistics or Data Science.\n\n\n\nHowever, wherever there is data, there is the need for a Data Scientist (or, at least, some of the principles from Data Science).\n\nSo, even if you are working in (what you might think is) a field that is far removed from Statsitics, the minute you start dealing with Data is the minute you start needing to know Data Science!\n\nHere’s a perhaps more pragmatic answer: even if you think you want to go straight into industry right after this course, no company wants to hire someone to just mindlessly crunch numbers - though computing experience is absolutely crucial in making yourself a good candidate, employers would much rather have someone who is both skilled at running code but also understands why they are running the code they are running!"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#recap-of-probability",
    "href": "Pages/Lectures/Lecture09/Lec09.html#recap-of-probability",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Recap of Probability",
    "text": "Recap of Probability\n\nRecall the basic ingredients of probability we have discussed so far:\n\nExperiment: any procedure we can repeat an infinite number of times, where on each repetition there is a fixed set of things (called outcomes) that can happen.\nOutcome space (\\(\\boldsymbol{\\Omega}\\)): the set of all outcomes associated with a particular experiment\nEvent: a subset of the outcome space\nProbability: a function that maps events to a number; specifically, one that quantifies our beliefs about a particular event"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\nLet’s actually conduct an experiment together!\nSpecifically, suppose we toss a coin 3 times and record the outcomes.\nFirst question: what is the outcome space?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\n\n\n\n\n\ntree_diagram\n\n  \n\nbase\n\no   \n\nH1\n\nH   \n\nbase->H1\n\n    \n\nT1\n\nT   \n\nbase->T1\n\n    \n\nH21\n\nH   \n\nH1->H21\n\n    \n\nT21\n\nT   \n\nH1->T21\n\n    \n\nH22\n\nH   \n\nT1->H22\n\n    \n\nT22\n\nT   \n\nT1->T22\n\n    \n\nH311\n\nH   \n\nH21->H311\n\n    \n\nT311\n\nT   \n\nH21->T311\n\n    \n\nH321\n\nH   \n\nT21->H321\n\n    \n\nT321\n\nT   \n\nT21->T321\n\n    \n\nH312\n\nH   \n\nH22->H312\n\n    \n\nT312\n\nT   \n\nH22->T312\n\n    \n\nH322\n\nH   \n\nT22->H322\n\n    \n\nT322\n\nT   \n\nT22->T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-2",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-2",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\nOkay, let’s conduct this experiment!\n\n\n\n\nviewof toss = Inputs.button(\"Toss\")\n\n\n\n\n\n\n\ndummy = toss + 1\ncoin = [\"H\", \"T\"]\ns1 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns2 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns3 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns4 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\n[s1, s2, s3];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, let’s keep track of the number of heads we observe each time we run this experiment.\n\nIn fact, let’s do this on the whiteboard."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-3",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-3",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\nAlright, let’s make note of a few things.\n\n\n\nNote that each time we run this experiment, we (sure enough) get an element of the outcome space.\nBut, also note that each time we run the experiment, we get a (potentially) different number of heads.\nIn fact, each outcome in the outcome space corresponds to a different number of heads:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-4",
    "href": "Pages/Lectures/Lecture09/Lec09.html#an-experiment-4",
    "title": "PSTAT 5A: Lecture 09",
    "section": "An Experiment",
    "text": "An Experiment\n\n\n\nOutcome\nNumber of Heads\n\n\n\n\n(H,  H,   H)\n3\n\n\n(H,   H,   T)\n2\n\n\n(H,   T,   H)\n2\n\n\n(T,   H,   H)\n2\n\n\n(H,   T,   T)\n1\n\n\n(T,   H,   T)\n1\n\n\n(T,   T,   H)\n1\n\n\n(T,   T,   T)\n0"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#random-variables",
    "href": "Pages/Lectures/Lecture09/Lec09.html#random-variables",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Random Variables",
    "text": "Random Variables\n\nThis leads us to the notion of random variables.\nLoosely speaking, a random variable is a variable or process with a random numerical outcome.\nWe denote random variables using capital letters; e.g. \\(X\\), \\(Y\\), \\(Z\\), \\(W\\), etc.\nSo, for example, \\(X =\\) “the number of heads in 3 tosses of a coin” is a random variable because (a) it is a numerical outcome of an experiment and (b) it is random (i.e. its value changes depending on the outcome of the experiment).\n\nBy the way, note that we also use capital letters to denote events. So, how will we know whether something is an event or a random variable?\nThat’s right; based on how it is defined! So, again, make sure you are defining everything clearly and explicitly."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#state-space",
    "href": "Pages/Lectures/Lecture09/Lec09.html#state-space",
    "title": "PSTAT 5A: Lecture 09",
    "section": "State Space",
    "text": "State Space\n\nA key part of the definition of random variables is that they must be numerical.\nWhat this means is we can always look at the set of values a random variable could take: this is what we call the state space of a random variable.\nFor example: if \\(X =\\) “number of heads in 3 tosses of a coin”, we see that \\(X\\) will only ever be \\(0\\), \\(1\\), \\(2\\), or \\(3\\).\n\nThis is because it is not possible to toss 3 coins and get, say, 5 heads, or a negative number of heads!\n\nWe often denote the state space of a random variable using the notation \\(S_{\\verb|<variable>|}\\); e.g. \\(S_X\\) to mean the state space of \\(X\\), \\(S_Y\\) to mean the state space of \\(Y\\), etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#classifying-random-variables",
    "href": "Pages/Lectures/Lecture09/Lec09.html#classifying-random-variables",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Classifying Random Variables",
    "text": "Classifying Random Variables\n\nBecause random variables are numerical, their state spaces will always be numerical sets of values.\nThis means we can classify state spaces using our Variable Classification scheme from Week 1!\n\nSpecifically: the state space \\(S_X\\) of a random variable will either have “jumps”, or not.\n\nWe extend the same classification language to random variables:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a random variable \\(X\\), we say that:\n\n\\(X\\) is a discrete random variable (or just “\\(X\\) is discrete) if \\(S_X\\) is has jumps\n\\(X\\) is a continuous random variable (or just “\\(X\\) is continuous) if \\(S_X\\) has no jumps"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#leadup",
    "href": "Pages/Lectures/Lecture09/Lec09.html#leadup",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s return to our coin tossing example.\nWhat is the probability that we observe zero heads?\nWell, in the language of our random variable \\(X\\) (which counts the number of heads in these three tosses of our fair coin), we can translate “zero heads” to the event “\\(\\{X = 0\\}\\)’’, meaning we want to find \\(\\mathbb{P}(X = 0)\\).\nObserving zero heads is equivalent to observing all tails, meaning the event \\(\\{X = 0\\}\\) is equivalent to the event { (T,  T,  T) }.\nNow, up to this point I have been careful to avoid explicitly mentioning whether our coin is fair or not.\n\nFor the time being, let’s assume that the probability our coin lands ‘heads’ on any given toss is some fixed value \\(p\\). (If the coin were fair, then \\(p = 0.5\\) but let’s not make that assumption yet.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#probability-mass-function",
    "href": "Pages/Lectures/Lecture09/Lec09.html#probability-mass-function",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\nThe table on the previous slide is called a probability mass function, and is often abbreviated as p.m.f..\nIn general, the p.m.f. of an arbitrary random variable \\(X\\) is a table or formula that specifies all the possible values a random variable can take (i.e. the state space), along with the probability with which the random variable attains those values.\nWe use the term “function” to describe this because, in abstraction, we can notate the p.m.f. as \\[ p_X(k) := \\mathbb{P}(X = k) \\] where \\(k\\) can be any value in the state space of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#example",
    "href": "Pages/Lectures/Lecture09/Lec09.html#example",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose we toss three fair coins independently, and let \\(X\\) denote the number of heads observed. Construct the p.m.f. (probability mass function) of \\(X\\).\n\n\n\n\n\n\nBy our work from above, the p.m.f. of \\(X\\) is given by \\[\\begin{array}{r|cccc}\n\\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n\\hline\n\\boldsymbol{\\mathbb{P}(X = k)}   & 1/8   & 3/8  & 3/8 &  1/8\n\\end{array}\\]\nBy the way, notice that the probabilities in the p.m.f. sum up to 1.\n\nThis is not a coincidence! Because the probabilities represent the probabilities of all values \\(X\\) can take, they must sum up to 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#properties-of-pmfs",
    "href": "Pages/Lectures/Lecture09/Lec09.html#properties-of-pmfs",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Properties of PMF’s",
    "text": "Properties of PMF’s\n\nThis leads us to posit the following two properties of probability mass functions:\n\n\n\n\n\n\n\n\n\nProperties of a PMF\n\n\n\n\nThe values in a PMF must sum to 1\nThe values in a PMF must always be nonnegative\n\n\n\n\n\n\n\n\nAlso: we implicitly set probabilities not contained in the p.m.f. to be zero.\n\nFor instance: in our coin tossing example, \\(\\mathbb{P}(X = 1.5) = 0\\).\nThis makes sense! If \\(k \\notin S_X\\), then by definition of the state space it is impossible for \\(X\\) to attain the value \\(k\\), and so \\(\\mathbb{P}(X = k) = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & \\boldsymbol{a} & 0.6\n\\end{array}\\] What must be the value of \\(\\boldsymbol{a}\\)?\n\n\n\n\n\n\nBecause the values in a p.m.f. must sum to 1, we must have \\[ 0.1 + 0.2 + a + 0.6 = 1 \\] which means \\[ a = 1 - (0.1 + 0.2 + 0.6) = \\boxed{0.1} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute both \\(\\mathbb{P}(X = 0)\\) and \\(\\mathbb{P}(X \\leq 0)\\).\n\n\n\n\n\n\nFor \\(\\mathbb{P}(X = 0)\\), we can simply read off the corresponding element from the p.m.f.:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#expected-value",
    "href": "Pages/Lectures/Lecture09/Lec09.html#expected-value",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Expected Value",
    "text": "Expected Value\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe expected value (or just expectation) of a discrete random variable \\(X\\) is \\[ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space.\n\n\n\n\n\n\nIn words: multiply each value in the state space by the corresponding probability, and then sum.\nThe expected value is a sort of ‘center’ of a random variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathbb{E}[X]\\).\n\n\n\n\n\n\nWe compute \\[\\begin{align*}\n\\mathbb{E}[X]   & = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k)   \\\\[5mm]\n  & = (-1.4) \\cdot \\mathbb{P}(X = -1.4) + (0) \\cdot \\mathbb{P}(X = 0) + (3) \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15) \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4) \\cdot (0.1) + (0) \\cdot (0.2) + (3) \\cdot (0.1) + (4.15) \\cdot (0.6) = \\boxed{2.65}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#variance-and-sd",
    "href": "Pages/Lectures/Lecture09/Lec09.html#variance-and-sd",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Variance and SD",
    "text": "Variance and SD\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a discrete random variable \\(X\\) is \\[ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space. The standard deviation is the square root of the variance: \\[ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} \\]\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Formula for Variance\n\n\n\n\\[ \\mathrm{Var}(X) = \\left( \\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - \\left( \\mathbb{E}[X] \\right)^2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-5",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-5",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 5\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\).\n\n\n\n\n\n\nWe previously found that \\(\\mathbb{E}[X] = 2.65\\).\nHence, we need only to find \\(\\sum_{k} k^2 \\cdot \\mathbb{P}(X = x)\\): \\[\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) & = (-1.4)^2 \\cdot \\mathbb{P}(X = -1.4) + (0)^2 \\cdot \\mathbb{P}(X = 0) + (3)^2 \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15)^2 \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2) + (3)^2 \\cdot (0.1) + (4.15)^2 \\cdot (0.6) = 11.4295\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSuppose \\(X\\) is a random variable with p.m.f. (probability mass function) given by \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1 & 0 & 1 & 2   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.3 & 0.2 & 0.1 & \\boldsymbol{a}\n\\end{array}\\]\n\nFind the state space \\(S_X\\) of \\(X\\).\nFind the value of \\(\\boldsymbol{a}\\)\nFind \\(\\mathbb{P}(X = 0.5)\\)\nFind \\(\\mathbb{P}(X \\leq 1)\\)\nFind \\(\\mathbb{P}(X > 1)\\)\nFind \\(\\mathbb{E}[X]\\)\nFind \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 2\n\n\n\n\nConsider the following game: a fair six-sided die is rolled. If the number showing is 1 or 2, you win a dollar; if the number showing is 3, 4, or 5 you win 2 dollars; if the number showing is 6, you lose 1 dollar. Let \\(W\\) denote your net winnings after playing this game once.\n\nWrite down the state space \\(S_W\\) of \\(W\\).\nFind the p.m.f. of \\(W\\).\nWhat are your expected winnings after one round of the game?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#back-to-coins",
    "href": "Pages/Lectures/Lecture09/Lec09.html#back-to-coins",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Back to Coins",
    "text": "Back to Coins\n\nAlright, let’s close out this lecture by returning to our coin tossing example.\nAs a reminder: if we let \\(X\\) denote the number of heads in 3 tosses of a \\(p-\\)coin (i.e. a coin that lands ‘heads’ with probability \\(p\\)), the p.m.f. of \\(X\\) is given by\n\n\n\\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & (1 - p)^3   & 3  p (1 - p)^2  & 3  p^2 (1 - p) &  p^3\n\\end{array}\\]\n\n\nWhat if instead of tossing 3 coins, we had tossed 4? Or 5? Or 10?\nWe could go through the same steps we did before, when deriving the p.m.f. for three tosses, but let’s be a little smarter about this; let’s answer the following more general question:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#binomial-distribution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#binomial-distribution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nThe Binomial Distribution\n\n\n\n\nSuppose the probability of a single trial resulting in a ‘success’ is \\(p\\). Letting \\(X\\) denote the number of successes in \\(n\\) independent trials, then we say that \\(X\\) follows the Binomial Distribution with parameters \\(n\\) and \\(p\\). We use the notation \\(X \\sim \\mathrm{Bin}(n, p)\\) to denote this.\n\n\n\n\n\n\n\n\n\n\n\n\nFacts about the Binomial Distribution\n\n\n\n\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\), then\n\n\\(\\displaystyle \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\)\n\\(\\mathbb{E}[X] = np\\) and \\(\\mathrm{Var}(X) = np(1 - p)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#binomial-conditions",
    "href": "Pages/Lectures/Lecture09/Lec09.html#binomial-conditions",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Binomial Conditions",
    "text": "Binomial Conditions\n\n\n\n\n\n\nFour Conditions to Check\n\n\n\n\nIf \\(X\\) counts the number of successes in \\(n\\) trials, there are four conditions that need to be satisfied in order for \\(X\\) to follow the Binomial Distribution:\n\nThe trials must be independent\nThe number of trials, \\(n\\), must be fixed\nThere should be a well-defined notion of “success” and “failure” on each trial\nThe probability of “success” must remain constant across trials.\n\n\n\n\n\n\n\nSo, remember: \\(X \\sim \\mathrm{Bin}(n, p)\\) just means “\\(X\\) counts the number of successes in \\(n\\) trials, where success occurs with probability \\(p\\) on any given trial, subject to the four conditions above being satisfied."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-6",
    "href": "Pages/Lectures/Lecture09/Lec09.html#worked-out-example-6",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 6\n\n\n\n\n\nIf we roll a fair \\(6-\\)sided die \\(13\\) times (assume rolls are independent of each other) and let \\(X\\) denote the number of times we observe an even number, is \\(X\\) binomially distributed?\nIn a large population of \\(100\\) students, of which \\(70\\) own Android phones, we draw a random sample of 10 without replacement and let \\(Y\\) denote the number of students in this sample that have Android phones. Is \\(Y\\) binomially distributed?\nConsider the same setup as in part (b) above, except this time suppose students are selected with replacement. Is \\(Y\\) binomially distributed?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-a",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-a",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (a)",
    "text": "Part (a)\n\nWe check the Binomial Conditions.\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 13\\))\nWell-defined notion of success? Yup! (“success” = “rolling an even number” and “failure” = “rolling an odd number”)\nFixed probability of success? Yup! (\\(p = 1/2\\)).\n\nSince all 4 conditions are satisfied, \\(X\\) binomially distributed: specifically, \\[ X \\sim \\mathrm{Bin}(13, \\ 1/2) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-b",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-b",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (b)",
    "text": "Part (b)\n\nWe check the Binomial Conditions.\n\nIndependent trials? ; because sampling is done without replacement, trials are no longer independent (i.e. the result of our second trial is very much dependent on the result of our first).\n\nSince at least one condition is violated, \\(Y\\) does follow the binomial distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#part-c",
    "href": "Pages/Lectures/Lecture09/Lec09.html#part-c",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Part (c)",
    "text": "Part (c)\n\nWe check the Binomial Conditions (a.k.a. the Binomial Criteria).\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 10\\))\nWell-defined notion of success? Yup! (“success” = “owning an Android phone” and “failure” = “not owning an Android phone”)\nFixed probability of success? Yup! (\\(p = 7/10\\)).\n\nSince all 4 conditions are satisfied, \\(Y\\) binomially distributed: specifically, \\[ Y \\sim \\mathrm{Bin}(10, \\ 7/10) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn-2",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 3\n\n\n\n\nSuppose Jana tosses \\(65\\) different \\(12-\\)sided dice, independently of each other; let \\(Z\\) denote the number of times a multiple of three results.\n\nVerify that \\(Z\\) follows the Binomial Distribution, and identify its parameters.\nWhat is the probability that Jana observes exactly 23 multiples of three?\nWhat is the expected number of multiples of three Jana will observe?\nWhat is the standard deviation of the number of multiples of three Jana will observe?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#what-is-data",
    "href": "Pages/Lectures/Lecture01/Lec01.html#what-is-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#what-is-data-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#what-is-data-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?\n\nAccording to Merriam-Webster (source), there are three definitions for data:\n\n\nfactual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation\ninformation in digital form that can be transmitted or processed\ninformation output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#example-of-data",
    "href": "Pages/Lectures/Lecture01/Lec01.html#example-of-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Example of Data",
    "text": "Example of Data\n\nAs a concrete example of a dataset, let’s explore the so-called palmerpenguins dataset.\nCollected by Dr. Kristen Gorman at the Palmer Station in Antarctica, this dataset contains various measurements of 344 different penguins Dr. Gorman encountered."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#the-data-matrix",
    "href": "Pages/Lectures/Lecture01/Lec01.html#the-data-matrix",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Data Matrix",
    "text": "The Data Matrix\n\nEach row of the data matrix above corresponds to an individual penguin.\n\nIn general, we refer to a given row of the data matrix as an observational unit, or case.\n\nFor each penguin, we can see that there are observations on several different characteristics; specifically, for each penguin she encountered, Dr. Gorman measured and recorded the penguin’s species, island, bill length (in mm), bill depth (in mm), flipper length (in mm), body mass (in grams), sex, and year of observation.\n\nNotice that these are the column names in our data matrix above. In general, the columns of the data matrix are referred to as variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#numerical-vs.-categorical",
    "href": "Pages/Lectures/Lecture01/Lec01.html#numerical-vs.-categorical",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Numerical vs. Categorical",
    "text": "Numerical vs. Categorical\n\nNumerical variables are variables whose observations consist of numbers.\n\nExamples: heights, temperatures, number of free throws, etc.\n\nNot all variables are numerical. For example, I could take a poll asking people’s opinions on the movie Ant-Man and the Wasp: Quantumania- the observations of this variable will most certainly not be numerical.\n\nRather, the observations of this variable will fall into one of a series of fixed categories (e.g. “Enjoyed the movie”, “Neutral about the movie”, “Too many bugs”, etc.).\nAs such, we describe non-numerical variables as categorical variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#a-note-on-language",
    "href": "Pages/Lectures/Lecture01/Lec01.html#a-note-on-language",
    "title": "PSTAT 5A: Lecture 01",
    "section": "A Note on Language",
    "text": "A Note on Language\n\nQuestion: can we say that data is numerical? Or, can we say we have “categorical data”?\nSure- if our data consists of just a single variable!\nThat is to say- the classification terms we learned (and will learn) can be used to describe data, provided our data contains only one variable.\nThe definition of data we are using (i.e. in the context of the data matrix) is that data is comprised of several variables. As such, we cannot simply take the classification of variables and apply that to the entire dataset (unless our dataset consists of only one variable).\n\nThis may seem like a subtle point… and it is! I’m just pointing it out so you are aware of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#continuous-vs.-discrete-variables",
    "href": "Pages/Lectures/Lecture01/Lec01.html#continuous-vs.-discrete-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Continuous vs. Discrete Variables",
    "text": "Continuous vs. Discrete Variables\n\nThere is a way we can further subdivide numerical variables.\nAs an example, let us consider two different variables, both of which are numerical: heights, and number of accidents on a stretch of highway.\n\nIt is perfectly conceivable to observe a height of 5.15 feet, or 5.1302 feet, or 5.02391829 feet. In other words, there are an infinite number of possible heights between, say, 5 feet and 6 feet.\nOn the other hand, it doesn’t make sense to talk about “1.5 accidents” occurring on a stretch of highway; the number of accidents needs to be an integer."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#ordinal-vs.-nominal-variables",
    "href": "Pages/Lectures/Lecture01/Lec01.html#ordinal-vs.-nominal-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Ordinal vs. Nominal Variables",
    "text": "Ordinal vs. Nominal Variables\n\nJust as there was a way to subdivide numerical variables, there is a way to further subdivide categorical variables as well.\nAs an example, consider the following two categorical variables: color, and letter grades (i.e. A, B+, etc.)\n\nFirstly, I hope you can see that both of these variables are indeed categorical: there are only a fixed set of values that “color” and “letter grade” can take, with nothing in between.\nNow, clearly letter grades can be ordered: that is, an A is better than a B, which is better than a C, and so on and so forth.\nIn contrast, “green” isnt inherently better than “red”, which isn’t inherently better than “grey”, and so on and so forth."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#full-classification-scheme",
    "href": "Pages/Lectures/Lecture01/Lec01.html#full-classification-scheme",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\nHere is a diagram of the full classification scheme:\n\n\n\n\n\n\n\n\n\ndata_classification\n\n \n\ncluster_main\n\n  \n\ncluster_0\n\n  \n\ncluster_1\n\n  \n\ncluster_2\n\n  \n\ncluster_3\n\n   \n\nData\n\n Variable   \n\nnumerical\n\n Numerical   \n\nData->numerical\n\n    \n\ncategorical\n\n Categorical   \n\nData->categorical\n\n    \n\ncontinuous\n\n Continuous   \n\nnumerical->continuous\n\n    \n\ndiscrete\n\n Discrete   \n\nnumerical->discrete\n\n    \n\nnominal\n\n Nominal   \n\ncategorical->nominal\n\n    \n\nordinal\n\n Ordinal   \n\ncategorical->ordinal"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nClassify each of the following variables as either discrete, continuous, ordinal, or nominal.\n\n\nThe number of times a computer program returns an error\nThe time it takes an experienced swimmer to complete 4 laps of a pool\nThe favorite flavor of donut of a randomly selected person\nThe months of the year, as written in MM format (e.g. “01” for “January”, “02” for “February”, etc.)\n\n\nDiscuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#important-note",
    "href": "Pages/Lectures/Lecture01/Lec01.html#important-note",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Important Note",
    "text": "Important Note\n\nIt is important to note that categorical data can be encoded using numbers (as we saw in the previous slide).\n\nIndeed, this is a fairly common practice as computers are more adept at dealing with numbers than things like words or symbols.\nAs such, when classifying data, it is not always enough to just check whether the data consists of numbers or not- it is important to think critically about what the data itself represents.\nAs a quick rule-of-thumb: check whether adding two numbers in your dataset makes interpretive sense. 12in \\(+\\) 13in is 15in, whereas blue + gold does not equal anything, regardless of whether blue is being encoded as 0 and gold is being encoded as 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#real-world-data-set",
    "href": "Pages/Lectures/Lecture01/Lec01.html#real-world-data-set",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Real-World Data Set",
    "text": "Real-World Data Set\n\nLet’s return to the palmerpenguins dataset.\nSpecifically, let’s examine the species variable:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#descriptive-statistics",
    "href": "Pages/Lectures/Lecture01/Lec01.html#descriptive-statistics",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nThis is the goal of Descriptive Statistics- to find different summarizing techniques to desribe the data.\n\n\n\nThere are two ways we can seek to summarize data: numerically (using numbers), and graphically.\nLet’s start with the latter- that is, let’s discuss how we might summarize our data using graphs."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#back-to-penguins",
    "href": "Pages/Lectures/Lecture01/Lec01.html#back-to-penguins",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Back To Penguins",
    "text": "Back To Penguins\n\nHere is the species variable one more time:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#bargraphsbarplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#bargraphsbarplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Bargraphs/Barplots",
    "text": "Bargraphs/Barplots\n\n\nThis is an example of what is known as a bargraph or barplot.\n\n\n\n\n\n\n\n\nResult\n\n\n\nA bargraph is the best type of visualization for categorical data.\n\n\n\n\n\nIn general, if you have \\(k\\) categories, then you will have \\(k\\) bars in your bargraph, each with height propotional to the number of observations within the corresponding category.\nAs you can see, computing software is very useful when it comes to data visualization! In a few weeks, you will explore how to generate visualizations of your own in Python during Lab."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-another-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-another-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time For Another Exercise!",
    "text": "Time For Another Exercise!\n\n\n\n\n\n\nExercise 2\n\n\n\nA recent survey asked 120 different PSTAT students what their favorite color is. The bargraph of the results is displayed below:\n\n\n\n\n\nApproximately what proportion of the students in the sample reported either blue or gold as their favorite color? Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#leadup",
    "href": "Pages/Lectures/Lecture01/Lec01.html#leadup",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Leadup",
    "text": "Leadup\n\n\nAll of our discussions above were related to categorical variables.\n\n\n\nAs we discussed at the beginning of this lecture, not all variables are categorical- how do we visualize numerical variables?\nAgain, I find it useful to consider a concrete example: this time, let’s use the bill_length_mm variable from the palmerpenguins dataset."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#discretizationbinning",
    "href": "Pages/Lectures/Lecture01/Lec01.html#discretizationbinning",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Discretization/Binning",
    "text": "Discretization/Binning\n\n\nThis is what is known as discretizing or binning our variable.\n\n\n\nIn other words, when we discretize our data, we carve it up into a bunch of chunks of equal width and see how many observations fall in each chunk.\n\nThe width of each chunk is what we call the binwidth. For example, if my categories are “between 30 and 35”, “between 35 and 40”, etc., then the binwidth is 5mm as each category spans a width of 5mm."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#the-importance-of-binwidth",
    "href": "Pages/Lectures/Lecture01/Lec01.html#the-importance-of-binwidth",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Importance of Binwidth",
    "text": "The Importance of Binwidth\n\nNotice that our notion of a histogram is intimately tied with our choice of binwidth.\nDifferent binwidths can produce wildly different histograms!\nHere is a demo\nIn practice, it is a good idea to play around with different binwidths to find one that results in a histogram that displays a moderate amount of detail without becoming so detailed as to lose sight of the bigger picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Boxplots",
    "text": "Boxplots\n\nIt turns out there is another way to summarize numerical data visually: using what is known as a boxplot.\nBoxplots can be a seem a bit peculiar at first, so let’s take a look at one together. Before diving back into the palmerpenguins dataset, let’s look at a slightly different dataset.\n\nThis dataset contains only one variable, which records the scores (out of 100 points) of 140 different students on a final exam.\n\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#anatomy-of-a-boxplot",
    "href": "Pages/Lectures/Lecture01/Lec01.html#anatomy-of-a-boxplot",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Anatomy of a Boxplot",
    "text": "Anatomy of a Boxplot"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#understanding-boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01.html#understanding-boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Understanding Boxplots",
    "text": "Understanding Boxplots\n\nLet’s discuss each of the quantities represented on the boxplot separately.\nBefore we do, there’s a bit of math we need to cover.\nThe first quantity we will define is a term you may have heard before- percentile.\nLet’s return to our histogram of scores (since we’re a bit more comfortable with reading histograms than boxplots, at this point)"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#percentiles",
    "href": "Pages/Lectures/Lecture01/Lec01.html#percentiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Percentiles",
    "text": "Percentiles\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe pth percentile of a set of observations \\(X\\) is the value \\(\\pi_{x, \\ 0.5}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ p}\\).\n\n\n\n\n\n\nMaybe now you can see why I switched over to this data of scores- I think percentiles are sometimes easier to interpret in the context of exam scores, since they are very commonly reported with standardized testing scores (e.g. SAT, GRE, etc.)\n\nIn the context of scores: someone who scored at the pth percentile performed better than p% of all test-takers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#quartiles",
    "href": "Pages/Lectures/Lecture01/Lec01.html#quartiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Quartiles",
    "text": "Quartiles\n\nWe give a special name to the 25th and 75th percentiles of a set of observations- we call these the first quartile and third quartile, respectively, and use the notation \\(Q_1\\) and \\(Q_3\\) to denote them, respectively.\n\nSo, \\(Q_1\\) is the value such that 25% of observations are less than \\(Q_1\\), and \\(Q_3\\) is the value such that 75% of observations are less than \\(Q_3\\)\n\nThe second quartile (i.e. the 50th percentile) is called the median.\n\nAs such, the median is the value that “splits the data in half”.\nWe’ll talk more about the median in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#small-caveat",
    "href": "Pages/Lectures/Lecture01/Lec01.html#small-caveat",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Small Caveat",
    "text": "Small Caveat\n\nI should quickly mention one small caveat- computing softwares often use a different procedure for computing quartiles.\nThis procedure is quite long and complicated, and is based off an entire paper written back in the 90’s.\nFor example, if we consider the set \\(S = \\{1, 2, 3, 4, 5, 6\\}\\), we would (based on the definition from the previous slide) call the first quartile \\(2\\), whereas most softwares would return a value of \\(2.25\\).\nDon’t worry about why this is- whenever we talk about quartiles in this class, you can just think of the definition I posed on the previous slide."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#whiskers",
    "href": "Pages/Lectures/Lecture01/Lec01.html#whiskers",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\nFinally, we discuss the role of the whiskers on the boxplot.\nThere are several different conventions for how far the whiskers extend. In some conventions, the whiskers extend to the minimum and maximum values of the data.\nThe convention we will use is the following: the whiskers will never reach farther than \\(\\boldsymbol{1.5 \\times (Q_3 - Q_1)}\\).\n\nWhat this means is that there may be points in our dataset that lie beyond the reach of the whiskers. These points are what we call outliers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#whiskers-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#whiskers-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\n\nThe rationale for constructing the whiskers in this way is to try and highlight any points that are unusually distant from the rest of the data.\n\n\n\nFor example, returning to our dataset of scores, we can see that though the median score was around 80.3% there was one person who scored a 97.9%. Because this score is unusually large, we would label it an outlier."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture01/Lec01.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nHere is a boxplot of the bill_length_mm variable from the palmerpenguins dataset:\n\n\n\n\n\n\n\n\nWhat is the median bill length?\nApproximately what percent of penguins had bills shorter than 37mm in length?\nAre there any outliers?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01.html#summary",
    "href": "Pages/Lectures/Lecture01/Lec01.html#summary",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Summary",
    "text": "Summary\n\nWe started off by talking about the structure of data, and the data matrix.\nWe then discussed how to classify variables.\nNext, we explored graphical methods for summarizing data.\n\nBargraphs are best-suited for categorical data\nHistograms and boxplots are best-suited for numerical data\n\nWe also introduced the notions of percentiles, the median, and outliers.\nNext time we’ll discuss how to visualize the relationship between two variables.\nWe’ll also discuss some numerical summaries for data, including the mean, median, standard deviation, and IQR."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#last-time",
    "href": "Pages/Lectures/Lecture12/Lec12.html#last-time",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we began discussing inference on a proportion.\nWe had a population with proportion \\(p\\), drew representative samples from this population, and used the sample proportion \\(\\widehat{P}\\) (i.e. the proportion observed in the sample) as a proxy for \\(p\\).\nOur main result was the Central Limit Theorem for Proportions which states \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] assuming the success-failure conditions are met:\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#substitution-approximation",
    "href": "Pages/Lectures/Lecture12/Lec12.html#substitution-approximation",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Substitution Approximation",
    "text": "Substitution Approximation\n\nCan anyone point out a potential difficulty with verifying the success-failure conditions?\nThat’s right; they involve the parameter \\(p\\), which is in many cases unknown!\n\nRemember - in the beginning of last lecture, I mentioned that the whole point of performing statistical inference is to try and make claims about a population parameter that is unknowable, or too difficult to determine exactly.\n\nTo remedy this, we often use the so-called substitution approximation to the success-failure conditions:\n\n\\(n \\widehat{p} \\geq 10\\)\n\\(n(1 - \\widehat{p}) \\geq 10\\)\n\nSometimes, we substitute \\(\\widehat{p}\\) into the formula for the standard deviation of \\(\\widehat{P}\\), as the next example illustrates."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. What is the probability that the proportion of cats that are FIV-positive in her sample of 500 cats lies within 1 percent of the true proportion of FIV-positive cats?"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nLet \\(p\\) denote the true proportion of FIV-positive cats. Let \\(\\widehat{P}\\) denote the proportion of FIV-positive cats in a representative sample of size 500.\n\nDo we know the value of \\(p\\)?\nNo we do not.\n\nWhat we do have is \\(\\widehat{p} = 0.032\\).\n\nTherefore, we use the substitution approximation to the success-failure conditions:\n\\(n \\widehat{p} = (500)(0.032) = 16 \\geq 10 \\ \\checkmark\\)\n\\(n (1 - \\widehat{p}) = (500)(1 - 0.032) = 484 \\geq 10 \\ \\checkmark\\)\n\nSince both conditions are met, the CLT tells us \\[ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{500}} \\right)\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nWe seek \\(\\mathbb{P}(p - 0.01 \\leq \\widehat{P} \\leq p + 0.01)\\).\nOur first step is to write this as \\[ \\mathbb{P}(\\widehat{P} \\leq p + 0.01) - \\mathbb{P}(\\widehat{P} \\leq p - 0.01 ) \\]\nNext, we find the associated \\(z-\\)scores: \\[\\begin{align*}\nz_1   & = \\frac{(p + 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}}  = \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}    \\\\\nz_2   & = \\frac{(p - 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}} = - \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}  \n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solution-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solution-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solution",
    "text": "Solution\n\nNow, we can apply the substitution approximation to plug in \\(\\widehat{p}\\) in place of \\(p\\) in the denominator of our \\(z-\\)scores to compute \\[\\begin{align*}\nz_{1, \\ \\text{sub}}   & = \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = \\frac{0.01}{0.00787}  = 1.27  \\\\\nz_{2, \\ \\text{sub}}   & = - \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = - \\frac{0.01}{0.00787}  = -1.27\n\\end{align*}\\]\nFinally, consulting our standard normal table, we find the answer to be \\[ 0.8980 - 0.1020 = \\boxed{0.796 = 79.6\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#a-note",
    "href": "Pages/Lectures/Lecture12/Lec12.html#a-note",
    "title": "PSTAT 5A: Lecture 12",
    "section": "A Note",
    "text": "A Note\n\nI’d like to stress that the substitution approximation is just that- an approximation.\nIt is not, for instance, true that \\(\\mathbb{E}[\\widehat{P}] = \\widehat{p}\\); the center of the distribution of \\(\\widehat{P}\\) will always (provided the success-failure conditions are met) be \\(p\\), the true value of the proportion."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s quickly take stock of what we’ve learned.\nIf we have a population with some unknown population parameter \\(p\\), we can repeatedly take representative samples, compute the sample proportion in each sample, and construct the sampling distribution of \\(\\widehat{P}\\).\nAssuming the success-failure conditions are met, this sampling distribution will be centered at \\(p\\), the true proportion value, and hence a decent estimator for \\(p\\) would be \\(\\mathbb{E}[\\widehat{P}]\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nHowever, the key assumption in this procedure is our ability to take multiple samples from the population.\nIn many practical situations, this is not feasible.\nSo, here is a new question to consider: given just a single sample from the population, what can we say about \\(p\\)?\nWell, we’ve already seen that it’s risky to simply take \\(\\widehat{p}\\) (i.e. the value of \\(\\widehat{P}\\) that was observed in the sample we took) to be an estimate of \\(p\\), due to the randomness associated with \\(\\widehat{P}\\).\nInstead of looking for point estimates of \\(p\\), what happens if we instead provide intervals we believe may contain \\(p\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s make things a bit more concrete. Since I like cats, let’s go back to our veterinarian example:\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\n\n\nAgain, it’s risky to say that “the true proportion of FIV-positive cats is 3.2%” based solely on this sample.\nInstead, we are going to start proposing intervals of values that we believe contain \\(p\\).\nNow, clearly the strengths of our beliefs will depend on the interval we provide.\nFor example, I am 100% confident that the true proportion of FIV-positive cats is somewhere in the interval \\((-\\infty, \\infty)\\).\nBut, suppose we instead consider the interval \\((0.030, \\ 0.034)\\); now we can’t really say that we’re 100% certain this interval covers the true value of \\(p\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the basic idea of what are known as confidence intervals.\nI particularly like the analogy our textbook (OpenIntro Statistics) uses:\n\n\n\n[…] Using only a point estimate is like fishing in a murky lake with a spear. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. (page 181)\n\n\n\nFor the purposes of this class, we will construct confidence intervals for an arbitrary parameter \\(\\theta\\) (e.g. a population proportion \\(p\\), a population mean \\(\\mu\\), etc.) of the form \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) where \\(\\widehat{\\theta}\\) represents some point estimate of \\(\\theta\\) and \\(\\mathrm{m.e.}\\) represents a margin of error.\nSo, for the veterinarian example, our confidence interval will be of the form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nBefore constructing a confidence interval, however, we need to specify our confidence level. In other words, we need first have an idea of how confident we want to be that our interval contains the true parameter value.\nFor example, a 95% confidence interval is an interval \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) that we are 95% confident covers the true value of \\(\\theta\\).\nHere’s a question: based on everything we’ve talked about thus far, do you think higher confidence levels correspond to wider or narrower intervals?\n\nThat’s right: the higher our confidence level, the wider our interval will be.\nAs an extreme example, consider again the slightly absurd confidence interval \\((-\\infty, \\ \\infty)\\); this is a 100% confidence interval because we are 100% confident that it covers the true value of the parameter!\n\nSo, therein lies the tradeoff: the more confidence we want, the wider we need to make our intervals and the less informative they become in pinning down the true value of the parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAlright, let’s return to our considerations on population proportions.\nAgain, our confidence interval will take the general form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\).\nIt makes sense that the margin of error should include some information about the variability of \\(\\widehat{P}\\). As such, we take our confidence intervals to be of the form \\[ \\widehat{p} \\pm z^{\\ast} \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\] where \\(z^{\\ast}\\) is a constant that depends on our confidence level.\n\n\\(z^{\\ast}\\) is sometimes called the confidence coefficient, though that name is not fully standard."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nTo see exactly how this dependency manifests itself, let’s make things a bit more concrete and consider a 95% confidence level. It turns out that this implies \\[ \\mathbb{P}(-z^{\\ast} \\leq Z \\leq z^{\\ast}) = 0.95 \\] where \\(Z \\sim \\mathcal{N}(0, \\ 1)\\).\n\nI’ll try to post some supplementary material for those of you curious as to why this is- for now, I ask you to just take this fact at face value.\n\nAs always, we draw a picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nThe region we will sketch is the area under the standard normal curve between \\(-z^{\\ast}\\) and \\(z^{\\ast}\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, here’s the slightly peculiar thing- in this case, we know that the area itself must be 0.95. What we don’t know is exactly where those endpoints are."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSome of you may have an inkling that a normal table may be helpful…. and it will be!\nTo make clear how a normal table will help, let’s convert our picture to be in terms of tail areas:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat must be the area of the shaded bit above?\n\nThat’s right: 5% (since the area in the middle is, by construction, 95%)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-4",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-4",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nBecause the standard normal density curve is symmetric, the area of any one of the two tails must be (5% / 2) = 2.5%:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, what we have shown, is that \\(z^{\\ast}\\) must satisfy the condition \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = 0.025 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-5",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-5",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAgain, \\(z^{\\ast}\\) must satisfy \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = 0.025 \\]\nFrom a normal table, we see that \\[ \\mathbb{P}(Z \\leq -1.96) = 0.025 \\]\nTherefore, we must have \\[ \\mathbb{P}(Z \\leq -z^{\\ast}) = \\mathbb{P}(Z \\leq -1.96) \\] that is, \\(-z^{\\ast} = -1.96\\) or \\(\\boxed{z^{\\ast} = 1.96}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-6",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-6",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSo, in conclusion, a 95% confidence interval for a population proportion will take the form \\[ \\widehat{p} \\pm 1.96 \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\]\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse a similar set of reasoning to show that a 90% confidence interval for a population proportion \\(p\\) takes the form \\[ \\widehat{p} \\pm 1.645 \\cdot \\sqrt{\\frac{p(1 - p)}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Quick Aside: Percentiles of the Standard Normal Distribution",
    "text": "Quick Aside: Percentiles of the Standard Normal Distribution\n\nAs a quick aside: notice that what we’ve done is actually found various percentiles of the standard normal distribution!\nPercentiles of a distribution are defined much in the same way we defined the percentiles of a list of numbers: the pth percentile of a random variable \\(X\\) is the value \\(\\pi_p\\) such that \\(\\mathbb{P}(X \\leq \\pi_p) = p\\).\nTo find the pth percentile of the standard normal table, here are the steps we use:\n\nFind \\(p\\) in the body of the table\nWhatever \\(z-\\)score that corresponds to the value of \\(p\\) in the table will be the pth percentile\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nFind the 4.55th, 83.4th, and 96.41th percentiles of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\nLet’s also quickly discuss one more property of the standard normal distribution: its density curve is symmetric about the \\(y-\\)axis.\nThis actually leads to an interesting (and very useful) result about percentiles of the standard normal distribution:\n\n\n\n\n\n\n\n\nResult\n\n\n\n\nThe \\(p\\)th percentile of the standard normal distribution is equal to negative one times the \\((1 - p)\\)th percentile of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\nTo see why this is, we sketch a picture. Suppose \\(\\pi_{p}\\) is the \\(p\\)th percentile for the standard normal distribution. Then, the area below must be equal to \\(p\\):"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\n\nBy the complement rule, the red area below must therefore be \\(1 - p\\):"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#symmetry-of-the-normal-distribution-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Symmetry of the Normal Distribution",
    "text": "Symmetry of the Normal Distribution\n\n\nFinally, it is precisely the symmetry of the standard normal density curve that guarantees the red area below will also be \\(1 - p\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTherefore, if the area to the left of \\(\\pi_p\\) is \\(p\\) (which was our initial assumption), the area to the left of \\(-\\pi_p\\) is \\((1 - p)\\).\n\nIn other words, the \\(p\\)th percentile (\\(\\pi_p\\)) is negative one times the \\((1 - p)\\)th percentile (\\(-\\pi_p\\))."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-7",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-7",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nHere are some common confidence levels, and their corresponding values of \\(z^{\\ast}\\).\n\n\n\n\n\nConfidence Level\nValue of \\(\\boldsymbol{z^{\\ast}}\\)\n\n\n\n\n90%\n1.645\n\n\n95%\n1.96\n\n\n99%\n2.575\n\n\n\n\n\nRecall that these \\(z^{\\ast}\\)’s are simply corresponding percentiles (scaled by \\(-1\\)) of the standard normal distribution.\nTo find \\(z^{\\ast}\\) corresponding to an arbitrary \\(100 \\times (1 - \\alpha)\\) interval we either:\n\nfind the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution and multiply by \\((-1)\\)\nfind the \\([1 - (\\alpha / 2)] \\times 100\\)th percentile of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-8",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-8",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nFor example, suppose we want to construct a 99% confidence interval.\nThis is equivalent to constructing a \\((1 - 0.01) \\times 100\\%\\) confidence interval, meaning to find the confidence coefficient we can:\n\nfind the \\((0.01 / 2) \\times 100 = 0.5\\)th percentile of the standard normal distribution and scale by \\(-1\\), which yields a value of around \\(2.575\\)\nfind the \\([1 - (0.01 / 2)] \\times 100 = 99.5\\)th percentile of the standard normal distribution, which (again) yields a value of around \\(2.575\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-9",
    "href": "Pages/Lectures/Lecture12/Lec12.html#confidence-intervals-for-proportions-9",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nIn practice: since the value of \\(p\\) is unknown, we typically replace \\(p\\) with \\(\\widehat{p}\\) to obtain an approximate confidence interval: \\[ \\widehat{p} \\pm z^{\\ast} \\cdot \\sqrt{ \\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. Construct a 95% confidence interval for the true poportion of FIV-positive cats.\n\n\n\n\n\n\n\nWe simply plug into our formula from above: \\[ (0.032) \\pm 1.96 \\cdot \\sqrt{ \\frac{(0.032) \\cdot (1 - 0.032)}{500}} = \\boxed{0.032 \\pm 0.0155}\\] or, written out more explicitly, \\[ \\boxed{ [0.0165 \\ , \\ 0.0475] } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nOkay, now that we have an example of a confidence interval under our belt, let’s talk about the correct interpretation of confidence intervals.\nThe following are all correct interpretations of our confidence interval:\n\nWe are 95% confident that the true proportion of FIV-positive cats is between 0.0165 and 0.0475.\nWe are 95% confident that the interval \\([0.0165 \\ , \\ 0.0475]\\) covers the true proportion of FIV-positive cats.\n\nHere is a technically incorrect way of interpreting the confidence interval: there is a 95% probability that the true proportion of FIV-positive cats lies between 0.0165 and 0.0475."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#interpreting-confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nWhy is this typically rejected as an interpretation of a confidence interval?\nBecause this phrasing makes it sound as though the true proportion of FIV-positive cats is a random variable!\n\nThe true proportion of FIV positive cats is a fixed, deterministic value \\(p\\).\nWhat is random are the endpoints of our confidence interval!\nThis is why we phrase our interpretation in terms of “coverage”; it is to highlight the fact that the endpoints of our interval are where our uncertainty (i.e. randomness) comes into play.\n\nI grant that the above is a very subtle point. However, Statisticians are quite particular about wording when it comes to interpreting confidence intervals. As such, we will be particular in this class as well!"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#your-turn",
    "href": "Pages/Lectures/Lecture12/Lec12.html#your-turn",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAs a film critic, you are interested in determining the true proportion of people that have watched The Mandalorian. You take a representative sample of 100 people, and note that 47% of these people have watched The Mandalorian.\n\nConstruct a 95% confidence interval for the proportion of people that have watched The Mandalorian, and interpret your interval in the context of the problem.\nWhen constructing an 85% confidence interval for the proportion of people that have watched The Mandalorian, would you expect this interval to be wider or shorter than the interval you found in part (a)?\nNow, actually construct an 85% confidence interval for the proportion of people that have watched The Mandalorian and see if this agrees with your answer to part (b)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#your-turn-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAs a political scientist, Morgan would like to know the true proportion of people in a city that support Candidate A in an upcoming election. To that effect, they take a representative sample of 120 people and determine that 51% of these sampled individuals support Candidate A.\nConstruct an 87% confidence interval for the true proportion of people that support Candidate A."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#cats",
    "href": "Pages/Lectures/Lecture14/Lec14.html#cats",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Cats!",
    "text": "Cats!\n\nAs we have previously seen…\n… I like cats.\nSo, let’s consider another cat example!\nIt is often stated that only 1 in 5 orange tabby cats is female; i.e. that only 20% of orange tabby cats are female.\nLet’s say we take a representative sample of 100 orange tabby cats and find that 19 of these cats are female.\nSince we observed a proportion of only 19% female cats in our sample, does that mean the claim of 20% of orange tabby cats being female is wrong?\nWell, no! We know that this 19% is actually an observed instance of \\(\\widehat{P}\\), which itself is random.\n\nFurthermore, 19% is pretty close to 20% so there’s nothing obviously letting us know that the claim is false."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#cats-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#cats-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Cats!",
    "text": "Cats!\n\nHowever, if instead our sample of 100 orange tabby cats contained only 1 female in this sample, we might start to question the claim that 20% of orange tabby cats are female.\nOkay, what if in our sample of 100 orange tabby cats we actually only observed 15 female cats?\nThings are perhaps a bit less clear now… we know that there will be some variability in our point estimator, but how much variability would we really expect? Enough to plausibly observe a sample proportion of 15%?\n\n\n\n\nA little more abstractly: we started with a hypothesis (in this example, “20% of orange tabby cats are female”). We then wish to use our data to test how plausible this hypothesis is."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nThis is exactly the framework of hypothesis testing.\nIn hypothesis testing, we start with a pair of competing claims which we call the null hypothesis and alternative hypothesis, respectively.\n\nWe use \\(H_0\\), read as “h-naught”, to denote the null hypothesis and \\(H_A\\) to denote the alternative hypothesis.\n\nFor instance, in our cat example above the null hypothesis would be “\\(H_0\\): the true proportion of orange cats that are female is 20%”.\nOftentimes we will want to phrase our hypotheses in more mathematical terms. This is where the notation we’ve used over the past few lectures comes into play: letting \\(p\\) denote the true proportion of orange tabby cats that are female, we can write our null hypothesis as \\[ H_0: \\ p = 0.2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nWhat about the alternative hypothesis?\nAs the name suggests, the alternative hypothesis provides some sort of alternative to the null.\nLet’s look at our cat example again. Here are some potential alternatives to the null:\n\n\\(p \\neq 0.2\\) (i.e. the true proportion of orange tabby cats that are female is not 20%)\n\\(p > 0.2\\) (i.e. the true proportion of orange tabby cats that are female is larger than 20%)\n\\(p < 0.2\\) (i.e. the true proportion of orange tabby cats that are female is less than 20%)\n\\(p = 0.10\\) (i.e. the true proportion of orange tabby cats that are female is 10%)"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nEach of these alternative hypotheses has a name.\nBefore we talk about these names, let’s establish a slightly more general framework for conducting hypothesis testing on a proportion.\nOur null hypothesis will often take the form \\[ H_0 : \\ p = p_0 \\] for some prespecified value \\(p_0\\) (e.g. 20%, like in our cat example above).\nThis leads to four possible alternative hypotheses:\n\n\\(H_A: \\ p \\neq p_0\\)\n\\(H_A: \\ p > p_0\\)\n\\(H_A: \\ p < p_0\\)\n\\(H_A: \\ p = p_1\\) (where \\(p_1 \\neq p_0\\))\n\nI’d like to stress: in a specific hypothesis testing problem, we need to pick one of these alternative hypotheses"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#terminology",
    "href": "Pages/Lectures/Lecture14/Lec14.html#terminology",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Terminology",
    "text": "Terminology\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p \\neq p_0\\), we refer to the situation as a two-sided hypothesis test.\nWhen our alternative hypothesis is of the form \\(H_A: \\ p > p_0\\) or \\(H_A: \\ p < p_0\\), we refer to the situation as a one-sided hypothesis test. Specifically:\n\n\\(H_A: \\ p < p_0\\) leads to a lower-tailed test\n\\(H_A: \\ p > p_0\\) leads to an upper-tailed test\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p = p_1\\) (for some value \\(p_1\\) different than our null value \\(p_0\\)), we refer to the situation as a simple-vs-simple hypothesis test.\nAgain, our test will be only one of the above!\nAdditionally, it is usually up to the tester (i.e. the statistician or datascientist in charge of conducting the hypothesis test) to pick which set of hypotheses to use."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#setting-the-hypotheses",
    "href": "Pages/Lectures/Lecture14/Lec14.html#setting-the-hypotheses",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Setting the Hypotheses",
    "text": "Setting the Hypotheses\n\nOkay, so which test to we use when?\nIn practice, there isn’t a one-size-fits-all approach to knowing which set of hypotheses to adapt in a given situation.\nUsually, in the absence of any additional information, we adopt a two-sided test as it tends to be the most general.\nHowever, sometimes additional information may be available to us that may influence us to select a different type of test.\n\nFor example, if previous studies have resulted in numerous observed sample proportions much less than the null value \\(p_0\\), we may want to adopt a lower-tailed test.\n\nHow do we set the null hypothesis? Well, typically the null hypothesis is easier to set: I like to think of it as the “status quo”.\n\nIn other words, the null hypothesis is often taken to be whatever the existing claims state; e.g. that 20% of orange tabby cats are female."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\n\nWhat is the population?\nWhat is the sample?\nDefine the parameter of interest.\nDefine the random variable of interest.\nWrite down the null hypothesis for this test.\nWrite down the two-sided alternative hypothesis for this test.\nWrite down the lower-tailed alternative hypothesis for this test.\nWrite down the upper-tailed alternative hypothesis for this test."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all US households.\nThe sample is the representative sample of 500 US households we took.\nThe parameter of interest is \\(p =\\) the true proportion of US households that own a vehicle.\nThe random variable of interest is \\(\\widehat{P} =\\) the proportion of households in a sample of 500 that own a vehicle.\nRecall that the null hypothesis can be thought of as the “status quo”.\n\nIn other words, we take the null to be whatever claim we want to test: \\[ H_0 : \\ p = 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nThe two-sided alternative hypothesis would be that the proportion of households that own a vehicle is not equal to 91.7%: \\[ H_A: \\ p \\neq 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is less than 91.7%: \\[ H_A: \\ p < 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is greater than 91.7%: \\[ H_A: \\ p > 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#hypothesis-testing-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nAlright, here is what we have so far in terms of hypotheses:\n\nThe null hypothesis represents a sort of “status quo” statement.\nThe alternative hypothesis represents an alternative to the status quo.\n\nSo, what is a hypothesis test?\nA hypothesis test is a framework/procedure that allows us to determine whether or not the null should be rejected in favor of the alternative.\nNaturally, a hypothesis test will depend on data! As such, we can think of a hypothesis test as a function that takes in data and outputs either reject H0 or fail to reject H0. \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#fail-to-reject",
    "href": "Pages/Lectures/Lecture14/Lec14.html#fail-to-reject",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Fail To Reject?",
    "text": "Fail To Reject?\n\nBy the way, the results of a hypothesis test are always framed in terms of the null hypothesis; e.g. “reject \\(H_0\\)” or “fail to reject \\(H_0\\)”.\nWait, why are we saying “fail to reject \\(H_0\\)”? Isn’t that just equivalent to “accept \\(H_0\\)”?\nWell, not quite…\nThink of it this way: just because we are saying the particular alternative hypothesis we picked is less plausible than the null, doesn’t mean there isn’t a different alternative hypothesis that is more plausible than the null.\nAll we are saying when we fail to reject the null is exactly that- we didn’t have enough information to reject \\(H_0\\) outright. We are not saying that \\(H_0\\) must be true.\nAdmittedly, some statisticians have gotten a little lax with this distinction and you may encounter textbooks and/or professors that use terms like “accept the null”.\n\nFor better or for worse, I am a bit of a traditionalist in this respect and will adhere to the terminology “fail to reject” in favor of “accept”."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world",
    "href": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Four States of the World",
    "text": "Four States of the World\n\nOkay, so we’ve talked a bit more about what a hypothesis test actually is: it is a procedure that takes in data and outputs a decision on whether or not to reject the null.\nBehind the scenes, however, the null will either be true or not.\nThis leads to the following four situations:\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\nSome of these situations are good, and some of these are bad! Which are which?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#four-states-of-the-world-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Four States of the World",
    "text": "Four States of the World\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nBAD\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nBAD\n\n\n\n\n\n\n\nWe give names to the two “bad” situations: Type I and Type II errors.\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nType I Error\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nType II Error"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors",
    "href": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\n\n\n\n\n\nDefinition: Type I and Type II errors\n\n\n\n\n\nA Type I Error occurs when we reject \\(H_0\\), when \\(H_0\\) was actually true.\nA Type II Error occurs when we fail to reject \\(H_0\\), when \\(H_0\\) was actually false.\n\n\n\n\n\n\n\n\nA common way of interpreting Type I and Type II errors are in the context of the judicial system.\nThe US judicial system is built upon a motto of “innocent until proven guilty.” As such, the null hypothesis is that a given person is innocent.\nA Type I error represents convicting an innocent person.\nA Type II error represents letting a guilty person go free."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#type-i-and-type-ii-errors-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\nViewing the two errors in the context of the judicial system also highlights a tradeoff.\nIf we want to reduce the number of times we wrongfully convict an innocent person, we may want to make the conditions for convicting someone even stronger.\nBut, this would have the consequence of having fewer people overall convicted, thereby (and inadvertently) increasing the chance we let a guilty person go free.\nAs such, controlling for one type of error increses the likelihood of committing the other type."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\nAssuming we are conducting a two-sided test, what would a Type I error be in the context of this experiment? What about a Type II error?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nA Type I error would be concluding that the true proportion of US households that own a vehicle is not 91.7%, when in fact 91.7% of US households own a vehicle.\nA Type II error would be concluding that the true proportion of US households that own a vehicle is 91.7%, when in fact the true proportion is not 91.8%."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#leadup",
    "href": "Pages/Lectures/Lecture14/Lec14.html#leadup",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Leadup",
    "text": "Leadup\n\nAlright, now we know about the basics and background surrounding hypothesis tests.\nHow do we actually construct one?\nLet’s focus on hypothesis testing for population proportions for now; we’ll deal with sample means later.\nRecall our setup: our hypothesis test should be some sort of decision-making process of the form \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#leadup-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#leadup-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Leadup",
    "text": "Leadup\n\nFor the moment, let’s return to the cat example from the beginning of the lecture.\nLetting \\(p\\) denote the true proportion of orange tabby cats that are female, our null hypothesis takes the form \\(H_0: \\ p = 0.2\\).\nSuppose we take a two-sided alternative: \\(H_A: \\ p \\neq 0.2\\).\nNow, we have a good summary statistic for proportions: \\(\\widehat{P}\\).\nAs such, our decision process should probably be of the form \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]\nSaid differently: if we observe a value of \\(\\widehat{p} = 0.82\\), or a value of \\(\\widehat{p} = 0.001\\), we would likely be inclined to reject the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nSo, it makes sense to reject \\(H_0\\) when \\(\\widehat{p}\\) is very far away from \\(p_0\\) (which, in the cat example, is \\(0.2\\)). \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $\\widehat{p}$ is far from $p_0$} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nFor reasons that will become clear in a few slides, we typically avoid using \\(\\widehat{p}\\) and instead use a standardized version of \\(\\widehat{p}\\): \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\] where \\(\\mathrm{TS}\\) stands for test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nNote that, by definition, the test statistic \\(\\mathrm{TS}\\) is a random variable!\n\nThis is because it is simply a centered and scaled version of \\(\\widehat{P}\\), which we know is a random variable.\n\nFor a given sample, however, we will have a given observed value of \\(\\widehat{P}\\), namely \\(\\widehat{p}\\), which will lead to an observed instance of our test statistic \\[ \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{ \\frac{p_0 (1 - p_0)}{n}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#test-statistic-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nLet’s try and convert our decision-making process to be in terms of the test statistic.\nFirst, note that saying \\(\\widehat{p}\\) is “far away” from \\(p_0\\) could mean one of two things:\n\n\\(\\widehat{p}\\) was much larger than \\(p_0\\)\n\\(\\widehat{p}\\) was much smaller than \\(p_0\\)\n\nThese two cases can be combined into a single case if we think in terms of the magnitude of the distance bewteen \\(\\widehat{p}\\) and \\(p_0\\), which is equivalent to considering \\(|\\mathrm{ts}|\\).\nWhat I’m getting at is this: if \\(\\widehat{p}\\) was far away from \\(p_0\\), then \\(|\\mathrm{ts}|\\) must be large.\nHence, we can rephrase our decision process as \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $|\\mathrm{TS}|$ is large} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#constructing-the-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#constructing-the-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Constructing the Test",
    "text": "Constructing the Test\n\nOkay, but how large is “large”?\nThat is, for what values of \\(|\\mathrm{TS}|\\) will we reject the null?\n\nBy the way, the set of values that correspond to a decision of reject is called the rejection region of a test.\n\nIn other words, if our test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] what value should we take \\(c\\) to be?"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-errors",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-errors",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Errors",
    "text": "The Errors\n\nWell, to answer this question, we need to return to our considerations of Type II and Type II errors.\nRecall that a Type I error occurs when we reject \\(H_0\\) when \\(H_0\\) was actually true, and a Type II error occurs when we fail to reject \\(H_0\\) when \\(H_0\\) was false.\nChanging the value of \\(c\\) changes the probability of committing the two types of errors!\nSpecifically, setting a larger value of \\(c\\) corresponds to rejecting \\(H_0\\) for fewer values, thereby decreasing the probability of committing a Type I errror but increasing the probability of committing a Type II error.\nConversely, setting a smaller value of \\(c\\) corresponds to rejecting \\(H_0\\) for more values, thereby increasing the probability of committing a Type I error but decreasing the probability of committing a Type II error."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-compromise",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-compromise",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Compromise",
    "text": "The Compromise\n\nWe need to compromise!\nIn practice, we go into the test knowing how much leeway we are going to allow ourselves to commit a Type I error. That is, we prespecify our tolerance for committing a Type I error.\nThe probability of committing a Type I error is called the level of significance (or just significance level), and is often denoted \\(\\alpha\\).\nStatisticians therefore construct a hypothesis test around a specific value of \\(\\alpha\\).\nA common level of significance is \\(\\alpha = 0.05\\), though \\(\\alpha = 0.01\\) and \\(\\alpha = 0.1\\) are sometimes used as well.\nOkay, so what does this mean for our test?\nWe now know that \\(\\alpha\\) denotes the probability of rejecting the null when the null is true; i.e. \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha \\] where the symbol \\(\\mathbb{P}_{H_0}\\) just means “assuming the null, the probability of….”"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-compromise-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Compromise",
    "text": "The Compromise\n\nAgain, remember that \\(\\alpha\\) is fixed (e.g. \\(0.05\\)); it is the value of \\(c\\) we are after!\nSo, a natural question arises: what is the distribution of \\(\\mathrm{TS}\\) under the null?\nRecall that \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]\nNow, assuming the null is true (i.e. that \\(p = p_0\\)), the Central Limit Theorem for Proportions tells us \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p_0)}{n}} \\right) \\] where the symbol \\(\\stackrel{H_0}{\\sim}\\) is just a shorthand for “distributed as, under the null”"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-distribution-of-the-test-statistic",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-distribution-of-the-test-statistic",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Distribution of the Test Statistic",
    "text": "The Distribution of the Test Statistic\n\nTherefore, assuming the null is correct, we have \\[ \\mathrm{TS} \\sim \\mathcal{N}(0, \\ 1)\\]\nSo, our condition \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha \\] which, by the symmetry of the standard normal distribution, is equivalent to \\[ \\mathbb{P}_{H_0}(\\mathrm{TS} < -c) = \\frac{\\alpha}{2} \\]\nHence, \\(-c\\) is just the \\((\\alpha / 2) \\times 100\\) percentile of the standard normal distribution!!!"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{1 - \\alpha/2}\\) denotes the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1 (which is equivalent to the \\((1 - \\alpha/2) \\times 100\\)th percentile)\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\nFor example, if \\(\\alpha = 0.05\\) then \\(c\\) is negative 1 times the 2.5th percentile of the standard normal distribution; i.e. 1.96 and our test becomes \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > 1.96 \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\n\n\nCAUTION!!!\n\n\nAll of this is predicated on our invocation of the Central Limit Theorem for Proportions!\nIn other words, the test above was derived assuming \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p)}{n}} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-assumptions",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-assumptions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Assumptions",
    "text": "The Assumptions\n\n\nThis is not always true!\n\n\n\nWhen is this true? In other words, what conditions do we need in order for the above distributional statement to be true?\n\nThat’s right, the success-failure conditions.\n\nBut now, since we only need the above to be true for \\(p = p_0\\), we only need to verify that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nIt is very important we check these conditions before conducting our test!"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#the-test-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#the-test-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion:\n\nCheck that the success-failure conditions hold. Namely, check that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nCompute the observed value of the test statistic \\[\\displaystyle \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\]\nCompute the critical value \\(z_{1 - \\alpha/2}\\), which is the the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1 (or simply the \\((1 - \\alpha/2) \\times 100\\)th percentile)\nReject \\(H_0\\) if \\(|\\mathrm{TS}| > z_{1 - \\alpha/2}\\), and fail to reject \\(H_0\\) otherwise."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(5\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-2",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nAll we really need to do is follow the steps outlined in the previous slide.\n\n\nCheck Conditions\n\n\\(n p_0 = 500 \\cdot (0.917) = 458.5 \\geq 10 \\ \\checkmark\\)\n\\(n (1 - p_0) = 500 \\cdot (1 - 0.917) = 41.5 \\geq 10 \\ \\checkmark\\)\nSince both conditions are met, we can proceed.\n\nCompute the Observed Value of the Test Statistic \\[ \\mathrm{ts} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} = \\frac{0.894 - 0.917}{\\sqrt{\\frac{0.917 (1 - 0.917)}{500}}} = -1.86 \\]\nCompute the critical value Because \\(\\alpha = 0.05\\), the critical value is \\(1.96\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-3",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nConduct the test: We see that \\(|\\mathrm{TS}| = |-1.86| = 1.86\\) which is not greater than \\(1.96\\). As such, we fail to reject the null.\n\n\nNow, the problem told us to phrase our conclusions carefully and in the context of the problem.\nIt is VERY important to include the level of significance in your final conclusions.\nSo, here is how we would phrase the final conclusion of our test:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there is insufficient evidence to reject Forbes’s claim that 91.7% of US households own a vehicle in favor of the alternative that the true proportion of US households that own a vehicle is not 91.7%.\n\n\n\nIt is very important to include the level of significance in our conclusion! This is because the final outcome of our test may change depending on which level of significance we use."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture14/Lec14.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(10\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-4",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe only thing that will change from before is our critical value.\nSince we are using an \\(\\alpha = 0.1\\) level of significance, we find the \\([1 - (0.1 / 2)] \\times 100\\)th percentile (which is equivalent to the \\((0.1 / 2) \\times 100\\)th percentile, scaled by negative 1).\nThere are several ways we could find this critical value.\nThe first is to use our normal table: \\(1.645\\).\nThe second is to use our \\(t-\\)table: \\(1.645\\)\nThe third is to use Python:\n\n\n\nimport scipy.stats as sps\n-sps.norm.ppf(0.1 / 2)\n\n1.6448536269514729"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture14/Lec14.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nHence, we adopt a critical value of \\(1.645\\).\nSince the observed value of our test statistic \\(|\\mathrm{ts}| = 1.86\\) is now larger than the critical value, we reject the null.\n\n\n\nAt an \\(\\alpha = 0.1\\) level of significance, there is evidence to reject Forbes’s claim that 91.7% of US households own a vehicle in favor of the alternative that the true proportion of households that own a vehicle is not 91.7%.\n\n\n\nIntuitively, it makes sense why we might reject for this new level of significance.\nWe know that the level of significance represents the probability of committing a Type I Error.\nIf we increase this value (which we did, in adopting \\(\\alpha = 0.1\\) as opposed to \\(\\alpha = 0.05\\) in the previous example), we are allowing a greater chance of falsely rejecting the null, which comes part-in-parcel with rejecting for more values of the test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#one-sided-tests",
    "href": "Pages/Lectures/Lecture14/Lec14.html#one-sided-tests",
    "title": "PSTAT 5A: Lecture 14",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n\nOn the homework, I ask you to walk through the details of deriving a lower-tailed test.\nYou can use a similar set of arguments to derive the upper-tailed test as well.\nFor posterity’s sake, here are the final results of each:"
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#lower-tailed-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#lower-tailed-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Lower-Tailed Test",
    "text": "Lower-Tailed Test\n\n\n\n\n\n\n\nLower-Tailed Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p < p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} < z_{\\alpha} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{\\alpha}\\) denotes the \\((\\alpha) \\times 100\\)th percentile of the standard normal distribution, not scaled by anything\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#upper-tailed-test",
    "href": "Pages/Lectures/Lecture14/Lec14.html#upper-tailed-test",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Upper-Tailed Test",
    "text": "Upper-Tailed Test\n\n\n\n\n\n\n\nUpper-Tailed Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p > p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} > z_{1 - \\alpha} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{1 - \\alpha}\\) denotes the \\((1 - \\alpha) \\times 100\\)th percentile of the standard normal distribution, not scaled by anything\n\nprovided that: \\(n p_0 \\geq 10\\) and \\(n (1 - p_0) \\geq 10\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture14/Lec14.html#note",
    "href": "Pages/Lectures/Lecture14/Lec14.html#note",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Note",
    "text": "Note\n\nI don’t recommend you try and memorize all of the different percentiles and critical values.\nInstead, I recommend you familiarize yourself with the process used to derive these tests, as that will immediately tell you what picture to draw, which will in turn tell you which quantile we are after in a given situation.\nThere are some problems on the practice problems that give you practice with these one-sided alternatives."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#previously",
    "href": "Pages/Lectures/Lecture13/Lec13.html#previously",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Previously",
    "text": "Previously\n\nOver the course of the past few lectures, we’ve been dealing primarily with population proportions.\nA natural point estimate of \\(p\\) is \\(\\widehat{P}\\), the sample proportion.\n\nThe Central Limit Theorem for Proportions helped us even further by providing the sampling distribution of \\(\\widehat{P}\\), under certain conditions (the success-failure conditions).\n\nWe then used the sampling distribution of \\(\\widehat{P}\\) to construct confidence intervals for the true proportion \\(p\\).\nNow we will turn our attention to a different population parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "href": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Leadup",
    "text": "Leadup\n\nRecall from last lecture that any of the descriptive statistics we discussed in Week 1 can be viewed as population parameters, when they apply to the population.\n\nE.g. population proportion (\\(p\\)), population variance (\\(\\sigma^2\\)), etc.\n\nOf particular interest to statisticians is often the population mean, \\(\\mu\\).\nLet’s try and draw some analogies from our work with population proportions.\nWhen trying to make inferences on a population proportion \\(p\\), we used the sample proportion \\(\\widehat{P}\\) as a proxy (specifically, a point estimator).\n\nAny guesses on what we might use as a point estimator of \\(\\mu\\)?\nThat’s right- the sample mean \\(\\overline{X}\\)!"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#notation",
    "href": "Pages/Lectures/Lecture13/Lec13.html#notation",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Notation",
    "text": "Notation\n\nAgain, it will be useful to establish some notation:\n\n\\(\\mu\\) represents the population mean, and is deterministic (i.e. fixed) but unknown.\n\\(\\overline{X}\\) represents the mean of some hypothetical sample, and is therefore random (as different samples result in different sample means)\n\\(\\overline{x}\\) represents the mean of a specific sample, and is therefore deterministic (i.e. “we’ve taken this particular sample right here and computed its mean”).\n\nJust as \\(\\widehat{P}\\) has a sampling distribution, so too does \\(\\overline{X}\\).\nThe sampling distribution of \\(\\overline{X}\\), however, will end up requiring a few more considerations than the sampling distribution of \\(\\widehat{P}\\).\n\nWe will need it, however, in order to construct confidence intervals for \\(\\mu\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Confidence Intervals",
    "text": "General Confidence Intervals\n\nWe will follow the general idea we used before of constructing confidence intervals as \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\).\nIn this case, we use \\(\\overline{X}\\) as our point estimator.\nHere is a useful result from Probability Theory:\n\n\n\n\n\n\n\n\nResult\n\n\n\n\nConsider a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). If \\(\\overline{X}\\) denotes the mean of a sample of size \\(n\\) taken from this population, then \\[ \\mathrm{SD}(\\overline{X}) = \\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Confidence Intervals",
    "text": "General Confidence Intervals\n\nTherefore, our confidence intervals will take the form \\[ \\overline{X} \\pm c \\cdot \\frac{\\sigma}{\\sqrt{n}} \\] where the constant \\(c\\) depends on both the sampling distribution of \\(\\overline{X}\\) as well as the confidence level."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#normal-population",
    "href": "Pages/Lectures/Lecture13/Lec13.html#normal-population",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Normal Population",
    "text": "Normal Population\n\nLet’s work on finding the sampling distribution of \\(\\overline{X}\\).\nIt turns out that the first thing we need to ask is whether the underlying population is normally distributed or not.\nIf the underlying population is normally distributed [again with population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\)], we have that \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] meaning the constant \\(c\\) should be selected as the appropriate percentile of the standard normal distribution: \\[ \\overline{x} \\pm z^{\\ast} \\cdot \\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nThe heights of adult males are assumed to follow a normal distribution with mean 70 in and standard deviation 15 in. A representative sample of 120 adult males is taken, and the average height of males in this sample is recorded.\n\nWhat is the random variable of interest?\nIs the value of 70 in a population parameter or a sample statistic?\nWhat is the probability that the average height of males in the sample is between 69.5 in and 71.5 in?"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\n\\(\\overline{X} =\\) the average height of a sample of 120 adult males.\nThe value of 70 in is a population parameter, as it is the true average height of all adult males.\nThe quantity we seek is \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\). Because the population is normally distributed, we can use our result above to conclude \\[ \\overline{X} \\sim \\mathcal{N}\\left( 70, \\ \\frac{15}{\\sqrt{120}} \\right) \\sim \\mathcal{N}\\left( 70, \\ 1.369 \\right) \\]\n\n\nTo find \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\), we therefore utilize the same techniques we used previously, when dealing with normal distribution problems: \\[\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5) = \\mathbb{P}(\\overline{X} \\leq 71.5) - \\mathbb{P}(\\overline{X} \\leq 69.5) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\n\nThe associated \\(z-\\)scores are \\[\\begin{align*}\nz_1     & = \\frac{71.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx 1.10    \\\\\nz_2     & = \\frac{69.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx -0.37\n\\end{align*}\\]\n\n\n\nThe associated probabilities (from a \\(z-\\)table) are \\(0.8643\\) and \\(0.3557\\), respectively, meaning the desired probability is \\[ 0.8643 - 0.3557 = \\boxed{ 0.5086 = 50.86\\% } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population",
    "text": "Non-Normal Population\n\nAlright, so that explains what to do if the population values follow a normal distribution.\nBut what if they don’t? In real-world settings, we don’t typically get to know exactly what the population distribution is.\nIf our population is not normally distributed, we need to ask ourselves whether we have a “large enough sample”.\nAdmittedly, there isn’t a single agreed-upon cutoff for “large enough”- for the purposes of this class, we will use \\(n \\geq 30\\) to mean “large enough” and \\(n < 30\\) to therefore be “not large enough.”"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-30",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-30",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population, \\(n < 30\\)",
    "text": "Non-Normal Population, \\(n < 30\\)\n\nIf the population is non-normal, and the sample size is not large enough…\n… we can’t do anything.\nMore specifically, there aren’t any results we can use to confidently make inferences about the population mean- there is just too much uncertainty, between the uncertainty regarding the population’s distribution and the small sample size."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-geq-30",
    "href": "Pages/Lectures/Lecture13/Lec13.html#non-normal-population-n-geq-30",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Non-Normal Population, \\(n \\geq 30\\)",
    "text": "Non-Normal Population, \\(n \\geq 30\\)\n\nIf the population is non-normal, and the sample size is large enough…\n… we’re still (perhaps surprisingly) in business!\nIt turns out that if \\(n\\) is large enough, \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] that is, the sample mean once again has a normal sampling distribution!\nIn fact, this is such an important result, we give it a name:"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#central-limit-theorem-for-the-sample-mean",
    "href": "Pages/Lectures/Lecture13/Lec13.html#central-limit-theorem-for-the-sample-mean",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Central Limit Theorem for the Sample Mean",
    "text": "Central Limit Theorem for the Sample Mean\n\n\n\n\n\n\n\nCentral Limit Theorem for the Sample Mean\n\n\n\nIf we have reasonably representative samples of large enough size \\(n\\), taken from a population with true mean \\(\\mu\\) and true standard deviation \\(\\sigma\\), then \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\] or, equivalently, \\[ \\overline{X} \\sim \\mathcal{N}\\left( \\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) \\] where \\(\\overline{X}\\) denotes the sample mean."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nThe temperatures collected at all weather stations in Antarctica follow some unknown distribution with unknown mean and known standard deviation 8oF. A researcher records the temperature measurements from a representative sample of 81 different weather stations, and finds the average temperature to be 26oF.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nWhat is the probability that this observed average of 26oF lies within 1oF of the true average temperature across all weather stations in Antarctica?\nConstruct a 90% confidence interval for the true average temperature across all weather stations in Antarctica."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all weather stations in Antarctica.\nThe sample is the 81 weather stations selected by the researcher.\nThe random variable of interest is \\(\\overline{X}\\), the average temperature across 81 randomly-selected weather stations in Antarctica.\n\n\nPart (d): This is where things get interesting!\n\n\nIs the population normally distributed?\n\nNo. Or, at least, we don’t know for certain, so it’s safer not to assume it is.\n\nIs our sample size large enough to invoke the CLT?\n\nYes, since \\(n = 81 \\geq 30\\).\n\nTherefore, the CLT applies and tells us that \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nAgain, what we have found is \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]\nWe seek \\(\\mathbb{P}(\\mu - 1 \\leq \\overline{X} \\leq \\mu + 1)\\), which we first write as \\[ \\mathbb{P}(\\overline{X} \\leq \\mu + 1) - \\mathbb{P}(\\overline{X} \\leq \\mu - 1) \\]\nComputing the necessary \\(z-\\)scores yields \\[\\begin{align*}\nz_1   &  = \\frac{(\\mu + 1) - \\mu}{8/9} = \\frac{9}{8} \\approx 1.13    \\\\\nz_2   &  = \\frac{(\\mu - 1) - \\mu}{8/9} = -\\frac{9}{8} \\approx -1.13\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe corresponding values from the normal table are \\(0.8708\\) and \\(0.1292\\), respectively, meaning the desired probability is \\[ 0.8708 - 0.1292 = \\boxed{74.16\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma",
    "href": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nNotice that in the previous worked-out example (and, indeed, in the CLT for sample means), we need information on the true population standard deviation \\(\\sigma\\).\nWhat happens if we don’t have access to \\(\\sigma\\)?\nWell, we encountered a somewhat similar situation in our discussion on proportions; the standard error of \\(\\widehat{P}\\) depended on \\(p\\), which proves to be a problem in practice (as, again, the true value of \\(p\\) is often unknown).\nDoes anyone remember how we solved this issue in the context of population proportions?\n\nThat’s right- we used the substitution approximation!\nSpecifically, we replaced the unknown parameter (\\(p\\)) with a natural point estimator of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#unknown-sigma-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nCan anyone propose a point estimator for \\(\\sigma\\)?\nThat’s right; \\(s\\), the sample standard deviation! \\[ s = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2} \\]\nIn other words, our proposition is to use confidence intervals of the form \\[ \\overline{x} \\pm c \\cdot \\frac{s}{\\sqrt{n}} \\]\nNotice, however, that this introduces additional uncertainty into the problem as \\(s\\) itself is a random variable (different samples result in different sample standard deviations).\nIt turns out that the additional uncertainty introduced is so large that we become no longer able to use the normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#using-s-in-place-of-sigma",
    "href": "Pages/Lectures/Lecture13/Lec13.html#using-s-in-place-of-sigma",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Using \\(s\\) in place of \\(\\sigma\\)",
    "text": "Using \\(s\\) in place of \\(\\sigma\\)\n\nFirstly, recall that we used percentiles of the standard normal distribution because \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\]\nMathematically, what the above discussion is saying is that the distribution of \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\] is no longer normal.\nIt turns out that, still assuming a large enough sample size, the quantity above follows what is known as a t-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nThe \\(t-\\)distribution looks very similar to the standard normal distribution in that it is centered at 1, and has a bell-like density curve.\nHowever, one key difference is that the \\(t-\\)distribution is parameterized by a single parameter, called the degrees of freedom, which we abbreviate \\(\\mathrm{df}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-t-distribution-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nAnother key property is that, for all finite degrees of freedom, the tails of the t-distribution density curve are “wider” (i.e. higher) than the tails of the standard normal density curve.\n\nWhat this means is that the t-distribution allows for higher probabilities of tail events, thereby incorporating the additional uncertainty injected into our confidence intervals by using \\(s\\) in place of \\(\\sigma\\)\n\nAn interesting fact is that the t-distribution with \\(\\infty\\) degrees of freedom is equivalent to the standard normal distribution.\n\nAs such, with greater degrees of freedom, the t-distribution - and its percentiles - more and more closely resembles the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#back-to-confidence-intervals",
    "href": "Pages/Lectures/Lecture13/Lec13.html#back-to-confidence-intervals",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Back to Confidence Intervals",
    "text": "Back to Confidence Intervals\n\nHere is the result we’ve been working toward: with samples of reasonably large size \\(n\\) from a distribution with mean \\(mu\\) and standard deviation \\(\\sigma\\), \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1} \\] where \\(t_{n - 1}\\) denotes the \\(t-\\)distribution with \\(n - 1\\) degrees of freedom.\nAs such, our confidence intervals become \\[ \\overline{x} \\pm t_{n - 1, \\ \\alpha} \\cdot \\frac{s}{\\sqrt{n}} \\] where \\(t_{n - 1, \\ \\alpha}\\) denotes the appropriate quantile (corresponding to our desired confidence level) of the \\(t_{n - 1}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA sociologist is interested in performing inference on the true average monthly income (in thousands of dollars) of all citizens of the nation of Gauchonia. As such, she takes a representative sample of 49 people, and finds that these 49 people have an average monthly income of 2.25 and a standard deviation of 1.66.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nConstruct a 95% confidence interval for the true average monthly income (in thousands of dollars) of Gauchonian citizens."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-5",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-5",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all Gauchonian residents.\nThe sample is the set of 49 Gauchonian residents included in the sociologist’s sample.\nThe random variable of interest is \\(\\overline{X}\\), the sample average monthly income (in thousands of dollars) of a representative sample of 49 Gauchonian* residents."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\nPart (d)\n\nIs the population normally distributed?\n\nNo.\n\nIs the sample size large enough?\n\nYes; \\(n = 49 \\geq 30\\).\n\nDo we know the population standard deviation?\n\nNo, we only know \\(s\\).\n\nTherefore, we need to use the t-distribution with \\(n - 1 = 49 - 1 = 48\\) degrees of freedom.\nSpecifically, we need to find the 2.5th percentile of the \\(t_{48}\\) distribution.\nLet’s go over how to read a t-table."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#reading-a-t-table",
    "href": "Pages/Lectures/Lecture13/Lec13.html#reading-a-t-table",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Reading a t-table",
    "text": "Reading a t-table\n(In-Class Exercise)"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nI’d also like to mention that we can use Python to help us:\n\n\n\nimport scipy.stats as sps\nsps.t.ppf(0.025, 48)\n\n-2.010634754696446\n\n\n\n\nYou’ll learn more about this code during Lab today!\nTherefore, our 95% confidence interval takes the form \\[ \\overline{x} \\pm 2.01 \\cdot \\frac{s}{\\sqrt{49}} \\] or, equivalently, \\[ (2.25) \\pm (2.01) \\cdot \\frac{1.66}{7} = 2.25 \\pm 0.477 = \\boxed{[1.773 \\ , \\ 2.727]}\\]\nThe interpretation of this interval is much the same as our intervals for proportions:\n\n\n\nWe are 95% confident that the true average monthly income (in thousands of dollars) of Gauchonian residents is between 1.773 and 2.727."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#general-flowchart",
    "href": "Pages/Lectures/Lecture13/Lec13.html#general-flowchart",
    "title": "PSTAT 5A: Lecture 13",
    "section": "General Flowchart",
    "text": "General Flowchart\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --> |Yes| B{{Use Normal .}}\n  A --> |No| C[Is n >= 30?  .]\n  C --> |Yes| D[sigma or s?  .]\n  C --> |No| E{{cannot proceed   .}}\n  D --> |sigma| F{{Use Normal .}}\n  D --> |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#testing-distributional-fits",
    "href": "Pages/Lectures/Lecture13/Lec13.html#testing-distributional-fits",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Testing Distributional Fits",
    "text": "Testing Distributional Fits\n\nNotice that the first question we need to ask ourselves when trying to perform inference on a sample mean is whether or not we believe our population to be normally distributed.\nThis begs the question: given a set of numbers, how can we tell if these numbers were drawn from a normal distribution or not?\nFor example, consider the following set of numbers (which have been assigned to a variable called x):\n\n\n\n\n[ 1.3315865   0.71527897 -1.54540029 -0.00838385  0.62133597 -0.72008556\n  0.26551159  0.10854853  0.00429143 -0.17460021  0.43302619  1.20303737\n -0.96506567  1.02827408  0.22863013  0.44513761 -1.13660221  0.13513688\n  1.484537   -1.07980489 -1.97772828 -1.7433723   0.26607016  2.38496733\n  1.12369125  1.67262221  0.09914922  1.39799638 -0.27124799  0.61320418\n -0.26731719 -0.54930901  0.1327083  -0.47614201  1.30847308  0.19501328\n  0.40020999 -0.33763234  1.25647226 -0.7319695   0.66023155 -0.35087189\n -0.93943336 -0.48933722 -0.80459114 -0.21269764 -0.33914025  0.31216994\n  0.56515267 -0.14742026 -0.02590534  0.2890942  -0.53987907  0.70816002\n  0.84222474  0.2035808   2.39470366  0.91745894 -0.11227247 -0.36218045\n -0.23218226 -0.5017289   1.12878515 -0.69781003 -0.08112218 -0.52929608\n  1.04618286 -1.41855603 -0.36249918 -0.12190569  0.31935642  0.4609029\n -0.21578989  0.98907246  0.31475378  2.46765106 -1.50832149  0.62060066\n -1.04513254 -0.79800882  1.98508459  1.74481415 -1.85618548 -0.2227737\n -0.06584785 -2.13171211 -0.04883051  0.39334122  0.21726515 -1.99439377\n  1.10770823  0.24454398 -0.06191203 -0.75389296  0.71195902  0.91826915\n -0.48209314  0.08958761  0.82699862 -1.95451212]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#idea-1-histogram",
    "href": "Pages/Lectures/Lecture13/Lec13.html#idea-1-histogram",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Idea 1: Histogram",
    "text": "Idea 1: Histogram\n\nOne idea would be to generate the histogram of x:"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#downsides",
    "href": "Pages/Lectures/Lecture13/Lec13.html#downsides",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Downsides",
    "text": "Downsides\n\nHowever, we know that histograms are visually very dependent on the binwidth that was selected!\n\nFurthermore, there are some distributions that look bell-shaped, but are not normal; e.g. the t-distribution!\n\nAs such, we would like a slightly more rigorous way to check for normality.\nThe tool statisticians most often use is called a quantile-quantile plot, or QQ-Plot for short.\nYou don’t have to worry too much (for now) about the details of how they are constructed; for now, we’ll just use Python to generate them and then interpret them ourselves."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#qq-plot",
    "href": "Pages/Lectures/Lecture13/Lec13.html#qq-plot",
    "title": "PSTAT 5A: Lecture 13",
    "section": "QQ-Plot",
    "text": "QQ-Plot\n\n\nscipy.stats.probplot(x, plot = plt);"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#interpreting-qq-plots",
    "href": "Pages/Lectures/Lecture13/Lec13.html#interpreting-qq-plots",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Interpreting QQ-Plots",
    "text": "Interpreting QQ-Plots\n\nThe more linear the QQ-plot, the more likely it is that the data came from a normal distribution.\nWhen checking for deviations from linearity, however, make sure to check the tails as that is most often where non-normality manifests itself the clearest."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#normal-or-not",
    "href": "Pages/Lectures/Lecture13/Lec13.html#normal-or-not",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Normal or Not?",
    "text": "Normal or Not?"
  },
  {
    "objectID": "Pages/schedule.html",
    "href": "Pages/schedule.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis schedule is tentative, and is subject to change. Please check back regularly for updates!\n\n\nLast Updated: Tuesday, July 18, 2023\n\nAs a reminder: Monday sections are Lab Sections and Wednesday sections are Discussion Sections.\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLectures\nDiscussion Worksheet\nLab\n\n\n\n\n1\n(6/26 - 7/02)\n\n00: Introduction\n01: Descriptive Statistics I\n02: Descriptive Statistics II\n\nRemember to read the section titled “Transforming Data” on your own\n\n03: Intro to Probability\n04: Counting\nQz01 Solns\n\nWorksheet 01\nSolns\nLab 01  Introduction to Python\nSolns\n\n\n2\n(7/03 - 7/09)\n\n05: Conditional Probabilities\n06: NO LECTURE (Fourth of July)\n07: Review\n08: Midterm 1\n\nMultiple Choice: Blank Solns\nFree Response: Blank Solns\n\n\nWorksheet 02: Work on the MT1 Practice Problems posted under the “Exam Prep” page\nLab 02  Data Classes, Comparisons, Conditionals, and Functions\nSolns\n\n\n3\n(7/10 - 7/16)\n\n09: Discrete Random Variables\n10: Continuous Random Variables\n\nZ-Table\n\n11: Introduction to Inference\n12: Confidence Intervals for Proportions\n\nExercise 3\n\nQz02 Solns\n\nWorksheet 03\nSolns\nLab 03  Descriptive Statistics and Plotting\nSolns\n\n\n4\n(7/17 - 7/23)\n\n13: Confidence Intervals for Means\n\nT-Table\n\n14: Hypothesis Testing for Proportions\n\nPlease note: this material is fair game for Midterm 2, even if we end up finishing it on Wednesday\n\n15: Review\n16: Midterm 2\n\nWorksheet 04: Work on the MT2 Practice Problems posted under the “Exam Prep” page\nLab 04  Simulations, Sampling, and Loops\nSolns\n\n\n5\n(7/24 - 7/30)\n\n17: Hypothesis Testing for Means\n18: Two-Sample t-Tests\n19: ANOVA (Analysis of Variance)\n20: Correlation, and an Intro to Statistical Modeling\n\nWorksheet 05\nLab05: Markdown Syntax, Importing Data, and Manipulating Tables\n\n\n6\n(7/31 - 8/06)\n\n21: Regression\n22: Regression Diagnostics\n23: Sampling Techniques and Experimental Design\n24: Review\nFINAL EXAM: Friday, August 4 from 4 - 7pm\n\nWorksheet 06\nLab06: Exploratory Data Analysis"
  },
  {
    "objectID": "Pages/calendar.html",
    "href": "Pages/calendar.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "TA\n\n\nEmail\n\n\nSection Time\n\n\nLocation\n\n\n\n\n\n\nOlivier Mulkin\n\n\nomulkin@pstat.ucsb.edu\n\n\n12:30 - 1:20pm\n\n\nPhelps 1513\n\n\n\n\nMengrui Zhang\n\n\nmengrui@ucsb.edu\n\n\n2 - 2:50pm\n\n\nPhelps 1513\n\n\n\n3 - 3:50pm\n\n\nPhelps 1513"
  },
  {
    "objectID": "Pages/calendar.html#visual-weekly-schedule",
    "href": "Pages/calendar.html#visual-weekly-schedule",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Visual Weekly Schedule",
    "text": "Visual Weekly Schedule"
  }
]